---
layout: default
title: DNA germline short variant discovery
parent: Design documents
has_children: true
---

<h1>Germline short variant discovery (SNPs + Indels) and interpretation</h1>
<h2>Design document</h2>

{: .no_toc }
<details open markdown="block">
<summary>Table of contents</summary>
{: .text-delta }
- TOC
{:toc}
</details>

---


## Introduction
DNA sequence data is received from the sequencing facility in FASTQ format.
It is generally considered that approximately 20-40% of rare genetic disease may be detectable by DNA germline short variant discovery and variant interpretation
Specifically, this is tailored to single nucleotide variants (SNVs) and short insertion/deletions (INDELs) in protein coding genes and splice sites.

To produce our final output (e.g. clinical grade reports, downstream statistical analysis), we must process the data as follows:
1. QC for pre-processing
2. Decide on protocol, alignment build, threshold criteria
3. Annotate known effect
4. Interpret impact
5. Report clinical grade genetics
6. Cohort analysis for variant association
7. Any additional custom outputs

<img 
src="{{ "pages/design_doc/images/variant_annotation_graph.png" | relative_url }}"
width="100%">
Figure: Summary of DNA germline short variant discovery pipeline plan.

## Summary of requirements

### Quality control
We use a number of tools such as [github.com/OpenGene/fastp](https://github.com/OpenGene/fastp) 
for comprehensive quality profiling for both before and after filtering data.

### Alignment
* We use [BWA](https://bio-bwa.sourceforge.net) for alignment with GRCh38.
* Our reference genome build is GRCh38 from following source:

```
ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz
```

This genome build is currently considered the best for our application.
You can read more about it on
[Illumina's website](https://www.illumina.com/science/genomics-research/articles/dragen-demystifying-reference-genomes.html) and 
[Heng Li's website](https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use).

* HPC requirements
    * Array of jobs - 1 task per fastq file
    * 8 cpu per task
    * 6 hours per task
    * 12G memory per task
    * Example testing: requires 2hr 15min for a 5.3Gb fastq pair

### Variant calling

We will implement the genome analysis tool kit 
[GATK](https://gatk.broadinstitute.org/hc/en-us)
best practices workflow for 
[germline short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels) (open source licence [here](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT)).
This workflow will be designed to operate on a set of samples constituting a study cohort; 
specifically, a set of per-sample BAM files that have been pre-processed as described in the GATK Best Practices for data pre-processing.

The following pages go into details:
* [GATK Duplicates](gatk_duplicates.html)
* [GATK BQSR](gatk_bsqr.html)
* [GATK Haplotype caller](gatk_hc.html)
* [GATK Genomic db import](gatk_dbimport.html)
* [GATK Genotyping gVCFs](gatk_genotypegvcf.html)
* [GATK VQSR](gatk_vqsr.html)
* [GATK Genotype refine](gatk_genotyperefine.html)
* [Pre-annotation processing](pre_annoprocess.html)
* [Pre-annotation MAF](pre_anno_maf.html)

### Annotation

Variant annotation is a critical step in clinical and statistical genetics.
Popular tools for applying annotation data to VCF format genetic data include:

* Variant Effect Predictor (VEP) [link: VEP](http://www.ensembl.org/info/docs/tools/vep/index.html)
* NIRVANA [link: NIRVANA](https://illumina.github.io/NirvanaDocumentation/)
* ANNOVAR [link: ANNOVAR](https://annovar.openbioinformatics.org/en/latest/)

We are using [VEP](http://www.ensembl.org/info/docs/tools/vep/index.html) with [Conda](https://docs.conda.io/en/latest/), but we are likely to test additional methods
([licence](http://www.ensembl.org/info/about/legal/code_licence.html)).
Additionally, these tools must be paired with a set of data sources containing the annotation information which will be applied to each variant.
* [View our list of approx. 160 databases]({{ site.baseurl }}{% link pages/annotation_table.md %}).

The variant consequence may be one of the defining criteria by which variants can 
be included in analysis since they are _interpretable_ or of ostensibly _known significance_.

The consequences provided by VEP can provide a simple reference example to understand its function.
For example, HIGH impact variants might be a likely consequence for identifying candidates disease-causing:
[Ensembl Variation - Calculated variant consequences](https://grch37.ensembl.org/info/genome/variation/prediction/predicted_data.html#consequences).\

{: .note }
You may have observed cases in literature where clinical research reporting relied on variant effect consequence alone for known disease-related genes, but this practice is likely to introduce spurious results. 
It is important to use established criteria for selecting consequences of interest combined with additional filtering methods to define evidence thresholds.
See the ACMG interpretation standards for examples.

<img 
src="{{ "pages/design_doc/images/VEP_consequences.jpg" | relative_url }}"
width="100%">

### Interpretation

We will perform a range of interpretation steps for:
1. Generalised single-case clinical variant classification
2. Cohort-level classification

For example, we will perform interpretation of variants by ACMG standards and guidelines.
Extensive annotation is applied during our genomics analysis.
Interpretation of genetic determinants of disease is based on many evidence sources.
One important source of interpretation comes from the
Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology
[richards2015standards], full text at doi:
[10.1038/gim.2015.30](https://www.gimjournal.org/article/S1098-3600(21)03031-8/fulltext).

Check the tables linked here:
[temporary link](https://lawlessgenomics.com/topic/acgm-criteria-table-main).
These are provided as they appear in the initial steps of our filtering protocol for the addition of ACMG-standardised labels to candidate causal variants.
* Criteria for classifications
* Caveats implementing filters

Implementing the guidelines for interpretation of annotation requires multiple programmatic steps. 
The number of individual caveat checks indicate the number of bioinformatic filter functions used.
Unnumbered caveat checks indicate that only a single filter function is required during reference to annotation databases.
However, each function depends on reference to either one or several evidence source databases (approximately 160 sources).

For reference, alternative public implementations of ACMG guidelines can be found in [li2017intervar] and [xavier2019tapes];
please note these tools have not implemented here nor is any assertion of their quality offered.
Examples of effective variant filtering and expected candidate variant yield in studies of rare human disease are provided by [pedersen2021effective].


We plan to use our tools built for these requirements which are currently in review:
* ACMGuru for automated clinical genetics evidence interpretation. 
* ProteoMCLustR for unbiased whole-genome pathway clustering; 
* SkatRbrain for statistical sequence kernel association testing with variant collapse.
* UntangleR for pathway visualisation.
* AutoDestructR for protein structure variant mapping. 
All tools were designed for modular automated high-performance computing. 

---
layout: default
title: GATK Duplicates
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 1
---

## Duplicate Marking and Merging

### Overview
The DNA Germline Short Variant Discovery pipeline includes a crucial step of marking duplicates and merging BAM files. This process is essential for ensuring the accuracy of variant calling by eliminating potential biases introduced by duplicate reads resulting from sequencing artifacts.

### Implementation
The `rmdup_merge.sh` script orchestrates this process using high-throughput computational resources managed by the SLURM job scheduler. The script is tailored for operation on a distributed computing environment and is designed to handle large-scale datasets efficiently.

#### Script Description
The script is executed with the following SLURM settings:

- **Nodes**: 1
- **Memory**: 22G
- **Tasks**: 16
- **CPUs per Task**: 1
- **Time Limit**: 32:00:00
- **Job Name**: rmdup_merge
- **Array Jobs**: Supports batch processing for 1-140 samples, allowing parallel processing of multiple samples.

The script starts by setting environment variables and loading necessary modules such as Java and GATK. It then reads from a predefined list of BAM files and processes each according to its SLURM array task ID.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilized for its `MarkDuplicatesSpark` tool which identifies and marks duplicate reads in BAM files. This tool is chosen for its ability to handle large datasets and its integration with Apache Spark for improved performance.

#### Process Flow
1. **Input and Output Directories**: Specified through environment variables.
2. **File Preparation**: Reads from a list containing paths to BAM files and maps them to their respective subject IDs.
3. **Execution**:
   - For each subject (or sample), the script merges and marks duplicates across multiple BAM files using `MarkDuplicatesSpark`.
   - It utilizes Spark to perform operations locally but is configured to work under the computational limits specified by SLURM job parameters.
   - Outputs are directed to specified output directories, and each merged BAM file is named after the subject ID.
   
4. **Post-Processing**:
   - Generation of metrics files for each processed BAM to evaluate the quality of the merging and marking steps.
   - Cleanup operations include removing temporary directories created during the job execution to conserve storage space.

### Quality Assurance
This script incorporates error handling and robust logging mechanisms to ensure that each step of the process is recorded for audit and troubleshooting purposes. Output and error logs are saved to specified directories, allowing easy access to detailed run-time logs and error messages.

### Conclusion
The duplicate marking and merging step is pivotal for preparing sequenced data for subsequent analysis phases like variant calling. By integrating robust tools and leveraging high-performance computing resources, our pipeline ensures that data processed through this stage is of the highest fidelity, setting a strong foundation for accurate and reliable variant discovery.

---
layout: default
title: GATK BQSR
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 2
---

## Base Quality Score Recalibration (BQSR)

### Overview
The Base Quality Score Recalibration (BQSR) step in our pipeline is designed to adjust the base quality scores in BAM files based on known sites of variation and sequencing artifacts. This recalibration helps to mitigate biases that might have been introduced during sequencing.

### Implementation
The `06_bqsr.sh` script manages the BQSR process using the Genome Analysis Toolkit (GATK). It operates within a high-throughput computational framework facilitated by the SLURM job scheduler, specifically configured for efficient handling of genomic data.

#### Script Description
The script employs specific SLURM settings to optimize the use of computational resources:

- **Nodes**: 1
- **Memory**: 6G
- **CPUs per Task**: 4
- **Time Limit**: 32:00:00
- **Job Name**: bqsr
- **Job Array**: Processes batches of 1-8 samples simultaneously.

The script initializes by setting up the necessary environment, loading computational modules, and defining input and output directories based on predefined variables.

#### Tools Used
- **GATK (v4.4.0.0)**: Used for its `BaseRecalibrator` and `ApplyBQSR` tools to recalibrate base quality scores based on multiple known sites databases.

#### Process Flow
1. **File Handling**:
   - Locates BAM files previously processed for duplicate marking and merging.
   - Assigns each file to a specific SLURM array job based on its task ID.
   
2. **Recalibration Execution**:
   - The `BaseRecalibrator` tool generates a recalibration table based on the input BAM, a reference genome, and known variant sites.
   - The `ApplyBQSR` tool then adjusts the base quality scores using the generated recalibration table, producing a recalibrated BAM file.

3. **Known Sites**:
   - Multiple databases are used as references for known sites, including dbSNP and the Mills and 1000 Genomes gold standard indels, to ensure comprehensive coverage and accuracy in recalibration.

4. **Output**:
   - Each sample results in a recalibrated BAM file and an accompanying BQSR table, stored in the designated output directory.

### Quality Assurance
This step includes detailed logging of each process, capturing standard output and errors to facilitate troubleshooting and ensure reproducibility. The script also features robust error handling mechanisms to address potential failures during the recalibration process.

### Conclusion
BQSR is a critical component of our variant discovery pipeline, enhancing the reliability of variant calls by refining the accuracy of base quality scores in sequenced data. By leveraging advanced tools and high-performance computing resources, this step ensures that subsequent analyses are based on the highest quality data.

---
layout: default
title: GATK Haplotype caller
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 3
---

## Haplotype Calling

### Overview
Haplotype calling is a critical step in our pipeline, involving the identification of variants from sequenced DNA by constructing haplotypes. This step uses the Genome Analysis Toolkit's (GATK) HaplotypeCaller, which allows for calling high-confidence variants.

### Implementation
The script `07_haplotype_caller.sh` is designed to manage this intensive computational task effectively, utilizing SLURM for job scheduling to handle potentially large genomic datasets.

#### Script Description
Configured to maximize efficiency given computational constraints:

- **Nodes**: 1
- **Memory**: 4G
- **CPUs per Task**: 2
- **Time Limit**: 48:00:00
- **Job Name**: hc
- **Job Array**: Capable of handling 1-68 samples concurrently.

The script begins by setting up the necessary computing environment, sourcing variables, and preparing input and output directories.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilized for the HaplotypeCaller tool, which is executed under specific java options to manage memory usage and parallel processing capabilities.

#### Process Flow
1. **File Preparation**:
   - Finds recalibrated BAM files from the previous BQSR step.
   - Each BAM file is assigned to a SLURM job based on its array task ID.

2. **Variant Calling Execution**:
   - HaplotypeCaller runs with parameters to produce a genomic VCF (gVCF) for each sample, which includes variant calls along with confidence scores.
   - Outputs are genomic VCF files named after each sample, stored in the designated output directory.

3. **Optimization and Debugging**:
   - Detailed logging of the script's execution, including start and end times, input and output details, and memory settings, helps in troubleshooting and ensuring reproducibility.

### Quality Assurance
This stage of the pipeline includes detailed logging and error checking to ensure that the haplotype calling process is robust against computational failures and produces reliable results.

### Conclusion
The Haplotype Calling step is pivotal for identifying variants accurately, setting the stage for subsequent processes such as variant annotation and interpretation. The use of high-performance computing resources ensures that our pipeline can handle large datasets efficiently and reliably.

---
layout: default
title: GATK Genomic db import
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 4
---

## Genomics Database Import

### Overview
The Genomics Database Import step is crucial for compiling variant data from multiple samples into a unified database, enhancing the efficiency of downstream variant analysis processes such as joint genotyping.

### Implementation
The script `08_genomics_db_import.sh` is designed to handle the consolidation of genomic variant call files (gVCFs) into a GenomicsDB workspace using GATK's `GenomicsDBImport` tool. This script is configured to process the data using SLURM job arrays, enabling parallel processing of genomic data.

#### Script Description
The script is optimized for high-throughput computational requirements:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 2
- **Time Limit**: 72:00:00
- **Job Name**: genomics_db_import
- **Job Array**: Supports handling 25 chromosome sets in a batch processing manner.

The script begins by establishing the environment, sourcing necessary variables, and setting up directories for input and output data.

#### Tools Used
- **GATK (v4.4.0.0)**: Used for its `GenomicsDBImport` tool, which allows for the efficient aggregation of gVCF files into a single database that can be queried and analyzed more efficiently.

#### Process Flow
1. **Input Preparation**:
   - Identifies all gVCF files from the Haplotype Calling step.
   - Constructs a command to include all these files in the GenomicsDB workspace.

2. **Database Creation**:
   - For each chromosome or chromosome set specified by the job array, a separate GenomicsDB workspace is created.
   - The process is customized to include optimizations for shared POSIX filesystems, which improves performance on distributed computing systems.

3. **Execution Details**:
   - The script uses dynamic allocation of memory and CPU resources to handle the intense demands of processing large genomic datasets.
   - Outputs include a GenomicsDB workspace for each chromosome, facilitating rapid access and manipulation in subsequent analytical steps.

### Quality Assurance
Robust logging, detailed output management, and stringent error handling are implemented to ensure the reliability and reproducibility of the Genomics Database Import process.

### Conclusion
The consolidation of gVCF files into a GenomicsDB workspace is a pivotal step in our pipeline. It not only optimizes the storage and querying of genomic data but also sets the stage for efficient joint genotyping and variant analysis across multiple samples. This process leverages advanced computational tools and techniques to handle the complexities of large-scale genomic datasets effectively.

---

---
layout: default
title: GATK Genotyping gVCFs
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 5
---

## Genotyping gVCFs

### Overview
Following the import of genomic variant call format (gVCF) files into a GenomicsDB workspace, the next stage in our pipeline involves genotyping these consolidated gVCFs. This step is crucial for calling variants across multiple samples simultaneously, which enhances the discovery and accuracy of genetic variants.

### Implementation
The `09_genotype_gvcf.sh` script manages the genotyping of variants from the GenomicsDB workspaces. This process uses the GATK's GenotypeGVCFs tool, specifically tailored to handle large genomic datasets with high computational efficiency.

#### Script Description
Configured for intensive computational tasks:

- **Dependency**: Waits for previous jobs to complete, ensuring that all necessary data is available for genotyping.
- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 2
- **Time Limit**: 96:00:00
- **Job Name**: genotype_gvcf
- **Job Array**: Capable of processing 25 chromosomal segments in one batch.

The script starts by setting up the required environment, sourcing variables, and preparing input and output directories.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilized for its `GenotypeGVCFs` tool, which is designed to perform the final genotyping step on the aggregated gVCF data stored in a GenomicsDB workspace.

#### Process Flow
1. **Input and Output Setup**:
   - Directories are established based on predefined paths, where GenomicsDB workspaces are the input and the output is specified as genomic VCF files.

2. **Execution of Genotyping**:
   - For each job in the array, corresponding to a specific chromosome or chromosomal segment, the script accesses the appropriate GenomicsDB workspace.
   - The `GenotypeGVCFs` command is executed to produce a gVCF file for each chromosome, containing the genotyped variants.

3. **Optimization and Resource Management**:
   - The Java options are configured to optimize memory usage and parallel processing capabilities to manage the large data volumes typically involved in genomic analysis.

### Quality Assurance
This stage includes comprehensive logging and error tracking to ensure the process is executed correctly and efficiently. Each step's outputs are systematically verified to maintain high data integrity and reproducibility.

### Conclusion
Genotyping of gVCFs is an essential process in our DNA Germline Short Variant Discovery pipeline, enabling the detailed analysis of genetic variations across multiple samples. By leveraging high-performance computing resources and sophisticated bioinformatics tools, this step ensures that our pipeline produces accurate and reliable variant calls.

---


---
layout: default
title: GATK VQSR
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 6
---

## Variant Quality Score Recalibration (VQSR)

### Overview
Variant Quality Score Recalibration (VQSR) is an advanced technique used in our pipeline to assess and recalibrate the confidence scores of identified variants. By using known, reliable sources as benchmarks, VQSR improves the accuracy of variant calls, distinguishing true biological variants from technical artifacts.

### Implementation
The `10_vqsr.sh` script executes the VQSR process using GATK's VariantRecalibrator and ApplyVQSR tools. This script is configured for high-demand computational tasks to handle variant data across all chromosome segments.

#### Script Description
Optimized for high-throughput:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: vqsr
- **Job Array**: Capable of processing 25 chromosome segments in a single run.

The script begins by preparing the environment, loading necessary modules, and setting up directories.

#### Tools Used
- **GATK (v4.4.0.0)**: Employed for both the VariantRecalibrator and ApplyVQSR tools, which together adjust the quality scores of variants based on their statistical likelihood of being true genetic variants.

#### Process Flow
1. **Input Setup**:
   - Retrieves gVCF files for each chromosome from the previous genotyping step.

2. **Execution of VQSR**:
   - For SNPs and INDELs, separate recalibration and application steps are performed:
     - **VariantRecalibrator**: Generates recalibration models based on a set of user-defined annotations and known variant sites.
     - **ApplyVQSR**: Applies the recalibration model to the variant calls, filtering or adjusting their quality scores based on the recalibration.

3. **Known Sites and Resources**:
   - Utilizes multiple databases such as HapMap, Omni, 1000 Genomes, and Mills for recalibration, which help in training the recalibration model to distinguish between high-confidence and low-confidence variants.

4. **Output Generation**:
   - Produces recalibrated VCF files for SNPs and INDELs, which are stored in specified output directories.

### Quality Assurance
Includes extensive logging and monitoring of the recalibration process to ensure accuracy and efficiency. Error handling and checkpointing ensure that the process can resume from intermediate stages in case of failures.

### Conclusion
VQSR is essential for our DNA Germline Short Variant Discovery pipeline as it refines variant calls, enhancing the reliability and accuracy of downstream genetic analyses. This process leverages comprehensive known variant datasets and robust computational resources to maintain high standards of variant calling.

---


---
layout: default
title: GATK Genotype refine
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 6
---

## Genotype Refinement

### Overview
Genotype refinement is a critical process in our pipeline aimed at enhancing the reliability of genotype calls by utilizing additional population data. This step applies statistical methods to refine genotype probabilities and filter variants based on genotype quality (GQ) scores.

### Implementation
The `11_genotype_refinement.sh` script uses tools from the Genome Analysis Toolkit (GATK) to refine genotypes based on a model that incorporates population-wide allele frequencies. The script is set up to process data efficiently across multiple chromosomes.

#### Script Description
Configured for substantial computational tasks:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: genotype_refine
- **Job Array**: Designed to handle multiple chromosomal segments in one pass.

The script initiates by setting up the required computational environment, loading modules, and preparing directories for input and output.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilizes `CalculateGenotypePosteriors` for refining genotype probabilities and `VariantFiltration` to apply quality filters to genotypes based on predefined thresholds.

#### Process Flow
1. **Input Preparation**:
   - The script retrieves recalibrated VCF files from the VQSR step and checks their existence before proceeding.

2. **Refinement Execution**:
   - **CalculateGenotypePosteriors**: This tool is used to adjust genotype likelihoods based on allele frequency data from large reference populations (e.g., gnomAD).
   - **VariantFiltration**: Applies a filter to flag genotypes with a GQ score less than 20, helping to ensure that only high-confidence genotypes are used in subsequent analyses.

3. **Outputs**:
   - Generates two sets of VCF files:
     - A refined VCF file with updated genotype likelihoods.
     - A filtered VCF file that excludes genotypes below a quality threshold, ensuring the dataset's integrity for further analysis.

### Quality Assurance
The process includes comprehensive logging and condition checks to ensure accuracy and efficiency. Error handling mechanisms are in place to address any issues during file handling or processing, enhancing the robustness of the pipeline.

### Conclusion
Genotype refinement is an essential step to ensure the high quality and reliability of variant calls in genomic studies. By integrating additional genomic data and applying rigorous quality control measures, this step helps in producing highly accurate genetic analyses.

---


---
layout: default
title: Pre-annotation processing
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 7
---

## Pre-Annotation Processing

### Overview
The Pre-Annotation Processing step refines VCF files prior to detailed annotation. This involves filtering, normalizing, and decomposing variants to ensure that the data fed into the annotation tools is of high quality and structured correctly.

### Implementation
The `12_pre_annotation_processing.sh` script employs a combination of bioinformatics tools including bcftools, GATK, and vt to refine VCF files. This script is designed to handle large datasets efficiently, processing data across multiple chromosomal segments.

#### Script Description
Optimized for high-throughput computing:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: pre_annotation
- **Job Array**: Capable of processing up to 25 chromosome segments in one pass.

The script starts by establishing the necessary computational environment and directories for input and output data.

#### Tools Used
- **bcftools**: Used for initial filtering based on quality metrics such as quality score, depth, and genotype quality.
- **GATK (v4.4.0.0)**: Utilized for selecting variants based on filtering criteria.
- **vt**: Applied for decomposing and normalizing variants to a canonical form, simplifying subsequent annotation processes.

#### Process Flow
1. **Initial Filtering**:
   - Filters variants using bcftools based on predefined quality criteria to ensure that only high-quality variants are processed further.

2. **Compression and Indexing**:
   - Compresses the filtered VCF files using bgzip and indexes them with tabix, preparing them for further processing.

3. **GATK Selection**:
   - Selects variants using GATK’s SelectVariants to exclude filtered and non-variant entries, refining the dataset.

4. **Normalization and Decomposition**:
   - Uses vt to decompose multiallelic sites into simpler allelic forms and normalizes them against the reference genome. This step ensures that variants are represented in their simplest form, aiding accurate annotation.

5. **Final Compression and Clean-up**:
   - Compresses and re-indexes the final VCF files for efficient storage and access.
   - Cleans up intermediate files to free storage space and maintain a tidy workspace.

### Quality Assurance
This step includes extensive error handling and logging to ensure that each sub-process is completed successfully. Detailed logs facilitate troubleshooting and ensure reproducibility.

### Conclusion
Pre-Annotation Processing is crucial for preparing VCF files for detailed annotation. By ensuring that the data is high-quality and properly formatted, this step lays the groundwork for accurate and efficient genomic annotation, which is critical for downstream genomic analyses.

---
layout: default
title: Pre-annotation MAF
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 8
---

## Pre-Annotation MAF Filtering

### Overview
The Pre-Annotation MAF Filtering step selectively removes variants from VCF files that exceed a specified minor allele frequency threshold. This step is crucial for focusing analyses on rare variants, which are often of particular interest in studies of rare genetic disorders.

### Implementation
The `13_pre_annotation_MAF.sh` script employs vcftools to apply MAF-based filtering to the VCF files prepared in the previous steps. This script is optimized to process multiple chromosomal segments, ensuring that only variants with desired allele frequency characteristics are retained for further analysis.

#### Script Description
Configured for resource-intensive tasks:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: maf_pre_annotation
- **Job Array**: Processes up to 25 chromosomal segments.

The script sets up the necessary environment and directories, preparing for efficient execution of MAF filtering.

#### Tools Used
- **vcftools**: This tool is used for applying filters to VCF files based on allele frequency data, effectively removing common variants according to the specified MAF threshold.

#### Process Flow
1. **Input File Checks**:
   - Verifies the presence of input VCF files to ensure that all necessary data is available for processing.

2. **MAF Filtering**:
   - Applies a filter to exclude variants with a MAF higher than the specified threshold (0.4 in this setup), focusing the dataset on rarer variants.
   - Recodes the VCF to include only the variants that pass the filtering criteria.

3. **Output Compression and Indexing**:
   - Compresses the filtered VCF files using bgzip and creates indexed files with tabix to facilitate efficient data retrieval and further processing.

4. **Cleanup**:
   - Removes intermediate files to conserve storage space and maintain a clean working environment.

### Quality Assurance
This step includes detailed logging of all operations and stringent checks to ensure that filtering is applied correctly and comprehensively. Error handling mechanisms safeguard against potential data processing issues.

### Conclusion
MAF filtering is a critical component of our variant analysis pipeline, enabling researchers to focus on variants of interest by filtering out common genetic variations. This process not only refines the dataset but also ensures that subsequent analyses, such as variant annotation and association studies, are more targeted and meaningful.

