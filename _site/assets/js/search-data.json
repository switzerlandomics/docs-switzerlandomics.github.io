{"0": {
    "doc": "Sepsis score: Phoenix",
    "title": "Implementing the Phoenix sepsis score",
    "content": " ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#implementing-the-phoenix-sepsis-score",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#implementing-the-phoenix-sepsis-score"
  },"1": {
    "doc": "Sepsis score: Phoenix",
    "title": "Abstract",
    "content": "This work deomostrates how to implement the Phoenix Sepsis Score criteria for sepsis and septic shock in children. Pediatric sepsis in children (&lt;18 years) with suspected infection can be identified by at least 2 points in the novel Phoenix Sepsis Score, including dysfunction of the respiratory, cardiovascular, coagulation, and/or neurological systems; and septic shock as sepsis with at least 1 cardiovascular point in the Phoenix Sepsis Score. These criteria for pediatric sepsis and septic shock are globally applicable. The original investigation can be read at: International consensus criteria for pediatric sepsis and septic shock (Schlapbach et al. 2024). The phoenix R package and Python model are available from (DeWitt et al. 2024). The original investigation can be read at: International consensus criteria for pediatric sepsis and septic shock (Schlapbach et al. 2024). The phoenix R package and Python model are available from (DeWitt et al. 2024). The implementations of the Phoenix Sepsis Criteria in R, Python, and SQL are found here https://github.com/cu-dbmi-peds/phoenix/ and here https://pypi.org/project/phoenix-sepsis/, by the original authors (DeWitt et al. 2024). An example implementation of the sofa score (a different score for adult sepsis) is shown in (Bennett, Plecko, and Ukor 2023) Intensive Care Unit Data with R https://github.com/eth-mds/ricu. Decision tree from (Schlapbach et al. 2024). ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#abstract",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#abstract"
  },"2": {
    "doc": "Sepsis score: Phoenix",
    "title": "History",
    "content": "Sepsis definitions have evolved incrementally. We first breifly summarise these stages before getting to the Phoenix sepsis score definitions. First Formal Definition (1992) - Sepsis-1 (Carlton et al. 2019) . | Introduced at the American College of Chest Physicians and Society of Critical Care Medicine Consensus Conference. Defined as an excessive inflammatory response to infection, requiring at least two abnormalities (temperature, heart rate, respiratory rate, or white blood cell count). | . Second Definition Update (Early 2000s) - Sepsis-2 (Souza et al. 2022) . | This update expanded the signs of an inflammatory response but remained similar to Sepsis-1. | . Pediatric-Specific Sepsis Criteria (2005) (Goldstein, Giroir, and Randolph 2005; Souza et al. 2022; Reinhart et al. 2017) . | Developed during the International Pediatric Sepsis Consensus Conference, defining pediatric sepsis as two or more SIRS criteria in the context of infection, adjusting for pediatric physiology. | . Third Definition Update (2016) - Sepsis-3 (Levy et al. 2003; Seymour et al. 2016) . | Sepsis redefined as life-threatening acute organ dysfunction due to a dysregulated host response to infection, with SIRS criteria removed. SOFA score of at least 2 points adopted for defining acute organ dysfunction. | . Phoenix Sepsis Criteria (2019) (Gebara 2005; Levy et al. 2003; Seymour et al. 2016; Singer et al. 2016) . | Following a data-driven review, pediatric sepsis is redefined as life-threatening organ dysfunction of the respiratory, cardiovascular, coagulation, and neurological systems, as demonstrated by a Phoenix Sepsis Score of at least 2 in the setting of confirmed or suspected infection. | . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#history",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#history"
  },"3": {
    "doc": "Sepsis score: Phoenix",
    "title": "Phoenix sepsis score criteria",
    "content": "This section delineates the scoring system used to evaluate respiratory and cardiovascular functions, mean arterial pressure by age, coagulation, and neurological status in a clinical setting. The scoring criteria are stratified to reflect varying degrees of severity in patient conditions, providing a structured framework for assessing the risk of sepsis: . Definitions . | Respiratory Score: Assesses pulmonary efficiency based on the ratio of arterial oxygen partial pressure (PaO2) to fractional inspired oxygen (FiO2), with adjustments for respiratory support and invasive mechanical ventilation (IMV). | Cardiovascular Score: Evaluates circulatory system stability by accounting for lactate levels and the usage of vasoactive medications, which are indicative of cardiovascular strain and potential shock. | Mean Arterial Pressure: Provides age-specific benchmarks for arterial pressure, serving as critical indicators of circulatory health in paediatric to adolescent patients. | Coagulation Score: Reviews the blood’s ability to clot appropriately, utilising platelet count, International Normalized Ratio (INR), D-dimer levels, and fibrinogen as markers of potential disseminated intravascular coagulation or other coagulopathies. | Neurological Score: Uses the Glasgow Coma Scale to assess neurological function, crucial for determining the impact of sepsis on the central nervous system. | . Scoring logic . Respiratory Score (0-3 points): . | 0 Points: PaO2:FiO2 ≥ 400 or SpO2:FiO2 ≥ 292 (only if SpO2 is 97% or less) [b] | 1 Point: PaO2:FiO2 &lt; 400 on any respiratory support or SpO2:FiO2 &lt; 292 on any respiratory support [b,c] | 2 Points: PaO2:FiO2 between 100-200 and IMV or SpO2:FiO2 between 148-220 and IMV [b] | 3 Points: PaO2:FiO2 &lt; 100 and IMV or SpO2:FiO2 &lt; 148 and IMV [b] | . Cardiovascular Score (0-6 points): . | 0 Points: No vasoactive medications and Lactate &lt;= 5 mmol/L [d] | 1 Point: Each up to 3 points for: . | 1 Vasoactive medication [d] | Lactate between 5.1-10.9 mmol/L [e] | . | 2 Points: Each up to 6 points for: . | ≥2 Vasoactive medications [d] | Lactate ≥11 mmol/L [e] | . | . Mean Arterial Pressure by age, mm Hg: [f,g] . | &lt; 1 month: &gt;30 (0 points), 17-30 (1 point), &lt;17 (2 points) | 1 to 11 months: &gt;38 (0 points), 25-38 (1 point), &lt;25 (2 points) | 1 to &lt;2 years: &gt;43 (0 points), 31-43 (1 point), &lt;31 (2 points) | 2 to &lt;5 years: &gt;44 (0 points), 32-44 (1 point), &lt;32 (2 points) | 5 to &lt;12 years: &gt;48 (0 points), 36-48 (1 point), &lt;36 (2 points) | 12 to 17 years: &gt;51 (0 points), 38-51 (1 point), &lt;38 (2 points) | . Coagulation Score (0-2 points): [h] . | 0 Points: Platelets ≥100 x 10^3/μL, INR &lt;=1.3, D-dimer &lt;=2 mg/L FEU, Fibrinogen ≥100 mg/dL | 1 Point each (maximum 2 points): Platelets &lt;100 x 10^3/μL, INR &gt;1.3, D-dimer &gt;2 mg/L FEU, Fibrinogen &lt;100 mg/dL | . Neurological Score (0-2 points): . | 0 Points: Glasgow Coma Scale score &gt;10; pupils reactive | 1 Point: Glasgow Coma Scale score ≤10 [j] | 2 Points: Fixed pupils bilaterally | . Phoenix Sepsis Criteria: . | Sepsis: Suspected infection and Phoenix Sepsis Score ≥2 points | Septic Shock: Sepsis with ≥1 cardiovascular points | . SI Conversion Factor: . | To convert lactate from mmol/L to mg/dL, divide by 0.111. | . Additional Notes: . | [a] The score may be calculated even if some variables are absent (e.g., even if lactate level is not measured and vasoactive medications are not used, a cardiovascular score can still be ascertained using blood pressure). Laboratory tests and other measurements should be obtained at the discretion of the medical team based on clinical judgment. Unmeasured variables contribute no points to the score. Ages are not adjusted for prematurity, and the criteria do not apply to birth hospitalizations, neonates whose postconceptional age is younger than 37 weeks, or those 18 years of age or older. | [b] SpO2:FIO2 ratio is only calculated if SpO2 is 97% or less. | [c] Respiratory dysfunction scoring varies based on the type of respiratory support provided and includes specific criteria for PaO2:FIO2 and SpO2:FIO2 ratios. | [d] Vasoactive medications include any dose of epinephrine, norepinephrine, dopamine, dobutamine, milrinone, and/or vasopressin (for shock). | [e] Lactate reference range is 0.5 to 2.2 mmol/L. Lactate can be arterial or venous. | [f] Ages are not adjusted for prematurity, and the criteria do not apply to birth hospitalizations, neonates whose postconceptional age is younger than 37 weeks, or those 18 years of age or older. | [g] Use measured MAP preferentially (invasive arterial if available or noninvasive oscillometric), and if measured MAP is not available, a calculated MAP (1/3 × systolic + 2/3 × diastolic) may be used as an alternative. | [h] Coagulation variable reference ranges: platelets, 150 to 450 × 10^3/μL; D-dimer, &lt;0.5 mg/L FEU; fibrinogen, 180 to 410 mg/dL. The INR reference range is based on the local reference prothrombin time. | [i] The neurological dysfunction subscore was validated in both sedated and nonsedated patients, and those receiving or not receiving IMV support. | [j] The Glasgow Coma Scale score measures the level of consciousness based on verbal, eye, and motor response (range, 3-15, with a higher score indicating better neurological function). | . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#phoenix-sepsis-score-criteria",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#phoenix-sepsis-score-criteria"
  },"4": {
    "doc": "Sepsis score: Phoenix",
    "title": "Define a dataset",
    "content": "This section details the synthetic generation of patient data, simulating a variety of clinical scenarios to test the scoring system. Data points including respiratory and cardiovascular function, coagulation factors, and neurological status are randomly generated within defined clinical ranges, creating a robust dataset for analysis: - The synthetic dataset is crucial for validating the scoring algorithms under controlled conditions, ensuring they are robust across typical clinical variations. # Subject cohort ---- sample_count &lt;- 200 # Define min-max ranges for each variable PaO2_FiO2_range &lt;- c(100, 450) SpO2_FiO2_range &lt;- c(100, 450) lactate_range &lt;- c(0.5, 5) platelets_range &lt;- c(20000, 450000) INR_range &lt;- c(0.8, 3.5) d_dimer_range &lt;- c(0, 5) fibrinogen_range &lt;- c(100, 500) GCS_range &lt;- c(3, 15) age_months_range &lt;- c(1, 228) # Generate random data for a given number of subjects data &lt;- data.frame( subject_id = 1:sample_count, PaO2_FiO2 = round(runif(sample_count, PaO2_FiO2_range[1], PaO2_FiO2_range[2]), 2), SpO2_FiO2 = round(runif(sample_count, SpO2_FiO2_range[1], SpO2_FiO2_range[2]), 2), is_on_IMV = sample(c(TRUE, FALSE), sample_count, replace = TRUE, prob = c(0.3, 0.7)), lactate = round(runif(sample_count, lactate_range[1], lactate_range[2]), 2), vasoactive_medications = sample(0:3, sample_count, replace = TRUE), platelets = round(runif(sample_count, platelets_range[1], platelets_range[2]), 0), INR = round(runif(sample_count, INR_range[1], INR_range[2]), 2), d_dimer = round(runif(sample_count, d_dimer_range[1], d_dimer_range[2]), 2), fibrinogen = round(runif(sample_count, fibrinogen_range[1], fibrinogen_range[2]), 2), GCS = sample(GCS_range[1]:GCS_range[2], sample_count, replace = TRUE), pupils_reactive = sample(c(TRUE, FALSE), sample_count, replace = TRUE), age_months = sample(age_months_range[1]:age_months_range[2], sample_count, replace = TRUE) ) . print(head(data)) . ## subject_id PaO2_FiO2 SpO2_FiO2 is_on_IMV lactate ## 1 1 200.65 183.55 TRUE 1.57 ## 2 2 375.91 436.83 FALSE 3.59 ## 3 3 243.14 310.48 TRUE 1.52 ## 4 4 409.06 280.26 FALSE 1.93 ## 5 5 429.16 240.90 FALSE 1.28 ## 6 6 115.94 408.09 FALSE 4.11 ## vasoactive_medications platelets INR d_dimer fibrinogen ## 1 2 137658 3.12 1.17 355.67 ## 2 0 275363 3.20 1.15 149.93 ## 3 3 88879 2.12 0.31 202.11 ## 4 3 386975 2.74 2.49 428.23 ## 5 2 384528 2.11 1.22 421.51 ## 6 0 225491 3.47 3.79 118.33 ## GCS pupils_reactive age_months ## 1 12 FALSE 199 ## 2 4 TRUE 26 ## 3 15 TRUE 209 ## 4 8 TRUE 175 ## 5 15 TRUE 37 ## 6 14 FALSE 32 . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#define-a-dataset",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#define-a-dataset"
  },"5": {
    "doc": "Sepsis score: Phoenix",
    "title": "Data distribution plots",
    "content": "Visual representations of the dataset are generated to illustrate the distribution of each clinical measurement. These plots are instrumental in identifying trends, outliers, and the overall distribution of data points within the synthetic cohort: - Histograms and bar plots provide insights into the frequency and categorisation of data points, facilitating preliminary assessments of data quality and distribution integrity. ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#data-distribution-plots",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#data-distribution-plots"
  },"6": {
    "doc": "Sepsis score: Phoenix",
    "title": "Logic functions for scoring",
    "content": "Defines the functional logic for calculating scores based on the clinical criteria outlined. Each function is tailored to process specific aspects of patient data, converting raw measurements into actionable clinical scores: - These functions are the computational backbone of the scoring system, translating clinical data into standardized scores that reflect patient health status. Respiratory score calculation: . # Respiratory score calculation respiratory_score &lt;- function(PaO2_FiO2, SpO2_FiO2, is_on_IMV) { if (PaO2_FiO2 &gt;= 400 || SpO2_FiO2 &gt;= 292) { return(0) } else if ((PaO2_FiO2 &lt; 400 &amp;&amp; SpO2_FiO2 &lt; 292) || !is_on_IMV) { return(1) } else if ((PaO2_FiO2 &gt;= 100 &amp;&amp; PaO2_FiO2 &lt; 200 &amp;&amp; is_on_IMV) || (SpO2_FiO2 &gt;= 148 &amp;&amp; SpO2_FiO2 &lt; 220 &amp;&amp; is_on_IMV)) { return(2) } else { return(3) } } . Cardiovascular score calculation: . # Cardiovascular score calculation cardiovascular_score &lt;- function(lactate, vasoactive_medications) { score &lt;- 0 # Check for 0 points condition explicitly if (vasoactive_medications == 0 &amp;&amp; lactate &lt;= 5) { return(score) # Return 0 if no medications and low lactate } # Points for vasoactive medications if (vasoactive_medications &gt;= 2) { score &lt;- score + 6 # Max 6 points for 2 or more medications } else if (vasoactive_medications == 1) { score &lt;- score + 2 # 2 points for 1 medication } # Points for lactate levels if (lactate &gt; 11) { score &lt;- score + 2 } else if (lactate &gt;= 5.1 &amp;&amp; lactate &lt;= 10.9) { score &lt;- score + 1 } return(score) } . Coagulation score calculation: . # Coagulation score calculation coagulation_score &lt;- function(platelets, INR, d_dimer, fibrinogen) { score &lt;- 0 if (platelets &lt; 100 || INR &gt; 1.3 || d_dimer &gt; 2 || fibrinogen &lt; 100) { score &lt;- score + 1 } return(min(score, 2)) # Max of 2 points } . Neurological score calculation: . # Neurological score calculation neurological_score &lt;- function(GCS, pupils_reactive) { if (!pupils_reactive) { return(2) } else if (GCS &lt;= 10) { return(1) } else { return(0) } } . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#logic-functions-for-scoring",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#logic-functions-for-scoring"
  },"7": {
    "doc": "Sepsis score: Phoenix",
    "title": "Decision algorithm for scoring",
    "content": "Integrates individual scores to derive a comprehensive sepsis risk assessment. The algorithm considers the cumulative impact of respiratory, cardiovascular, coagulation, and neurological scores to determine the overall risk of sepsis and septic shock: - This decision-making algorithm is pivotal for applying the scoring system in a clinical context, providing a final determination that supports clinical decision-making. # Modified evaluate_sepsis to handle a vector from apply evaluate_sepsis &lt;- function(subject) { # Calculate scores res_score &lt;- respiratory_score(subject[\"PaO2_FiO2\"], subject[\"SpO2_FiO2\"], subject[\"is_on_IMV\"]) cardio_score &lt;- cardiovascular_score(subject[\"lactate\"], subject[\"vasoactive_medications\"]) coag_score &lt;- coagulation_score(subject[\"platelets\"], subject[\"INR\"], subject[\"d_dimer\"], subject[\"fibrinogen\"]) neuro_score &lt;- neurological_score(subject[\"GCS\"], subject[\"pupils_reactive\"]) # Total Phoenix Sepsis Score total_score &lt;- res_score + cardio_score + coag_score + neuro_score # Assess criteria result &lt;- if (total_score &gt;= 2) { if (cardio_score &gt;= 1) { \"Septic shock\" } else { \"Sepsis\" } } else { \"Monitor and reassess\" } # Ensure output is always a data frame return(data.frame(result = result, total_score = total_score)) } . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#decision-algorithm-for-scoring",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#decision-algorithm-for-scoring"
  },"8": {
    "doc": "Sepsis score: Phoenix",
    "title": "Run evaluation and return result",
    "content": "Applies the decision algorithm across the patient dataset, appending the results to the original data frame. This process culminates in the generation of a final dataset that includes both the raw clinical measurements and the derived sepsis scores, ready for further analysis or review: - The output enriches the dataset with valuable insights into patient health, enabling clinicians and researchers to conduct detailed analyses or refine the scoring criteria based on empirical evidence. # Running the assessment for each subject # Check if the 'data' DataFrame exists and is correct if (\"data\" %in% ls() &amp;&amp; is.data.frame(data)) { # Running the assessment for each subject and storing results evaluation_results &lt;- do.call(rbind, apply(data, 1, evaluate_sepsis)) # Apply function row-wise # Append results to the original data frame data$result &lt;- evaluation_results$result data$total_score &lt;- evaluation_results$total_score data &lt;- data %&gt;% dplyr::select(subject_id, result, total_score, everything()) } else { print(\"Data object not found or is not a data frame.\") } . print(head(data)) . ## subject_id result total_score PaO2_FiO2 SpO2_FiO2 ## 1 1 Septic shock 10 200.65 183.55 ## 2 2 Sepsis 2 375.91 436.83 ## 3 3 Septic shock 7 243.14 310.48 ## 4 4 Septic shock 8 409.06 280.26 ## 5 5 Septic shock 7 429.16 240.90 ## 6 6 Sepsis 3 115.94 408.09 ## is_on_IMV lactate vasoactive_medications platelets INR ## 1 TRUE 1.57 2 137658 3.12 ## 2 FALSE 3.59 0 275363 3.20 ## 3 TRUE 1.52 3 88879 2.12 ## 4 FALSE 1.93 3 386975 2.74 ## 5 FALSE 1.28 2 384528 2.11 ## 6 FALSE 4.11 0 225491 3.47 ## d_dimer fibrinogen GCS pupils_reactive age_months ## 1 1.17 355.67 12 FALSE 199 ## 2 1.15 149.93 4 TRUE 26 ## 3 0.31 202.11 15 TRUE 209 ## 4 2.49 428.23 8 TRUE 175 ## 5 1.22 421.51 15 TRUE 37 ## 6 3.79 118.33 14 FALSE 32 . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#run-evaluation-and-return-result",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#run-evaluation-and-return-result"
  },"9": {
    "doc": "Sepsis score: Phoenix",
    "title": "Conclustion",
    "content": "For every patient (subject) the newly calculatied result columns provide the Phoenix score conclusions: result of sepsis, septic shock, or monitor and reassess and total_score of the value 0..n. Audomated classificaion algorithms may be used to provide a precision medicine approach to monitoring and treatment, either locally or as part of national registries. ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#conclustion",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#conclustion"
  },"10": {
    "doc": "Sepsis score: Phoenix",
    "title": "Abbreviations and definitions",
    "content": ". | FEU: Fibrinogen Equivalent Units | GCS: Glasgow Coma Scale | IMV: Invasive Mechanical Ventilation | INR: International Normalized Ratio of Prothrombin Time | MAP: Mean Arterial Pressure | PaO2:FIO2: Arterial Partial Pressure of Oxygen to Fraction of Inspired Oxygen Ratio - used to determine the degree of hypoxemia and acute respiratory distress syndrome (ARDS). | Phoenix Sepsis Criteria: A modern set of guidelines designed to assess sepsis with a focus on paediatric patients, emphasizing organ dysfunction. | Sepsis-1, Sepsis-2, Sepsis-3: These refer to the evolving definitions of sepsis over time, each marking a significant update in how sepsis is clinically identified and diagnosed based on symptoms and physiological responses. | SIRS: Systemic inflammatory response syndrome | SOFA: Sequential Organ Failure Assessment | SpO2: Oxygen Saturation Measured by Pulse Oximetry (only SpO2 of ≤97%) | . Criteria table from (Schlapbach et al. 2024). ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#abbreviations-and-definitions",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#abbreviations-and-definitions"
  },"11": {
    "doc": "Sepsis score: Phoenix",
    "title": "References",
    "content": "Bennett, Nicolas, Drago Plecko, and Ida-Fong Ukor. 2023. *Ricu: Intensive Care Unit Data with r*. Carlton, EF, RP Barbaro, TJ Iwashyna, and HC Prescott. 2019. “Cost of Pediatric Severe Sepsis Hospitalizations.” *JAMA Pediatrics* 173 (10): 986–87. &lt;https://doi.org/10.1001/jamapediatrics.2019.2570&gt;. DeWitt, Peter E, Seth Russell, Margaret N Rebull, L Nelson Sanchez-Pinto, and Tellen D Bennett. 2024. “Phoenix: An r Package and Python Module for Calculating the Phoenix Pediatric Sepsis Score and Criteria.” *JAMIA Open* 7 (3): ooae066. &lt;https://doi.org/10.1093/jamiaopen/ooae066&gt;. Gebara, BM. 2005. “Values for Systolic Blood Pressure.” *Pediatric Critical Care Medicine* 6 (4): 500. &lt;https://doi.org/10.1097/01.PCC.0000164344.07588.83&gt;. Goldstein, B, B Giroir, and A Randolph. 2005. “International Pediatric Sepsis Consensus Conference: Definitions for Sepsis and Organ Dysfunction in Pediatrics.” *Pediatric Critical Care Medicine* 6 (1): 2–8. &lt;https://doi.org/10.1097/01.PCC.0000149131.72248.E6&gt;. Levy, MM, MP Fink, JC Marshall, et al. 2003. “2001 SCCM/ESICM/ACCP/ATS/SIS International Sepsis Definitions Conference.” *Critical Care Medicine* 31 (4): 1250–56. &lt;https://doi.org/10.1097/01.CCM.0000050454.01978.3B&gt;. Reinhart, K, R Daniels, N Kissoon, FR Machado, RD Schachter, and S Finfer. 2017. “Recognizing Sepsis as a Global Health Priority—a WHO Resolution.” *New England Journal of Medicine* 377 (5): 414–17. &lt;https://doi.org/10.1056/NEJMp1707170&gt;. Schlapbach, Luregn J, R Scott Watson, Lauren R Sorce, Andrew C Argent, Kusum Menon, Mark W Hall, Samuel Akech, et al. 2024. “International Consensus Criteria for Pediatric Sepsis and Septic Shock.” *JAMA* 331 (8): 665–74. &lt;https://doi.org/10.1001/jama.2024.0179&gt;. Seymour, CW, VX Liu, TJ Iwashyna, et al. 2016. “Assessment of Clinical Criteria for Sepsis: For the Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3).” *JAMA* 315 (8): 762–74. &lt;https://doi.org/10.1001/jama.2016.0288&gt;. Singer, M, CS Deutschman, CW Seymour, et al. 2016. “The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3).” *JAMA* 315 (8): 801–10. &lt;https://doi.org/10.1001/jama.2016.0287&gt;. Souza, DC, JC Jaramillo-Bustamante, M Céspedes-Lesczinsky, et al. 2022. “Challenges and Health-Care Priorities for Reducing the Burden of Paediatric Sepsis in Latin America: A Call to Action.” *Lancet Child Adolesc Health* 6 (2): 129–36. &lt;https://doi.org/10.1016/S2352-4642(21)00341-2&gt;. ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html#references",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html#references"
  },"12": {
    "doc": "Sepsis score: Phoenix",
    "title": "Sepsis score: Phoenix",
    "content": "This doc was built with: rmarkdown::render(\"sepsis_score_logic_phoenix.Rmd\", output_file = \"../pages/phoenix_sepsis_score_logic.md\") . ",
    "url": "/pages/Phoenix_Sepsis_Score_logic.html",
    
    "relUrl": "/pages/Phoenix_Sepsis_Score_logic.html"
  },"13": {
    "doc": "ACAT",
    "title": "Aggregated Cauchy Association Test (ACAT)",
    "content": ". | Aggregated Cauchy Association Test (ACAT) . | Abbreviations | Intro to this topic | Papers | Step-by-step explanation of ACAT.R | Applying STAAR-O for multiple annotation weights | Non-gene-centric analysis using dynamic windows with SCANG-STAAR | Multi-weight annotation analysis | Main equations for ACAT | tan and \\(\\pi\\) | Original R code from yaowuliu | Example exercise . | Key Elements of the R Code: | . | R code examples | . | . ",
    "url": "/pages/acat.html#aggregated-cauchy-association-test-acat",
    
    "relUrl": "/pages/acat.html#aggregated-cauchy-association-test-acat"
  },"14": {
    "doc": "ACAT",
    "title": "Abbreviations",
    "content": ". | ACAT: Aggregated Cauchy Association Test | ACAT-V: Aggregated Cauchy Association Test - Variant level | ACAT-O: Aggregated Cauchy Association Test - Omnibus | SKAT: Sequence Kernel Association Test | ARIC: Atherosclerosis Risk in Communities | . ",
    "url": "/pages/acat.html#abbreviations",
    
    "relUrl": "/pages/acat.html#abbreviations"
  },"15": {
    "doc": "ACAT",
    "title": "Intro to this topic",
    "content": "The Aggregated Cauchy Association Test (ACAT) is a statistical method used for rare-variant association tests (RVATs) in genetic studies. ACAT is designed to aggregate the association signals of multiple rare genetic variants within a genomic region or a gene, while accounting for the directions of the effects of these variants on the phenotype of interest. The ACAT method utilizes a Cauchy distribution, which allows for improved performance in identifying true associations, especially when the directions and magnitudes of variant effects are heterogeneous. First, here is a great talk by author of SKAT and other methods: . Watch on YouTube Dr. Xihong Lin: Overview of Rare Variant Analysis of Whole Genome Sequencing Association Studies. | The major part starts at time: 22:30 | and the best is at: 30:50 where she describes the aggregated Cauchy association test (ACAT) method for combining multiple annotations (like CADD score, MAF, etc.) to calculate the final P-value. | This is their annotation database discussed: https://favor.genohub.org | . ",
    "url": "/pages/acat.html#intro-to-this-topic",
    
    "relUrl": "/pages/acat.html#intro-to-this-topic"
  },"16": {
    "doc": "ACAT",
    "title": "Papers",
    "content": ". | ACAT paper, Yaowu Liu, et al AJGH 2019. | Application of STAAR protocol to TOPMed, Xihao Li, et al. NatGen 2020. | STAAR pipeline methods paper, Zilin Li, et al. NatMethods 2022. | STAAR pipeline github | Controlling SKAT function: Here’s a summary of the SKAT package functions - which are easier to understand than reading the notation in the SKAT papers. If you read the code you see each new implement is added sequentially and how weights work. Although, the ACAT git repo is independent. | ACAT git repo: https://github.com/yaowuliu/ACAT/blob/master/R/ACAT.R | . ",
    "url": "/pages/acat.html#papers",
    
    "relUrl": "/pages/acat.html#papers"
  },"17": {
    "doc": "ACAT",
    "title": "Step-by-step explanation of ACAT.R",
    "content": "This discussion refers to code in the main ACAT function found at https://github.com/yaowuliu/ACAT. Figure 1. Summary of the Proposed Methods ACAT, ACAT-V, and ACAT-O and the Relationship Among Them. From the ACAT paper . | The R code defines several functions to perform the Aggregated Cauchy Association Test (ACAT) and the ACAT-V test. | ACAT: This function combines p-values using the Cauchy distribution. | ACAT_V: A set-based test that uses ACAT to combine the variant-level p-values. | NULL_Model: Computes model parameters and residuals for ACAT-V. | Get.marginal.pval: A helper function to calculate the marginal p-values for ACAT-V. | . | ACAT function . | a. It accepts Pvals (p-values), weights, and an optional is.check parameter to validate the input. | b. Checks for NA, p-value range (0 to 1), and existence of both 0 and 1 p-values in the same column. | c. If weights are not provided, equal weights are used. Otherwise, user-supplied weights are validated and standardized. | d. The function calculates the Cauchy statistics and returns the ACAT p-value(s). | . | ACAT_V function . | a. It accepts G (genotype matrix), obj (output object of NULL_Model), weights.beta, weights, and mac.thresh. | b. It checks for the validity of input weights. | c. Based on the mac.thresh value, it decides to use the Burden test, the Cauchy method, or a combination of both. | d. It calculates the final p-value and returns it. | . | NULL_Model function . | a. It accepts Y (outcome phenotypes) and Z (covariates). | b. It determines if Y is continuous or binary. | c. It fits a linear regression model if Y is continuous and a logistic model if Y is binary. | d. It returns an object with model parameters and residuals. | . | Get.marginal.pval function . | a. It accepts G (genotype matrix) and obj (output object of NULL_Model). | b. It checks the validity of the obj input. | c. It calculates the marginal p-values and returns them. | . | . The Aggregated Cauchy Association Test (ACAT) is a powerful and computationally efficient method designed to improve the analysis of rare and low-frequency genetic variants in sequencing studies. Traditional set-based tests can experience power loss when only a small proportion of variants are causal, and their power can be sensitive to factors such as the number, effect sizes, and effect directions of causal variants, as well as weight choices. ACAT addresses these issues by combining variant-level p-values to create a set-based test called ACAT-V. ACAT-V is particularly powerful when there are only a few causal variants in a set, making it a valuable tool for genetic analysis. Additionally, ACAT can be used to create an omnibus test called ACAT-O by combining different variant-set-level p-values. ACAT-O incorporates the strengths of multiple complementary set-based tests, such as the burden test, sequence kernel association test (SKAT), and ACAT-V. By analyzing extensive simulated data and real-world data from the Atherosclerosis Risk in Communities (ARIC) study, it has been demonstrated that ACAT-V complements other tests like SKAT and the burden test. Furthermore, ACAT-O consistently delivers more robust and higher power than alternative tests, making it a valuable addition to the toolkit of researchers working with sequencing studies. ACAT is designed to combine p-values from multiple variants or tests rather than combining annotation scores directly. If you have p-values associated with each of the 5 annotation columns (CADD_score, MAF, GnomAD_AF, REVEL_score, ClinVar_score) for a single variant, you could potentially use ACAT to combine these p-values to obtain a single combined p-value for that variant. However, it’s essential to ensure that the p-values are valid and independent for ACAT to be effective. To do this see the STAAR framework for this. Figure 2. Slide from presentation of ACAT method. ",
    "url": "/pages/acat.html#step-by-step-explanation-of-acatr",
    
    "relUrl": "/pages/acat.html#step-by-step-explanation-of-acatr"
  },"18": {
    "doc": "ACAT",
    "title": "Applying STAAR-O for multiple annotation weights",
    "content": "In a separate page, I discuss the STAAR method. The following passages are included in both pages since they related. In the STAAR Nature Methods paper, the section Gene-centric analysis of the noncoding genome shows how the STAAR method can indeed be used to capitalize on the ACAT method to obtain a combined p-value from a set of annotations for a single variant. The STAAR framework incorporates multiple functional annotation scores into the RVATs (rare-variant association tests) to increase the power of association analysis. In this context, it uses the STAAR-O test, an omnibus test that aggregates annotation-weighted burden test, SKAT, and ACAT-V within the STAAR framework. By incorporating multiple functional annotation scores, such as CADD, LINSIGHT, FATHMM-XF, and annotation principal components (aPCs), the STAAR method enhances the ability to detect associations between variants and traits of interest. Therefore, the STAAR framework can be used to leverage the strengths of the ACAT method and obtain a combined p-value from a set of annotations for a single variant or a set of variants. ",
    "url": "/pages/acat.html#applying-staar-o-for-multiple-annotation-weights",
    
    "relUrl": "/pages/acat.html#applying-staar-o-for-multiple-annotation-weights"
  },"19": {
    "doc": "ACAT",
    "title": "Non-gene-centric analysis using dynamic windows with SCANG-STAAR",
    "content": "The SCANG-STAAR method is an improvement over the fixed-size sliding window RVAT in the STAAR framework. It proposes a dynamic window-based approach called SCANG-STAAR, which extends the SCANG procedure by incorporating multidimensional functional annotations. This method allows for flexible detection of locations and sizes of signal windows across the genome, as the locations of regions associated with a disease or trait are often unknown in advance, and their sizes may vary across the genome. Using a prespecified fixed-size sliding window for RVAT can lead to power loss if the prespecified window sizes do not align with the true locations of the signals. The SCANG-STAAR method has two main procedures: SCANG-STAAR-S and SCANG-STAAR-B. SCANG-STAAR-S extends the SCANG-SKAT (SCANG-S) procedure by calculating the STAAR-SKAT (STAAR-S) p-value in each overlapping window by incorporating multiple variant functional annotations, instead of using just the MAF-weight-based SKAT p-value. SCANG-STAAR-B is based on the STAAR-Burden p-value. SCANG-STAAR-S has two advantages over SCANG-STAAR-B in detecting noncoding associations using dynamic windows: first, the effects of causal variants in a neighborhood in the noncoding genome tend to be in different directions, especially in intergenic regions; second, due to the different correlation structures of the two test statistics for overlapping windows, the genome-wide significance threshold of SCANG-STAAR-B is lower than that of SCANG-STAAR-S. SCANG-STAAR also provides the SCANG-STAAR-O procedure, based on an omnibus p-value of SCANG-STAAR-S and SCANG-STAAR-B calculated by the ACAT method. However, unlike STAAR-O, the ACAT-V test is not incorporated into the omnibus test because it is designed for sparse alternatives, and as a result, it tends to detect the region with the smallest size that contains the most significant variant in the dynamic window procedure. Figure 3. Slide from presentation of ACAT application in STAAR. ",
    "url": "/pages/acat.html#non-gene-centric-analysis-using-dynamic-windows-with-scang-staar",
    
    "relUrl": "/pages/acat.html#non-gene-centric-analysis-using-dynamic-windows-with-scang-staar"
  },"20": {
    "doc": "ACAT",
    "title": "Multi-weight annotation analysis",
    "content": "The STAAR framework can be used to combine the p-values associated with each of the 5 annotation columns (CADD_score, MAF, GnomAD_AF, REVEL_score, ClinVar_score) for a single variant. STAAR incorporates multiple functional annotation scores as weights when constructing its statistics, making it suitable for combining p-values from different annotation columns to obtain a single combined p-value for that variant. ",
    "url": "/pages/acat.html#multi-weight-annotation-analysis",
    
    "relUrl": "/pages/acat.html#multi-weight-annotation-analysis"
  },"21": {
    "doc": "ACAT",
    "title": "Main equations for ACAT",
    "content": ". | ACAT test statistic: | . \\[T_{ACAT} = \\sum_{i=1}^{k} w_i \\tan{[(0.5 - p_i)\\pi]}\\] where \\(p_i\\) are the p-values, and \\(w_i\\) are non-negative weights. | P-value calculation for the ACAT test statistic: | . \\[p \\text{-value} \\approx 1 - \\frac{1}{2} + \\frac{\\arctan{(T_{ACAT} / w)}}{\\pi}\\] where \\(w = \\sum_{i=1}^{k} w_i\\). | ACAT is a general and flexible method of combining p-values, which can represent the statistical significance of different kinds of genetic variations in sequencing studies. | ACAT only aggregates p-values, so one can automatically control cryptic relatedness and/or population stratification by fitting appropriate models from which p-values are calculated through methods such as principal-component analysis or mixed models. | The null distribution of the test statistic \\(T_{ACAT}\\) can be well approximated by a Cauchy distribution without the need for estimating and accounting for the correlation among p-values. | Calculating the p-value of ACAT requires almost negligible computation and is extremely fast. | The approximation is particularly accurate when ACAT has a very small p-value, which is useful in sequencing studies because only very small p-values can pass the stringent genome-wide significance threshold and are of particular interest. | . ",
    "url": "/pages/acat.html#main-equations-for-acat",
    
    "relUrl": "/pages/acat.html#main-equations-for-acat"
  },"22": {
    "doc": "ACAT",
    "title": "tan and \\(\\pi\\)",
    "content": "In the ACAT method, the “tan” and “π” functions are used to transform the p-values in such a way that they follow a standard Cauchy distribution under the null hypothesis. This transformation is essential to the ACAT method because it allows for an efficient and accurate combination of p-values, even when they are correlated. The reason for using the tangent function (“tan”) specifically is because of its connection to the Cauchy distribution. The Cauchy distribution has some unique properties, such as having a heavy tail, which make it suitable for handling correlated p-values in this context. The transformation function used in the ACAT method, given by \\(tan((0.5 - p_i) \\pi)\\), ensures that if the p-value \\(p_i\\) is from the null distribution, the transformed value will follow a standard Cauchy distribution. The constant \\(\\pi\\) (Pi) is used in the formula because it is a natural component of the tangent function. In the context of the ACAT method, \\(\\pi\\) is used to scale the input of the tangent function, which is necessary to map the range of p-values (0 to 1) to the entire domain of the tangent function. This ensures that the transformed values will follow the desired Cauchy distribution. Therefore, the “tan” and \\(\\pi\\) functions in the ACAT method are used to transform p-values so that they follow a standard Cauchy distribution under the null hypothesis, which allows for an efficient and accurate combination of correlated p-values. ",
    "url": "/pages/acat.html#tan-and-pi",
    
    "relUrl": "/pages/acat.html#tan-and-pi"
  },"23": {
    "doc": "ACAT",
    "title": "Original R code from yaowuliu",
    "content": "This code is the main ACAT function found at https://github.com/yaowuliu/ACAT . ",
    "url": "/pages/acat.html#original-r-code-from-yaowuliu",
    
    "relUrl": "/pages/acat.html#original-r-code-from-yaowuliu"
  },"24": {
    "doc": "ACAT",
    "title": "Example exercise",
    "content": "The following R code demonstrates the use of ACAT on simulation data. This method aggregates multiple rare genetic variant signals within a genomic region or gene, accounting for the heterogeneous directions and magnitudes of variant effects on the phenotype. Additionally, it incorporates GTEx data in simulations to enhance realism and applicability in genetic research. Key Elements of the R Code: . | Loading Necessary Libraries: The script loads R packages like ggplot2, dplyr, reshape2, and ACAT for data manipulation, statistical analysis, and visualization. | Simulating Data: It simulates genetic data, including phenotypes, genotypes, and annotations such as CADD scores, allele frequencies, and pathogenicity scores, setting a realistic backdrop for applying the ACAT method. | Generating P-values with Outliers: A function generate_pvalues_with_outliers produces example p-values with intentional outliers, this will be necessary to show ACAT’s ability to handle diverse data distributions. | Data Aggregation and Visualization: Post-simulation, the script aggregates and visualizes the data, highlighting the distribution of original scores and p-values through histograms and scatter plots. | ACAT Function Application: At its core, the script applies the ACAT function to the simulated p-values. This function uses the Cauchy distribution to combine p-values across multiple tests or variants, aiding in identifying genuine genetic associations. | Incorporating GTEx Data: The code uses real data for GTEx gene expression to provide a context for ACAT application, more similar to real-world genetic analysis. | Analysis and interpretation: The script includes modeling steps with limma. This is used for gene expression data, especially the use of linear models for analysing designed experiments and the assessment of differential expression. | . This R code example allows us to observe the ACAT method’s with simple data. The visual outputs illustrate the impact of different data distributions on ACAT results, demonstrating its effectiveness in genetic association studies, especially those focusing on rare variants. ",
    "url": "/pages/acat.html#example-exercise",
    
    "relUrl": "/pages/acat.html#example-exercise"
  },"25": {
    "doc": "ACAT",
    "title": "R code examples",
    "content": "# Loading necessary libraries library(ggplot2) library(reshape2) library(dplyr) library(scales) library(ACAT) library(EnhancedVolcano) library(gridExtra) library(limma) library(reshape2) # library(devtools) # devtools::install_github(\"yaowuliu/ACAT\") # if (!requireNamespace('BiocManager', quietly = TRUE)) # install.packages('BiocManager') # BiocManager::install('limma') # Set up example test data ---- # Set seed for reproducibility set.seed(123) # Number of variants n_variants &lt;- 50 # Simulate Associated Variables CADD_score &lt;- runif(n_variants, 1, 20) # Score can be any value allele_frequency &lt;- runif(n_variants, 0, 1) # Score can be any value pathogenicity &lt;- rbinom(n_variants, 1, 0.5) # Score can be any value # Simulate P-values with a few outliers generate_pvalues_with_outliers &lt;- function(n, outlier_indices) { p_values &lt;- runif(n, 0, 1) # Assigning very small values to selected outliers p_values[outlier_indices] &lt;- runif(length(outlier_indices), 0, 0.001) return(p_values) } # Indices of potential outliers outlier_indices &lt;- sample(1:n_variants, 2) # Randomly select 2 variants to be outliers # Generate P-values CADD_score_pval &lt;- generate_pvalues_with_outliers(n_variants, outlier_indices) allele_frequency_pval &lt;- generate_pvalues_with_outliers(n_variants, outlier_indices) pathogenicity_pval &lt;- generate_pvalues_with_outliers(n_variants, outlier_indices) # Combine variables into a single dataframe for visualization variant_data &lt;- data.frame( variant = rep(1:n_variants, times = 6), measure = rep(c( \"CADD_score\", \"allele_frequency\", \"pathogenicity\" ), each = n_variants * 2), type = rep(c( rep(\"Original Score\", n_variants), rep(\"P-Value\", n_variants) ), times = 3), value = c( CADD_score, CADD_score_pval, allele_frequency, allele_frequency_pval, pathogenicity, pathogenicity_pval ) # Alternating Original Scores and P-values for each measure ) # Melt the data for ggplot variant_data_melted &lt;- melt(variant_data, id.vars = c('variant', 'measure', 'type')) # Calculate the significance threshold for P-values (not log-transformed) significance_threshold &lt;- 0.05 / n_variants # Separate the data into two parts: one for original scores and one for P-values original_scores_data &lt;- variant_data_melted %&gt;% filter(type == \"Original Score\") p_values_data &lt;- variant_data_melted %&gt;% filter(type == \"P-Value\") # Plot for original scores p_original_scores &lt;- ggplot(original_scores_data, aes(x = variant, y = value, color = measure)) + geom_point() + theme_classic() + labs(title = \"Original Scores\", y = \"Values\", color = \"Variables\") # Plot for P-values with log scale p_p_values &lt;- ggplot(p_values_data, aes( x = variant, y = -log10(value), color = measure )) + geom_point() + theme_classic() + labs(title = \"P-values\", y = \"-log10(P-values)\") + geom_hline(aes(yintercept = -log10(significance_threshold)), linetype = \"dashed\", color = \"red\") # Combine the two plots p_combined &lt;- grid.arrange(p_original_scores, p_p_values, ncol = 1) print(p_combined) # Save the combined plot (optional) # ggsave(\"./images/p_combined.png\", plot = p_combined) #, width = 8, height = 12) . # ACAT library ---- # Assuming variant_data_melted is the dataframe created in your previous code # Filter the data to include only P-values p_values_data &lt;- variant_data_melted %&gt;% filter(type == \"P-Value\") %&gt;% mutate(log_p_value = -log10(value)) # Plot the histogram of P-values p_histogram &lt;- ggplot(p_values_data, aes(x = value)) + geom_histogram(bins = 30, fill = \"blue\", alpha = 0.7) + theme_minimal() + labs(title = \"Histogram of P-values for Variants\", x = \"P-values\", y = \"Frequency\") # Display the histogram print(p_histogram) # QQ-Plot of P-values p_qqplot &lt;- ggplot(p_values_data, aes(sample = value)) + geom_qq() + geom_qq_line() + theme_minimal() + labs(title = \"QQ-Plot of P-values for Variants\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") # Display the QQ-plot print(p_qqplot) # Combine the two plots p_hist_qq &lt;- grid.arrange(p_histogram, p_qqplot, ncol = 1) print(p_hist_qq) # ggsave(\"./images/p_hist_qq.png\", plot = p_hist_qq) . # Group by variant and apply ACAT p_values_per_variant &lt;- p_values_data %&gt;% group_by(variant) %&gt;% summarize(aggregated_pval = ACAT(Pvals = value)) # Apply ACAT to aggregated P-values for each variant acat_results_per_variant &lt;- apply(p_values_per_variant[, -1], 1, function(pvals) ACAT(pvals)) # Prepare data for visualization acat_results_df &lt;- data.frame(Variant = p_values_per_variant$variant, ACAT_P_Value = acat_results_per_variant) # Plot ACAT results for each variant p_acat &lt;- ggplot(acat_results_df, aes(x = Variant, y = -log10(ACAT_P_Value))) + geom_point() + theme_minimal() + labs(title = \"ACAT P-values for Each Variant\", y = \"-log10(ACAT P-value)\", x = \"Variant\") + geom_hline(aes(yintercept = -log10(significance_threshold)), linetype = \"dashed\", color = \"red\") p_acat # ggsave(\"./images/p_acat.png\", plot = p_acat) . # GTEx ---- # GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz Median gene-level TPM by tissue. Median expression was calculated from the file GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_tpm.gct.gz. # https://storage.googleapis.com/adult-gtex/bulk-gex/v8/rna-seq/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz tmp &lt;- read.csv( file = \"./data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct\", sep = \"\\t\", comment.char = \"#\", skip = 2 ) # Adjust this number based on whether you count the header line or not # Calculate the row means, excluding the first two metadata columns tmp$tissue_mean &lt;- rowMeans(tmp[, -c(1, 2)]) tmp &lt;- tmp |&gt; dplyr::select(\"Name\", \"Description\", \"tissue_mean\") names(tmp) # Assuming tmp has been read and tissue_mean calculated as before set.seed(123) # For reproducibility # Standard deviation assumption (modify as needed) std_dev_assumption &lt;- 1 # Simulating 10 cases and 10 controls for each gene n_samples &lt;- 10 # Simulating data using a Poisson distribution # The lambda parameter is the mean for each gene tmp$Cases &lt;- replicate(n_samples, rpois(nrow(tmp), lambda = tmp$tissue_mean)) tmp$Controls &lt;- replicate(n_samples, rpois(nrow(tmp), lambda = tmp$tissue_mean)) # Assuming tmp and n_samples are defined as before # Choose a number of genes to modify n_genes_to_modify &lt;- 10 # Randomly select genes selected_genes &lt;- sample(1:nrow(tmp), n_genes_to_modify) # Modify the expression values for these genes # Here, we'll increase the expression values for cases # You can choose a multiplication factor or an additive factor modification_factor &lt;- 10 for (gene in selected_genes) { tmp$Cases[gene, ] &lt;- tmp$Cases[gene, ] * modification_factor } # Reshaping the data for limma analysis case_data &lt;- as.data.frame(tmp$Cases) control_data &lt;- as.data.frame(tmp$Controls) colnames(case_data) &lt;- paste(\"Case\", 1:n_samples, sep = \"_\") colnames(control_data) &lt;- paste(\"Control\", 1:n_samples, sep = \"_\") combined_data &lt;- cbind(case_data, control_data) # Reshape combined_data for ggplot long_combined_data &lt;- melt(combined_data) long_combined_data$group &lt;- ifelse(grepl(\"Case\", long_combined_data$variable), \"Case\", \"Control\") # Create the plot ggplot(long_combined_data, aes(x = log(value), fill = group)) + geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) + labs(x = \"Expression Level\", y = \"Frequency\", title = \"Distribution of Simulated Expression Levels\") + theme_minimal() + scale_fill_manual(values = c(\"Case\" = \"blue\", \"Control\" = \"red\")) # p_gtex_gene_median_tpm_histogram # Creating a design matrix for the differential expression analysis group &lt;- factor(c(rep(\"Case\", n_samples), rep(\"Control\", n_samples))) design &lt;- model.matrix( ~ group) # Applying voom transformation v &lt;- voom(combined_data, design) # Fit the linear model fit &lt;- lmFit(v, design) # Apply empirical Bayes moderation fit &lt;- eBayes(fit) # Extract differentially expressed genes results &lt;- topTable(fit) # results &lt;- topTable(fit, adjust.method = \"bonferroni\") results . |   | logFC | AveExpr | t | P.Value | adj.P.Val | B | . | 21692 | -3.140047 | 4.68799 | -14.06914 | 2.764495e-11 | 0.0000016 | 15.700482 | . | 43000 | -3.425020 | 3.53374 | -6.37637 | 4.769799e-06 | 0.1340314 | 3.563774 | . | 30156 | -1.182569 | -0.30486 | -5.73847 | 1.785158e-05 | 0.3344196 | 2.463673 | . | 34017 | -1.562842 | 0.20193 | -5.27569 | 4.798450e-05 | 0.6594008 | 1.480612 | . | 9603 | -1.070434 | 2.41423 | -5.00915 | 8.571267e-05 | 0.6594008 | 0.751640 | . | 53242 | -1.591902 | 0.83988 | -4.85549 | 1.201280e-04 | 0.6594008 | 0.657138 | . | 51799 | 0.951387 | -0.42095 | 4.74540 | 1.531894e-04 | 0.6594008 | 0.253207 | . | 33526 | 0.951487 | -0.42095 | 4.74485 | 1.533751e-04 | 0.6594008 | 0.251985 | . | 17397 | -0.950057 | -0.42095 | -4.74068 | 1.547968e-04 | 0.6594008 | 0.242677 | . | 15000 | -0.950611 | -0.42095 | -4.73765 | 1.558397e-04 | 0.6594008 | 0.235904 | . # Assuming results has columns like \"logFC\" (log fold change), \"P.Value\", and \"adj.P.Val\" ggplot(results, aes( x = logFC, y = -log10(P.Value), colour = adj.P.Val &lt; 0.05 )) + geom_point(alpha = 0.5) + geom_jitter(alpha = 0.5) + scale_colour_manual(values = c(\"FALSE\" = \"grey\", \"TRUE\" = \"red\")) + labs(x = \"Log Fold Change\", y = \"-log10 P-value\", title = \"Volcano Plot of Differential Expression Results\") + theme_minimal() # names(fit) # head(fit$coefficients) # head(fit$p.value) # Extract logFC, corresponding to your condition of interest (here assumed as \"groupControl\") fit_results &lt;- as.data.frame(fit$coefficients) # Ensure the rows in tmp align with those in fit_results fit_results$GeneDescription &lt;- tmp$Description fit_results$logFC &lt;- fit_results[, \"groupControl\"] # Replace with the correct column name as per your design # Extract p-values and apply adjustment fit_results$p_value &lt;- fit$p.value[, \"groupControl\"] # Corresponding p-values fit_results$adjPVal &lt;- p.adjust(fit_results$p_value, method = \"BH\") # Adjust p-values # Enhanced analysis volcano plot ---- # Extracting necessary data from the fit object volcano_data &lt;- data.frame(logFC = fit$coefficients[, \"groupControl\"], PValue = fit$p.value[, \"groupControl\"]) # Add gene descriptions from tmp to volcano_data volcano_data$GeneDescription &lt;- tmp$Description EnhancedVolcano( volcano_data, lab = volcano_data$GeneDescription, x = 'logFC', y = 'PValue', pCutoff = 0.05, FCcutoff = 1.5, # Adjust fold change cutoff as appropriate title = 'Volcano Plot', subtitle = 'Differential Expression' ) # p_gtex_gene_median_tpm_volcano # 8x6 png . # FC weights ---- # To convert the p-values from your differential expression analysis into weights scaled between 0 and 1, you can invert the p-values and then normalize them so that they sum up to 1. This approach assigns higher weights to genes with lower p-values (indicating higher significance), which aligns with your goal of giving more weight to genes more likely to be causal. # Apply negative log10 transformation fit_results$NegLogPValue &lt;- -log10(fit_results$p_value) # Normalize the negative log p-values fit_results$ScaledWeights &lt;- fit_results$NegLogPValue / max(fit_results$NegLogPValue) # Histogram of Scaled Weights ggplot(fit_results, aes(x = ScaledWeights)) + geom_histogram(binwidth = 0.01, fill = \"blue\", color = \"black\") + labs(x = \"Scaled Weights\", y = \"Frequency\", title = \"Distribution of Scaled Weights\") + theme_minimal() # p_gtex_gene_median_tpm_scaledweight . # Weights in ACAT ---- # we want the top ten volcano_data$GeneDescription ranked on fit_results$ScaledWeights, which we can then use the top ten names from volcano_data$GeneDescription to pretend that they consist of 50 variants from the df variant_data_melted. the variant_data_melted$variants are numbered 1-50 so lets give 10 variants to each one volcano_data$GeneDescription (gene name). # we can then merge those two dataframes on GeneDescription. # Add ScaledWeights to volcano_data volcano_data$ScaledWeights &lt;- fit_results$ScaledWeights[match(volcano_data$GeneDescription, fit_results$GeneDescription)] # Rank and select the top 10 genes top_genes &lt;- head(volcano_data[order(-volcano_data$ScaledWeights), \"GeneDescription\"], 10) # Create a dataframe variants_to_genes &lt;- data.frame( GeneDescription = rep(top_genes, each = 5), variant = rep(1:50, length.out = length(top_genes) * 5) ) # Merge the dataframes merged_data &lt;- merge(volcano_data, variants_to_genes, by = \"GeneDescription\") # Merge with variant_data_melted variant_data_melted_gtex &lt;- merge(merged_data, variant_data_melted, by = \"variant\") # ACAT GTEx ---- # Filter the data to include only P-values p_values_data &lt;- variant_data_melted_gtex %&gt;% filter(type == \"P-Value\") %&gt;% mutate(log_p_value = -log10(value)) # Group by variant and apply ACAT p_values_per_variant &lt;- p_values_data %&gt;% group_by(variant) %&gt;% summarize(aggregated_pval = ACAT(Pvals = value)) p_values_per_variant_weight &lt;- p_values_data %&gt;% group_by(variant) %&gt;% summarize(aggregated_pval = ACAT(Pvals = value, weight = ScaledWeights)) # Test weights ---- # Transform and scale p-values into weights p_values_data &lt;- p_values_data %&gt;% mutate( LogWeight = -log10(value), ScaledWeight = (LogWeight - min(LogWeight)) / (max(LogWeight) - min(LogWeight)) ) # Group by variant and apply ACAT with new ScaledWeights p_values_per_variant_weight &lt;- p_values_data %&gt;% group_by(variant) %&gt;% summarize(aggregated_pval = ACAT(Pvals = value, weight = ScaledWeight)) # Prepare data for the first plot (without weights) acat_results_df &lt;- data.frame( Variant = p_values_per_variant$variant, ACAT_P_Value = p_values_per_variant$aggregated_pval, GTEx_DE_weight = \"unweighted\" ) # Prepare data for the second plot (with weights) acat_results_df_weighted &lt;- data.frame( Variant = p_values_per_variant_weight$variant, ACAT_P_Value = p_values_per_variant_weight$aggregated_pval, GTEx_DE_weight = \"GTEx weighted\" ) acat_results &lt;- rbind(acat_results_df, acat_results_df_weighted) p_acat_weight_GTEx &lt;- acat_results |&gt; ggplot(aes(x = Variant, y = -log10(ACAT_P_Value))) + geom_point(aes(color = GTEx_DE_weight)) + theme_minimal() + labs(title = \"GTEx weight in ACAT\", y = \"-log10(ACAT P-value)\", x = \"Variant\") + geom_hline(aes(yintercept = -log10(significance_threshold)), linetype = \"dashed\", color = \"red\") p_acat_weight_GTEx # Save the combined plot (optional) # ggsave(\"./images/p_acat_weight_GTEx_test.png\", plot = p_acat_weight_GTEx) . ",
    "url": "/pages/acat.html#r-code-examples",
    
    "relUrl": "/pages/acat.html#r-code-examples"
  },"26": {
    "doc": "ACAT",
    "title": "ACAT",
    "content": "Last update: 20230425 . ",
    "url": "/pages/acat.html",
    
    "relUrl": "/pages/acat.html"
  },"27": {
    "doc": "ACMG criteria",
    "title": "Interpretation of variants by ACMG standards and guidelines",
    "content": " ",
    "url": "/pages/acmg_criteria_table_main.html#interpretation-of-variants-by-acmg-standards-and-guidelines",
    
    "relUrl": "/pages/acmg_criteria_table_main.html#interpretation-of-variants-by-acmg-standards-and-guidelines"
  },"28": {
    "doc": "ACMG criteria",
    "title": "Classification criteria",
    "content": "Extensive annotation is applied during our genomics analysis. Interpretation of genetic determinants of disease is based on many evidence sources. One important source of interpretation comes from the Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology, Richards et al. 1. See also Li et al., 2017 2 and Riggs et al., 2020 3. The following tables are provided as they appear in the initial steps of our filtering protocol for the addition of ACMG-standardised labels to candidate causal variants. For reference, alternative public implementations of ACMG guidelines can be found in Li et al., 2017 4 and Xavier et al., 2019 5; please note these tools have not implemented here nor is any assertion of their quality offered. Examples of effective variant filtering and expected candidate variant yield in studies of rare human disease are provided by Pedersen et al., 2021 6. Main criteria for classifications . | Evidence type | label | Evidence | Manual adjustment | ACGM label | Caveat checks | Criteria | . | pathogenicity | VS1 | very_strong |   | PVS1 | 4 | null variant (nonsense, frameshift, canonical +- 2 splice sites, initiation codon, single or multiexon deletion) in a gene where LOF is a known mechanism of disease | . | pathogenicity | S1 | strong |   | PS1 | 1 | Same amino acid change as a previously established pathogenic variant regardless of nucleotide change | . | pathogenicity | S2 | strong | manual | PS2 | 1 | De novo (both maternity and paternity confirmed) in a patient with the disease and no family history | . | pathogenicity | S3 | strong | manual | PS3 | 1 | Well-established in vitro or in vivo functional studies supportive of a damaging effect on the gene or gene product | . | pathogenicity | S4 | strong |   | PS4 | 2 | The prevalence of the variant in affected individuals is significantly increased compared with the prevalence in controls | . | pathogenicity | S5 | other |   | PS5 |   | The user has additional (value) strong pathogenic evidence | . | pathogenicity | M1 | moderate |   | PM1 |   | Located in a mutational hot spot and/or critical and well-established functional domain (e.g., active site of an enzyme) without benign variation | . | pathogenicity | M2 | moderate |   | PM2 | 1 | Absent from controls (or at extremely low frequency if recessive) in Exome Sequencing Project, 1000 Genomes Project, or Exome Aggregation Consortium | . | pathogenicity | M3 | moderate | manual | PM3 | 1 | For recessive disorders, detected in trans with a pathogenic variant | . | pathogenicity | M4 | moderate |   | PM4 |   | Protein length changes as a result of in-frame deletions/insertions in a nonrepeat region or stop-loss variants | . | pathogenicity | M5 | moderate |   | PM5 | 1 | Novel missense change at an amino acid residue where a different missense change determined to be pathogenic has been seen before | . | pathogenicity | M6 | moderate |   | PM6 |   | Assumed de novo, but without confirmation of paternity and maternity | . | pathogenicity | M7 | moderate | other | PM7 |   | The user has additional (value) moderate pathogenic evidence | . | pathogenicity | P1 | supporting | manual | PP1 |   | Cosegregation with disease in multiple affected family members in a gene definitively known to cause the disease | . | pathogenicity | P2 | supporting |   | PP2 |   | Missense variant in a gene that has a low rate of benign missense variation and in which missense variants are a common mechanism of disease | . | pathogenicity | P3 | supporting |   | PP3 | 1 | Multiple lines of computational evidence support a deleterious effect on the gene or gene product (conservation, evolutionary, splicing impact, etc.) | . | pathogenicity | P4 | supporting | manual | PP4 |   | Patient’s phenotype or family history is highly specific for a disease with a single genetic etiology | . | pathogenicity | P5 | supporting |   | PP5 |   | Reputable source recently reports variant as pathogenic, but the evidence is not available to the laboratory to perform an independent evaluation | . | pathogenicity | P6 | supporting | other | PP6 |   | The user has additional (value) supporting pathogenic evidence | . | benign | A1 | stand_alone |   | BA1 |   | Allele frequency is &gt;5% in Exome Sequencing Project, 1000 Genomes Project, or Exome Aggregation Consortium | . | benign | S1 | strong |   | BS1 |   | Allele frequency is greater than expected for disorder | . | benign | S2 | strong |   | BS2 |   | Observed in a healthy adult individual for a recessive (homozygous), dominant (heterozygous), or X-linked (hemizygous) disorder, with full penetrance expected at an early age | . | benign | S3 | strong | manual | BS3 |   | Well-established in vitro or in vivo functional studies show no damaging effect on protein function or splicing | . | benign | S4 | strong | manual | BS4 | 1 | Lack of segregation in affected members of a family | . | benign | S5 | strong | other | BS5 |   | The user has additional (value) strong benign evidence | . | benign | P1 | supporting |   | BP1 |   | Missense variant in a gene for which primarily truncating variants are known to cause disease | . | benign | P2 | supporting | manual | BP2 |   | Observed in trans with a pathogenic variant for a fully penetrant dominant gene/disorder or observed in cis with a pathogenic variant in any inheritance pattern | . | benign | P3 | supporting |   | BP3 |   | In-frame deletions/insertions in a repetitive region without a known function | . | benign | P4 | supporting |   | BP4 | 1 | Multiple lines of computational evidence suggest no impact on gene or gene product (conservation, evolutionary, splicing impact, etc.) | . | benign | P5 | supporting | manual | BP5 |   | Variant found in a case with an alternate molecular basis for disease | . | benign | P6 | supporting |   | BP6 |   | Reputable source recently reports variant as benign, but the evidence is not available to the laboratory to perform an independent evaluation | . | benign | P7 | supporting |   | BP7 |   | A synonymous (silent) variant for which splicing prediction algorithms predict no impact to the splice consensus sequence nor the creation of a new splice site AND the nucleotide is not highly conserved | . | benign | P8 | supporting | other | BP8 |   | The user has additional (value) supporting benign evidence | . Caveats implementing filters . Implementing the guidelines for interpretation of annotation requires multiple programmatic steps. The number of individual caveat checks indicate the number of bioinformatic filter functions used. Unnumbered caveat checks indicate that only a single filter function is required during reference to annotation databases. However, each function depends on reference to either one or several evidence source databases (approximately 150 sources) which are not shown here. | Evidence type | label | Evidence | Manual adjustment | ACGM label | Caveat number | Caveat | . | pathogenicity | VS1 | very_strong |   | PVS1 | 1 | LoF not disease causing for gene | . | pathogenicity | VS1 | very_strong |   | PVS1 | 2 | LoF at 3prime end (loftee) | . | pathogenicity | VS1 | very_strong |   | PVS1 | 3 | exon skipping splce that leave functional protein | . | pathogenicity | VS1 | very_strong |   | PVS1 | 4 | multiple transcript check | . | pathogenicity | S1 | strong |   | PS1 | 1 | Assess for splicing vs amino acid change | . | pathogenicity | S2 | strong | manual | PS2 | 1 | “Do not assume maternity or paternity i.e. egg donor, surrogate” | . | pathogenicity | S3 | strong | manual | PS3 | 1 | Quality of functional evidence | . | pathogenicity | S4 | strong |   | PS4 | 1 | “Assess RR, OR, CI for case/control evidence” | . | pathogenicity | S4 | strong |   | PS4 | 2 | Rare variant in multiple indipendent cases can guide | . | pathogenicity | S5 | other |   | PS5 |   |   | . | pathogenicity | M1 | moderate |   | PM1 |   |   | . | pathogenicity | M2 | moderate |   | PM2 | 1 | Indels in population reference may not match your protocol | . | pathogenicity | M3 | moderate | manual | PM3 | 1 | Pedigree sequencing is ideal. Phasing (GATK) may guide. Ldlink may guide | . | pathogenicity | M4 | moderate |   | PM4 |   |   | . | pathogenicity | M5 | moderate |   | PM5 | 1 | Assess for splicing vs amino acid change | . | pathogenicity | M6 | moderate |   | PM6 |   |   | . | pathogenicity | M7 | moderate | other | PM7 |   |   | . | pathogenicity | P1 | supporting | manual | PP1 |   |   | . | pathogenicity | P2 | supporting |   | PP2 |   |   | . | pathogenicity | P3 | supporting |   | PP3 | 1 | “Prediction methods may have used the same data sources, do not assume each as independent evidence” | . | pathogenicity | P4 | supporting | manual | PP4 |   |   | . | pathogenicity | P5 | supporting |   | PP5 |   |   | . | pathogenicity | P6 | supporting | other | PP6 |   |   | . | benign | A1 | stand_alone |   | BA1 |   |   | . | benign | S1 | strong |   | BS1 |   |   | . | benign | S2 | strong |   | BS2 |   |   | . | benign | S3 | strong | manual | BS3 |   |   | . | benign | S4 | strong | manual | BS4 | 1 | Question the accuracy of segregation. Presence of phenocopies can mimic lack of segregation. Additional pathogenic variants may produce autosomal dominant pattern. | . | benign | S5 | strong | other | BS5 |   |   | . | benign | P1 | supporting |   | BP1 |   |   | . | benign | P2 | supporting | manual | BP2 |   |   | . | benign | P3 | supporting |   | BP3 |   |   | . | benign | P4 | supporting |   | BP4 | 1 | “Prediction methods may have used the same data sources, do not assume each as independent evidence” | . | benign | P5 | supporting | manual | BP5 |   |   | . | benign | P6 | supporting |   | BP6 |   |   | . | benign | P7 | supporting |   | BP7 |   |   | . | benign | P8 | supporting | other | BP8 |   |   | . ",
    "url": "/pages/acmg_criteria_table_main.html#classification-criteria",
    
    "relUrl": "/pages/acmg_criteria_table_main.html#classification-criteria"
  },"29": {
    "doc": "ACMG criteria",
    "title": "Scoring point system",
    "content": "Table 3 from Tavtigian et al. 2020 table 2 7: Point values for ACMG/AMP strength of evidence categories . | Evidence | Point scale | Pathogenic | Benign | . | Indeterminate | 0 | 0 | 0a | . | Supporting | 1 | 1 | -1 | . | Moderate | 2 | 2 | -2b | . | Strong | 4 | 4 | -4 | . | Very strong | 8 | 8 | -8b | . a Note is made that Richards et al. did not specifically recognize indeterminate evidence. Nonetheless, if one thinks of the odds in favor of pathogenicity as a continuous variable, there exists a range that falls between Supporting Benign and Supporting Pathogenic. This is Indeterminate. b Note is also made that Richards et al. did not specify benign evidence at the moderate or very strong levels. Nevertheless, the point system would readily support the addition of such criteria. Table 4. from Tavtigian et al. 2020 table 2 7: Point-based variant classification categories . | Category | Point ranges | Notes | . | Pathogenic | ≥10 |   | . | Likely Pathogenic | 6 to 9a |   | . | Uncertain | 0 to 5 |   | . | Likely Benign | −1 to −6a |   | . | Benign | ≤ −7 |   | . a Operationally, the prior probability should be understood to be infinitesimally &gt;0.10. This has two effects. First, it makes the posterior probability of the American College of Medical Genetics (ACMG) Likely Pathogenic combining rules infinitesimally greater than 0.90, so that the Likely Pathogenic rules work properly. A specific value of 0.102 would have the added benefit that seven points would meet the IARC (International Agency for Research on Cancer) Likely Pathogenic threshold of 0.95. Second, it enforces a requirement for some evidence of benign effect for sequence variants to be classified as Likely Benign. One could also argue that the point threshold for Likely Benign should really be −2. This would match the ACMG rule “Likely Benign (ii)” rather than the simple numerical requirement that the posterior probability be &lt;0.10. For reference, alternative public implementations of ACMG guidelines can be found in Li &amp; Wang, 2017 and Xavier et al., 2019; please note these tools have not been implemented here nor is any assertion of their quality offered. Examples of effective variant filtering and expected candidate variant yield in studies of rare human disease are provided by Pedersen et al., 2021. We use the evaluation of in silico predictors from Varsome. However, this paper discusses it also Wilcox et al., 2022 8. ",
    "url": "/pages/acmg_criteria_table_main.html#scoring-point-system",
    
    "relUrl": "/pages/acmg_criteria_table_main.html#scoring-point-system"
  },"30": {
    "doc": "ACMG criteria",
    "title": "ACMGuru code example",
    "content": "The code from ACMGuru runs these runs using a range of functions and then performs a final tally. Some excerts from the code are shown here: . # acmg_filters ---- # PVS1 ---- # PVS1 are null variants where IMPACT==\"HIGH\" and inheritance match, # in gene where LoF cause disease. df$ACMG_PVS1 &lt;- NA df &lt;- df %&gt;% dplyr::select(ACMG_PVS1, everything()) df$ACMG_PVS1 &lt;- ifelse(df$IMPACT == \"HIGH\" &amp; df$genotype == 2, \"PVS1\", NA) # homozygous df$ACMG_PVS1 &lt;- ifelse(df$IMPACT == \"HIGH\" &amp; df$Inheritance == \"AD\", \"PVS1\", df$ACMG_PVS1) # dominant # All functions for classification ... # acmg tally ---- # List of all ACMG labels # acmg_labels &lt;- c(\"ACMG_PVS1\", \"ACMG_PS1\", \"ACMG_PS2\", \"ACMG_PS3\", \"ACMG_PS4\", \"ACMG_PS5\", \"ACMG_PM1\", \"ACMG_PM2\", \"ACMG_PM3\", \"ACMG_PM4\", \"ACMG_PM5\", \"ACMG_PM6\", \"ACMG_PM7\", \"ACMG_PP1\", \"ACMG_PP2\", \"ACMG_PP3\", \"ACMG_PP4\") # Transform 'Evidence_type' to 'P' for pathogenic and 'B' for benign df_acmg$code_prefix &lt;- ifelse(df_acmg$Evidence_type == \"pathogenicity\", \"P\", \"B\") # Create the ACMG code by combining the new prefix and the label, prepending 'ACMG_' df_acmg$ACMG_code &lt;- paste0(\"ACMG_\", df_acmg$code_prefix, df_acmg$label) acmg_labels &lt;- df_acmg$ACMG_code print(acmg_labels) # df_acmg$ACMG_label names(df) |&gt; head(30) |&gt; as.character() # Check if each ACMG column exists, if not create it and fill with NA for (acmg_label in acmg_labels) { if (!acmg_label %in% names(df)) { print(\"missing label\") df[[acmg_label]] &lt;- NA } } # Then use coalesce to find the first non-NA ACMG label df$ACMG_highest &lt;- dplyr::coalesce(!!!df[acmg_labels]) df &lt;- df %&gt;% dplyr::select(ACMG_highest, everything()) # Count the number of non-NA values across the columns df$ACMG_count &lt;- rowSums(!is.na(df[, acmg_labels ])) df &lt;- df %&gt;% dplyr::select(ACMG_count, everything()) # df$ACMG_count[df$ACMG_count == 0] &lt;- NA # ACMG Verdict---- # Define scores for Pathogenic criteria pathogenic_scores &lt;- c( \"PVS1\" = 8, setNames(rep(4, 5), paste0(\"PS\", 1:5)), setNames(rep(2, 7), paste0(\"PM\", 1:7)), \"PP3\" = 1 ) # Define scores for Benign criteria benign_scores &lt;- c( \"BA1\" = -8, setNames(rep(-4, 5), paste0(\"BS\", 1:5)), setNames(rep(-1, 8), paste0(\"BP\", 1:8)) ) # Combine both scoring systems into one vector acmg_scores &lt;- c(pathogenic_scores, benign_scores) # Print the complete ACMG scoring system print(acmg_scores) # Create ACMG_score column by looking up ACMG_highest in acmg_scores df$ACMG_score &lt;- acmg_scores[df$ACMG_highest] # If there are any ACMG labels that don't have a corresponding score, # these will be NA. You may want to set these to 0. df$ACMG_score[is.na(df$ACMG_score)] &lt;- 0 df &lt;- df |&gt; dplyr::select(ACMG_score, everything()) # Total ACMG score ---- # Mutate all ACMG columns df &lt;- df %&gt;% mutate_at(acmg_labels, function(x) acmg_scores[x]) # Replace NAs with 0 in ACMG columns only df[acmg_labels] &lt;- lapply(df[acmg_labels], function(x) ifelse(is.na(x), 0, x)) # Calculate total ACMG score df$ACMG_total_score &lt;- rowSums(df[acmg_labels]) df &lt;- df |&gt; dplyr::select(ACMG_total_score, everything()) . ",
    "url": "/pages/acmg_criteria_table_main.html#acmguru-code-example",
    
    "relUrl": "/pages/acmg_criteria_table_main.html#acmguru-code-example"
  },"31": {
    "doc": "ACMG criteria",
    "title": "References",
    "content": ". | Richards, S. et al., 2015. Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology. Genetics in Medicine, 17(5), pp.405–423. DOI: 10.1038/gim2015.30. &#8617; . | Li, M.M. et al., 2017. Standards and guidelines for the interpretation and reporting of sequence variants in cancer: a joint consensus recommendation of the Association for Molecular Pathology, American Society of Clinical Oncology, and College of American Pathologists. The Journal of Molecular Diagnostics, 19(1), pp.4–23. DOI: 10.1016/j.jmoldx.2016.10.002. &#8617; . | Riggs, E.R. et al., 2020. Technical standards for the interpretation and reporting of constitutional copy-number variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics (ACMGe and the Clinical Genome Resource (ClinGen). Genetics in Medicine, 22(2), pp.245–257. DOI: 10.1038/s41436-019-0686-8. &#8617; . | Li, Q. and Wang, K., 2017. InterVar: clinical interpretation of genetic variants by the 2015 ACMG-AMP guidelines. The American Journal of Human Genetics, 100(2), pp.267–280. DOI: 10.1016/j.ajhg.2017.01.004. &#8617; . | Xavier, A. et al., 2019. TAPES: A tool for assessment and prioritisation in exome studies. PLoS Computational Biology, 15(10), e1007453. DOI: 10.1371/journal.pcbi.1007453. &#8617; . | Pedersen, B.S. et al., 2021. Effective variant filtering and expected candidate variant yield in studies of rare human disease. NPJ Genomic Medicine, 6(1), pp.1–8. DOI: 10.1038/s41525-021-00227-3. &#8617; . | Tavtigian SV, Harrison SM, Boucher KM, Biesecker LG. (2020). Fitting a naturally scaled point system to the ACMG/AMP variant classification guidelines. Human Mutation, 41, 1734–1737. https://doi.org/10.1002/humu.24088 &#8617; &#8617;2 . | Emma H. Wilcox, Mahdi Sarmady, Bryan Wulf, Matt W. Wright, Heidi L. Rehm, Leslie G. Biesecker, Ahmad N. Abou Tayoun. (2022). Evaluating the impact of in silico predictors on clinical variant classification. Genetics in Medicine, Volume 24, Issue 4, Pages 924-930, ISSN 1098-3600, https://doi.org/10.1016/j.gim.2021.11.018. &#8617; . | . ",
    "url": "/pages/acmg_criteria_table_main.html#references",
    
    "relUrl": "/pages/acmg_criteria_table_main.html#references"
  },"32": {
    "doc": "ACMG criteria",
    "title": "ACMG criteria",
    "content": "Last update: 20220131 Last update: 20241205 . | Interpretation of variants by ACMG standards and guidelines . | Classification criteria . | Main criteria for classifications | Caveats implementing filters | . | Scoring point system | ACMGuru code example | References | . | . ",
    "url": "/pages/acmg_criteria_table_main.html",
    
    "relUrl": "/pages/acmg_criteria_table_main.html"
  },"33": {
    "doc": "Aggregate multiplexed data",
    "title": "Aggregate multiplexed data",
    "content": "Last update: 20240611 . The following content is modified from: . How should I pre-process data from multiplexed sequencing and multi-library designs? . https://gatk.broadinstitute.org/hc/en-us/articles/360035889471-How-should-I-pre-process-data-from-multiplexed-sequencing-and-multi-library-designs . We use 05_rmdup_merge.sh to process bam files for data aggregation and deduplication. Script Summary: 05_rmdup_merge.sh . Purpose: This script is tailored for efficiently merging and deduplicating sequencing data from multiple libraries and lanes per individual subject. It addresses complex setups where subjects are represented across numerous sequencing files. Process Description: . | Data Preparation: Each subject’s sequencing data, potentially spanning multiple libraries and lanes, is initially processed separately to ensure accurate read group assignment and preliminary sorting. | Aggregation and Deduplication: . | File Aggregation: BAM files from the same subject, but different lanes or libraries, are combined into a single dataset. This step merges these various inputs into one unified file. | Deduplication: Implements GATK’s MarkDuplicatesSpark to simultaneously mark and remove both PCR and optical duplicates from the merged files, improving data accuracy and quality. | . | Output Generation: Outputs a single, consolidated, and deduplicated BAM file for each subject, ready for further analysis like Base Recalibration. | . Example of File Processing: . | Input Files: . | For subject sampleA, files from two different lanes: . | sampleA_lane1_R1.fq | sampleA_lane1_R2.fq | sampleA_lane2_R1.fq | sampleA_lane2_R2.fq | . | For subject sampleB, files from two different lanes: . | sampleB_lane1_R1.fq | sampleB_lane1_R2.fq | sampleB_lane2_R1.fq | sampleB_lane2_R2.fq | . | . | Processing: . | These paired FASTQ files are first individually processed to assign read groups and generate initial BAM files: . | From sampleA_lane1_R1.fq and sampleA_lane1_R2.fq → sampleA_rgA1.bam | From sampleA_lane2_R1.fq and sampleA_lane2_R2.fq → sampleA_rgA2.bam | From sampleB_lane1_R1.fq and sampleB_lane1_R2.fq → sampleB_rgB1.bam | From sampleB_lane2_R1.fq and sampleB_lane2_R2.fq → sampleB_rgB2.bam | . | Aggregation and Deduplication: The script then aggregates and deduplicates read group BAMs for each subject: . | sampleA read groups (sampleA_rgA1.bam and sampleA_rgA2.bam) are merged and deduplicated to produce sampleA.merged.dedup.bam. | Similarly, sampleB read groups (sampleB_rgB1.bam and sampleB_rgB2.bam) are merged and deduplicated to produce sampleB.merged.dedup.bam. | . | . | Output: . | The final outputs are deduplicated BAM files for each subject, such as sampleA.merged.dedup.bam and sampleB.merged.dedup.bam. These files integrate all sequencing data from different lanes or libraries for each subject and are now ready for subsequent quality control steps like Base Recalibration. | . | . ",
    "url": "/pages/aggregate_multiplex.html",
    
    "relUrl": "/pages/aggregate_multiplex.html"
  },"34": {
    "doc": "Annotation table",
    "title": "Annotation table",
    "content": "The majority of these databases are included in dbNSFP which is thus the simplest joint source 1, 2. | Liu X, Jian X, and Boerwinkle E. 2011. dbNSFP: a lightweight database of human non-synonymous SNPs and their functional predictions. Human Mutation. 32:894-899. | Liu X, Li C, Mou C, Dong Y, and Tu Y. 2020. dbNSFP v4: a comprehensive database of transcript-specific functional predictions and annotations for human nonsynonymous and splice-site SNVs. Genome Medicine. 12:103. | . | Liu X, Jian X, and Boerwinkle E. 2011. dbNSFP: a lightweight database of human non-synonymous SNPs and their functional predictions. Human Mutation. 32:894-899. &#8617; . | Liu X, Li C, Mou C, Dong Y, and Tu Y. 2020. dbNSFP v4: a comprehensive database of transcript-specific functional predictions and annotations for human nonsynonymous and splice-site SNVs. Genome Medicine. 12:103. &#8617; . | . ",
    "url": "/pages/annotation_table.html",
    
    "relUrl": "/pages/annotation_table.html"
  },"35": {
    "doc": "Annotation table",
    "title": "Annotation table",
    "content": ". Last update: 20230531 . ",
    "url": "/pages/annotation_table.html",
    
    "relUrl": "/pages/annotation_table.html"
  },"36": {
    "doc": "BWA",
    "title": "BAM to fastq",
    "content": "Last update: 20240227 . See reference: https://www.metagenomics.wiki/tools/samtools/converting-bam-to-fastq . We have WGS data aligned to an old reference genome which required updated analysis. We must convert BAM to FASTQ so that we can re-align. For subsequent re-alignement we will use BWA. The Samtools method is the most reasonable first approach bacuase it was written by the author of BWA, Heng Li, with modifications by Martin Pollard and Jennifer Liddle, all from the Sanger Institute. The other alternative methods are listed below for comparison. ",
    "url": "/pages/bam2fastq.html#bam-to-fastq",
    
    "relUrl": "/pages/bam2fastq.html#bam-to-fastq"
  },"37": {
    "doc": "BWA",
    "title": "SAMtools",
    "content": ". | sort paired read alignment .bam file (sort by name -n) | . samtools sort -n SAMPLE.bam -o SAMPLE_sorted.bam . | save fastq reads in separate R1 and R2 files | . samtools fastq -@ 8 SAMPLE_sorted.bam \\ -1 SAMPLE_R1.fastq.gz \\ -2 SAMPLE_R2.fastq.gz \\ -0 /dev/null -s /dev/null -n . http://www.htslib.org/doc/samtools-fasta.html . On our system: . | save fastq reads in separate R1 and R2 files | Example input: 78G BAM file aligned to GRCh37 | Example output: Size R1 and R2 FASTQ | Time: 1hr 30min | Expected size: 70 GB per subject (total FASTQ) | Memory use: 1GB | . #SBATCH --nodes 1 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH --mem 24G #SBATCH --time 12:00:00 . # Result indicates that reads are not paired and we require singletons bam2fastq_1_40002.err [M::bam2fq_mainloop] discarded 903778738 singletons [M::bam2fq_mainloop] processed 909146794 reads . ",
    "url": "/pages/bam2fastq.html#samtools",
    
    "relUrl": "/pages/bam2fastq.html#samtools"
  },"38": {
    "doc": "BWA",
    "title": "Collate interleaved",
    "content": "echo \"samsort collate shuffles and groups reads together by their names.\" samtools sort -n ${TEMP_DIR}/${file1} -o ${TEMP_DIR}/${file1}_collate.bam echo \"samsort -n complete\" echo \"bam2fastq for interleaved file start\" samtools fastq -@ 8 \\ -0 /dev/null \\ ${TEMP_DIR}/${file1}_collate.bam \\ &gt; ${TEMP_DIR}/${file1}_all_reads.fq . ",
    "url": "/pages/bam2fastq.html#collate-interleaved",
    
    "relUrl": "/pages/bam2fastq.html#collate-interleaved"
  },"39": {
    "doc": "BWA",
    "title": "Paried files",
    "content": "echo \"samsort -n start\" samtools sort -n ${TEMP_DIR}/${file1} -o ${TEMP_DIR}/${file1}_sortn.bam echo \"samsort -n complete\" echo \"bam2fastq for paired files start\" samtools fastq -@ 8 ${TEMP_DIR}/${file1}_sortn.bam \\ -1 ${TEMP_DIR}/${file1}_R1.fastq.gz \\ -2 ${TEMP_DIR}/${file1}_R2.fastq.gz \\ -0 /dev/null -s /dev/null -n . | Using bam2fq samtools bam2fq SAMPLE.bam &gt; SAMPLE.fastq . paired-end reads: ‘/1’ or ‘/2’ is added to the end of read names . | . http://www.htslib.org/doc/samtools.html . | How to split a single .fastq file of paired-end reads into two separated files? # extracting reads ending with '/1' or '/2' cat SAMPLE.fastq | grep '^@.*/1$' -A 3 --no-group-separator &gt; SAMPLE_R1.fastq cat SAMPLE.fastq | grep '^@.*/2$' -A 3 --no-group-separator &gt; SAMPLE_R2.fastq . | . ",
    "url": "/pages/bam2fastq.html#paried-files",
    
    "relUrl": "/pages/bam2fastq.html#paried-files"
  },"40": {
    "doc": "BWA",
    "title": "Picard",
    "content": ". | converting a SAMPLE.bam file into paired end SAMPLE_R1.fastq and SAMPLE_R2.fastq files . | F2 to get two files for paired-end reads (R1 and R2) | -Xmx2g allows a maximum use of 2GB memory for the JVM java -Xmx2g -jar Picard/SamToFastq.jar I=SAMPLE.bam F=SAMPLE_R1.fastq F2=SAMPLE_R2.fastq . | . | . http://broadinstitute.github.io/picard/command-line-overview.html#SamToFastq . ",
    "url": "/pages/bam2fastq.html#picard",
    
    "relUrl": "/pages/bam2fastq.html#picard"
  },"41": {
    "doc": "BWA",
    "title": "bam2fastx",
    "content": "http://manpages.ubuntu.com/manpages/quantal/man1/bam2fastx.1.html . ",
    "url": "/pages/bam2fastq.html#bam2fastx",
    
    "relUrl": "/pages/bam2fastq.html#bam2fastx"
  },"42": {
    "doc": "BWA",
    "title": "Bedtools",
    "content": "bedtools bamtofastq -i input.bam -fq output.fastq . paired-end reads: . samtools sort -n input.bam -o input_sorted.bam # sort reads by identifier-name (-n) bedtools bamtofastq -i input_sorted.bam -fq output_R3.fastq -fq2 output_R2.fastq . http://bedtools.readthedocs.org/en/latest/content/tools/bamtofastq.html . ",
    "url": "/pages/bam2fastq.html#bedtools",
    
    "relUrl": "/pages/bam2fastq.html#bedtools"
  },"43": {
    "doc": "BWA",
    "title": "Bamtools",
    "content": "http://github.com/pezmaster31/bamtools . ",
    "url": "/pages/bam2fastq.html#bamtools",
    
    "relUrl": "/pages/bam2fastq.html#bamtools"
  },"44": {
    "doc": "BWA",
    "title": "BWA",
    "content": " ",
    "url": "/pages/bam2fastq.html",
    
    "relUrl": "/pages/bam2fastq.html"
  },"45": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Bayesian: Part 1 Probability of a girl birth given placenta previa",
    "content": "Last update: . ## [1] \"2024-12-04\" . This doc was built with: rmarkdown::render(\"Bayesian_example1.Rmd\", output_file = \"../pages/bayesian_example1.md\") . ",
    "url": "/pages/bayesian_example1.html#bayesian-part-1-probability-of-a-girl-birth-given-placenta-previa",
    
    "relUrl": "/pages/bayesian_example1.html#bayesian-part-1-probability-of-a-girl-birth-given-placenta-previa"
  },"46": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Introduction",
    "content": "This example is described in the textbook: Bayesian Data Analysis, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. Third edition, (BDA3), http://www.stat.columbia.edu/~gelman/book/. The code is based on a version by Aki Vehtari. In an interesting Bayesian case study, we examine the probability of a girl birth among births with the condition placenta previa, where the placenta obstructs a normal vaginal delivery. An early study in Germany found that out of 980 births with placenta previa, 437 were female. We aim to assess the evidence supporting the hypothesis that the proportion of female births in this condition is less than the general population proportion of 0.485. https://commons.wikimedia.org/wiki/Category:Placenta_previa#/media/File:2906_Placenta_Previa-02.jpg CC BY 3.0. ",
    "url": "/pages/bayesian_example1.html#introduction",
    
    "relUrl": "/pages/bayesian_example1.html#introduction"
  },"47": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Bayesian Framework",
    "content": "Data and Model . We define the observed study data: . | \\(X = 437\\) - number of female births in placenta previa | \\(Y = 543\\) - number of male births in placenta previa | \\(n = 980\\) - total births in placenta previa | \\(0.485\\) - frequency of normal female births in the population | Posterior is Beta(438,544) | . The parameter of interest, \\(\\theta\\), represents the probability of a female birth in placenta previa cases. We calculate and plot the posterior distribution of the proportion of \\(\\theta\\), using uniform prior on \\(\\theta\\). In Bayesian analysis, especially when dealing with proportions like the probability of a girl birth in this scenario, understanding the entire distribution is crucial. The Beta distribution, being the conjugate prior for binomial likelihoods, is particularly sensitive to the shape parameters (\\(\\alpha\\) and \\(\\beta\\)), which in this case are derived from the observed data (437 girls, 543 boys) plus one for each due to the uniform prior assumption (\\(\\alpha = X + 1\\), \\(\\beta = n - X + 1\\)): . | Alpha (438): Represents the number of successes (female births) plus one. | Beta (544): Represents the number of failures (male births) plus one. | . Binomial likelihoods: This relates to scenarios where you have binary data that result from a series of trials with two possible outcomes (like success and failure). For example, flipping a coin multiple times and counting how many times it lands heads is a situation that would use a binomial likelihood because each flip has two possible outcomes (heads or tails). Conjugate prior: A conjugate prior is a special type of prior that, when used with a particular likelihood function (like the binomial likelihood), results in a posterior distribution that is the same type of distribution as the prior. This is useful because it simplifies the mathematical calculations involved in updating beliefs with new data. library(ggplot2) theme_set(theme_bw()) # Posterior is Beta(438,544) # seq creates evenly spaced values df1 &lt;- data.frame(theta = seq(0.375, 0.525, 0.001)) a &lt;- 438 b &lt;- 544 . Prior and Posterior Distributions . Under a uniform prior distribution (implying no prior bias towards specific values), the beta distribution is utilised as the conjugate prior. The posterior distribution then is: . \\[\\text{Posterior}(\\theta) = \\text{Beta}(\\alpha + X, \\beta + n - X) = \\text{Beta}(438, 544)\\] This choice of prior reflects an initial belief that all values of \\(\\theta\\) are equally likely, from 0 to 1. # dbeta computes the posterior density df1$p &lt;- dbeta(df1$theta, a, b) . # compute also 95% central interval # seq creates evenly spaced values from 2.5% quantile # to 97.5% quantile (i.e., 95% central interval) # qbeta computes the value for a given quantile given parameters a and b df2 &lt;- data.frame(theta = seq(qbeta(0.025, a, b), qbeta(0.975, a, b), length.out = 100)) . # compute the posterior density df2$p &lt;- dbeta(df2$theta, a, b) . data_mean &lt;- round(mean(df2$theta), digits = 3) data_sd &lt;- round(sd(df2$theta), digits = 3) data_lab &lt;- paste0(\"Frequency of\\nfemale births\\nin placenta previa\\nmean = \", data_mean, \"\\nsd = \", data_sd) . # Plot posterior (Beta(438,544)) and 48.8% line for population average ggplot(mapping = aes(theta, p)) + geom_line(data = df1) + # Add a layer of colorized 95% posterior interval geom_area(data = df2, aes(fill='1')) + geom_vline(xintercept = data_mean, linetype='dotted') + annotate(geom = \"label\", label = data_lab, x = data_mean, y = 20, hjust = 0, fill = \"white\", alpha = 0.5) + # Add the proportion of girl babies in general population geom_vline(xintercept = pop_freq, linetype='dotted') + annotate(geom = \"label\", label = lab_pop, x = pop_freq, y = 20, hjust = 0, fill = \"white\", alpha = 0.5) + # Decorate the plot a little labs(title='Uniform prior -&gt; Posterior is Beta(438,544)') + scale_y_continuous(expand = c(0, 0.1)) + scale_fill_manual(values = 'lightblue', labels = '95% posterior interval') + theme(legend.position = 'bottom', legend.title = element_blank()) . Posterior Analysis . Calculation Methods . | Analytical approach: Using properties of the beta distribution, the posterior mean is 0.446 and the posterior standard deviation is 0.016. | Simulation approach: Drawing 1000 samples from the Beta(438, 544) posterior, the sample mean and standard deviation closely match the analytical results. | . Confidence Intervals . | Beta quantiles: The 95% confidence interval for \\(\\theta\\) from beta properties is [0.415, 0.477]. | Simulation-based estimate: Using ordered draws, the 95% interval is similarly [0.415, 0.476]. | Normal approximation: For practical ease, a normal approximation gives [0.414, 0.476], indicating robustness of the estimate. | . Enhanced precision with logit transformation . Transforming \\(\\theta\\) to the logit scale: . \\[\\text{logit}(\\theta) = \\log\\left(\\frac{\\theta}{1-\\theta}\\right)\\] This transformation stabilises variance, especially beneficial for values of \\(\\theta\\) near boundaries. The logit-transformed values follow a normal distribution, allowing us to back-calculate the confidence interval for \\(\\theta\\) effectively. ",
    "url": "/pages/bayesian_example1.html#bayesian-framework",
    
    "relUrl": "/pages/bayesian_example1.html#bayesian-framework"
  },"48": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Considerations on prior sensitivity",
    "content": "Exploring different conjugate priors with varying strengths of belief around the general population proportion (0.485), the results show that large sample sizes dilute the influence of these priors, as seen with the posterior distributions retaining similar confidence intervals across various priors. ",
    "url": "/pages/bayesian_example1.html#considerations-on-prior-sensitivity",
    
    "relUrl": "/pages/bayesian_example1.html#considerations-on-prior-sensitivity"
  },"49": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Plotting decision",
    "content": "The choice of the values for the sequence seq(0.375, 0.525, 0.001) in df1 is designed to provide a visualization of the posterior probability density function (pdf) of \\(\\theta\\) (the probability of a girl birth given placenta previa) over a relevant range of \\(\\theta\\) values. | Start (0.375) and end (0.525): These values define the range over which the posterior distribution will be evaluated and plotted. The range is chosen to be slightly broader than the central 95% posterior interval calculated from the Beta distribution (Beta(438, 544)), which is [0.415, 0.477]. This broader range allows the plot to display the tails of the distribution, providing a complete view of how the density behaves towards the edges, which is informative for understanding the distribution’s shape and spread. | Relevance to the data: The range centers around the expected posterior mean (\\(0.446\\)) and includes the entire 95% confidence interval, thereby capturing the most statistically significant values of \\(\\theta\\) under the given model and data. | . ",
    "url": "/pages/bayesian_example1.html#plotting-decision",
    
    "relUrl": "/pages/bayesian_example1.html#plotting-decision"
  },"50": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Conclusion",
    "content": "Based on the data, the probability of a female birth given placenta previa is less than the general population’s proportion. The findings are consistent despite different computational methods and prior assumptions, illustrating the power of Bayesian inference in real-world data interpretation. ",
    "url": "/pages/bayesian_example1.html#conclusion",
    
    "relUrl": "/pages/bayesian_example1.html#conclusion"
  },"51": {
    "doc": "Bayes 1 probability in placenta previa",
    "title": "Bayes 1 probability in placenta previa",
    "content": " ",
    "url": "/pages/bayesian_example1.html",
    
    "relUrl": "/pages/bayesian_example1.html"
  },"52": {
    "doc": "Bayesian 2 probability in placenta previa",
    "title": "Bayesian: Part 2 Probability of a girl birth given placenta previa",
    "content": "Last update: . ## [1] \"2024-11-29\" . This doc was built with: rmarkdown::render(\"Bayesian_example2.Rmd\", output_file = \"../pages/bayesian_example2.md\") . ",
    "url": "/pages/bayesian_example2.html#bayesian-part-2-probability-of-a-girl-birth-given-placenta-previa",
    
    "relUrl": "/pages/bayesian_example2.html#bayesian-part-2-probability-of-a-girl-birth-given-placenta-previa"
  },"53": {
    "doc": "Bayesian 2 probability in placenta previa",
    "title": "Introduction to part 2",
    "content": "The code is based on a version by Aki Vehtari. In the continuation of the analysis on placenta previa, we look at different priors to understand their impact on the posterior distribution. This part not only revisits the basic steps introduced in Part 1 but also expands on them by exploring alternative prior settings to illustrate the sensitivity of our posterior estimates to the choice of prior. Following the initial setup where we determined the posterior distribution using a uniform prior, Part 2 investigates how different priors influence the results. This is crucial for assessing the robustness of our conclusions against the assumptions we make in our Bayesian framework. | \\(X = 437\\) - number of female births in placenta previa | \\(Y = 543\\) - number of male births in placenta previa | \\(n = 980\\) - total births in placenta previa | \\(0.485\\) - the frequency of normal female births in the population | As in part 1, we had a posterior Beta(438,544) | . a &lt;- 437 # girls b &lt;- 543 # boys . ",
    "url": "/pages/bayesian_example2.html#introduction-to-part-2",
    
    "relUrl": "/pages/bayesian_example2.html#introduction-to-part-2"
  },"54": {
    "doc": "Bayesian 2 probability in placenta previa",
    "title": "Exploring the effect of different priors",
    "content": "Density evaluation: We calculate the density of the posterior distribution over a range of theta values using a uniform prior for simplicity. # Evaluate densities at evenly spaced points between 0.375 and 0.525 df1 &lt;- data.frame(theta = seq(0.375, 0.525, 0.001)) # Posterior with Beta(1,1), ie. uniform prior df1$pu &lt;- dbeta(df1$theta, a+1, b+1) . Further prior variations: We set up priors with varying strengths by modifying the prior counts and success ratio, reflecting different degrees of confidence in the prior information. # 3 different choices for priors # Beta(0.485*2,(1-0.485)*2) # Beta(0.485*20,(1-0.485)*20) # Beta(0.485*200,(1-0.485)*200) n &lt;- c(2, 20, 200) # prior counts apr &lt;- 0.485 # prior ratio of success . | Beta distribution parameters: These parameters are set to reflect increasing confidence in prior information, with the product of 0.485 and multipliers (2, 20, 200) determining the strength of the prior belief. This adjustment in parameters influences how much the prior beliefs affect the Bayesian updating process. | Prior counts (n): Specifies the strength of the prior. Lower counts like 2 suggest minimal prior influence, while higher counts like 200 indicate a strong prior belief based on substantial evidence or confidence. | Prior probability of success (apr): Represents an assumed rate of female births, serving as the basis for setting the Beta distribution parameters, thereby impacting the shape of the posterior distribution. | . This setup allows for the examination of how prior beliefs, quantified by n and apr, impact the posterior estimates. By varying these priors, we see the Bayesian framework’s sensitivity to initial assumptions, highlighting the need for careful consideration of prior information in Bayesian analysis. The following helper function and lapply construct compile the dataset as described: helperf computes prior and posterior densities for a range of theta values based on varying strengths of prior beliefs. This is combined using lapply across different prior settings, and the results are consolidated and reshaped for plotting . # helperf returns for given number of prior observations, prior ratio # of successes, number of observed successes and failures and a data # frame with values of theta, a new data frame with prior and posterior # values evaluated at points theta. helperf &lt;- function(n, apr, a, b, df) cbind(df, pr = dbeta(df$theta, n*apr, n*(1-apr)), po = dbeta(df$theta, n*apr + a, n*(1-apr) + b), n = n) # lapply function over prior counts n and pivot results into key-value pairs. df2 &lt;- lapply(n, helperf, apr, a, b, df1) %&gt;% do.call(rbind, args = .) %&gt;% pivot_longer(!c(theta, n), names_to = \"density_group\", values_to = \"p\") %&gt;% mutate(density_group = factor(density_group, labels=c('Posterior','Prior','Posterior with unif prior'))) # add correct labels for plotting df2$title &lt;- factor(paste0('alpha/(alpha+beta)=0.485, alpha+beta=',df2$n)) . # Plot distributions ggplot(data = df2) + geom_line(aes(theta, p, color = density_group)) + # proportion of girl babies in general population geom_vline(xintercept = pop_freq, linetype='dotted') + annotate(geom = \"label\", label = lab_pop, x = pop_freq, y = 20, hjust = 0, fill = \"white\", alpha = 0.5, size =2) + facet_wrap(~title, ncol = 1) . The resulting plots highlight how the choice of prior affects the posterior, with visual cues like vertical lines at the prior mean and faceting by scenario. ",
    "url": "/pages/bayesian_example2.html#exploring-the-effect-of-different-priors",
    
    "relUrl": "/pages/bayesian_example2.html#exploring-the-effect-of-different-priors"
  },"55": {
    "doc": "Bayesian 2 probability in placenta previa",
    "title": "Conclusion",
    "content": "The exploration in part 2 demonstrates the Bayesian framework’s flexibility and the critical role of prior selection. By comparing different priors, we see how prior beliefs can substantially influence posterior outcomes, thereby affecting conclusions drawn from Bayesian analysis. ",
    "url": "/pages/bayesian_example2.html#conclusion",
    
    "relUrl": "/pages/bayesian_example2.html#conclusion"
  },"56": {
    "doc": "Bayesian 2 probability in placenta previa",
    "title": "Bayesian 2 probability in placenta previa",
    "content": " ",
    "url": "/pages/bayesian_example2.html",
    
    "relUrl": "/pages/bayesian_example2.html"
  },"57": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Bayesian: Discrete probability example in genetics",
    "content": "Last update: . ## [1] \"2024-11-29\" . This doc was built with: rmarkdown::render(\"bayesian_genetic_carrier.Rmd\", output_file = \"../pages/bayesian_genetic_carrier.md\") . ",
    "url": "/pages/bayesian_genetic_carrier.html#bayesian-discrete-probability-example-in-genetics",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#bayesian-discrete-probability-example-in-genetics"
  },"58": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Inference about Genetic Status",
    "content": " ",
    "url": "/pages/bayesian_genetic_carrier.html#inference-about-genetic-status",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#inference-about-genetic-status"
  },"59": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Background",
    "content": "Human males have one X-chromosome and one Y-chromosome, whereas females have two X-chromosomes, each chromosome being inherited from one parent. Hemophilia is a disease that exhibits X-chromosome-linked recessive inheritance, meaning that a male who inherits the gene that causes the disease on the X-chromosome is affected, whereas a female carrying the gene on only one of her two X-chromosomes is not affected. The disease is generally fatal for women who inherit two such genes, and this is rare, since the frequency of occurrence of the gene is low in human populations. ",
    "url": "/pages/bayesian_genetic_carrier.html#background",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#background"
  },"60": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Prior Distribution",
    "content": "Consider a woman who has an affected brother. This implies her mother must be a carrier of the hemophilia gene, possessing one ‘good’ and one ‘bad’ hemophilia gene. We also know her father is not affected; therefore, the woman herself has a fifty-fifty chance of being a carrier of the gene. The unknown quantity of interest, the state of the woman, has just two possible values: - \\(\\theta = 1\\): The woman is a carrier of the gene. - \\(\\theta = 0\\): The woman is not a carrier of the gene. Based on the information provided thus far, the prior distribution for the unknown \\(\\theta\\) can be expressed with equal probabilities for being a carrier or not: . \\[\\Pr(\\theta = 1) = \\Pr(\\theta = 0) = 0.5\\] ",
    "url": "/pages/bayesian_genetic_carrier.html#prior-distribution",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#prior-distribution"
  },"61": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Data Model and Likelihood",
    "content": "The data used to update the prior information consist of the affection status of the woman’s sons. Suppose she has two sons, neither of whom is affected. Let \\(y_i = 1\\) or \\(0\\) denote an affected or unaffected son, respectively. The outcomes of the two sons are exchangeable and, conditional on the unknown \\(\\theta\\), are independent; we assume the sons are not identical twins. The likelihood function based on the outcome where neither son is affected is given by: . | \\[\\Pr(y_1 = 0, y_2 = 0 | \\theta = 1) = (0.5)(0.5) = 0.25\\] | \\[\\Pr(y_1 = 0, y_2 = 0 | \\theta = 0) = (1)(1) = 1\\] | . These expressions arise from the fact that if the woman is a carrier (\\(\\theta = 1\\)), each of her sons has a 50% chance of inheriting the gene and thus being affected. Conversely, if she is not a carrier (\\(\\theta = 0\\)), the probability is close to 1 that her sons will be unaffected. We ignore the small probability of being affected even if the mother is not a carrier for this example. ",
    "url": "/pages/bayesian_genetic_carrier.html#data-model-and-likelihood",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#data-model-and-likelihood"
  },"62": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Posterior Distribution After Observing Two Unaffected Sons",
    "content": "Using Bayes’ rule, we combine the likelihoods with the prior probabilities to determine the posterior probability that the woman is a carrier, denoted as \\(\\theta = 1\\), given the data \\(y = (y_1 = 0, y_2 = 0)\\): . \\[\\Pr(\\theta = 1 | y_1, y_2) = \\frac{\\Pr(y_1 = 0, y_2 = 0 | \\theta = 1) \\Pr(\\theta = 1)}{\\Pr(y_1 = 0, y_2 = 0 | \\theta = 1) \\Pr(\\theta = 1) + \\Pr(y_1 = 0, y_2 = 0 | \\theta = 0) \\Pr(\\theta = 0)}\\] Substituting the values: . \\[\\Pr(\\theta = 1 | y_1, y_2) = \\frac{0.25 \\times 0.5}{0.25 \\times 0.5 + 1 \\times 0.5} = \\frac{0.125}{0.625} = 0.20\\] Or more simply: \\(\\Pr(\\theta = 1 | y) = \\frac{\\Pr(y | \\theta = 1) \\Pr(\\theta = 1)}{\\Pr(y | \\theta = 1) \\Pr(\\theta = 1) + \\Pr(y | \\theta = 0) \\Pr(\\theta = 0)}\\) . Substituting the values: . \\[\\Pr(\\theta = 1 | y) = \\frac{0.25 \\times 0.5}{0.25 \\times 0.5 + 1 \\times 0.5} = \\frac{0.125}{0.625} = 0.20\\] This calculation intuitively suggests that it is less probable for the woman to be a carrier if her children are unaffected. Bayes’ rule provides a formal mechanism for this correction. The results can also be expressed in terms of odds: . | Prior odds: \\(0.5 / 0.5 = 1\\) | Likelihood ratio (based on the information about her two unaffected sons): \\(0.25 / 1 = 0.25\\) | Posterior odds: \\(1 \\times 0.25 = 0.25\\) | . Converting back to probability: . \\[\\frac{0.25}{1 + 0.25} = 0.2\\] The posterior probability that the woman is a carrier is thus 20%, which adjusts the prior belief based on the outcomes of her sons. Conversely, the probability that she is not a carrier is updated to: . \\[\\Pr(\\theta = 0 | y_1, y_2) = 0.80\\] This posterior probability now serves as the new prior for further analysis if additional data is considered. ",
    "url": "/pages/bayesian_genetic_carrier.html#posterior-distribution-after-observing-two-unaffected-sons",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#posterior-distribution-after-observing-two-unaffected-sons"
  },"63": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Adding More Data",
    "content": "A key aspect of Bayesian analysis is the ease with which sequential analyses can be performed. For example, suppose that the woman has a third son, who is also unaffected. The entire calculation does not need to be redone; rather, we use the previous posterior distribution as the new prior distribution, to update our belief about the woman’s carrier status. Updating with an Unaffected Third Son . If the third son is also unaffected, the likelihood for \\(\\theta = 1\\) is $0.5$ (probability that the son does not inherit the gene if the mother is a carrier). The likelihood for \\(\\theta = 0\\) (probability that the son does not inherit the gene if the mother is not a carrier) remains 1, given the genetic condition being recessive. Using the posterior from the previous calculation as the new prior, the Bayesian update would be: . \\[\\Pr(\\theta = 1 | y_1, y_2, y_3) = \\frac{\\Pr(y_3 = 0 | \\theta = 1) \\times \\Pr(\\theta = 1 | y_1, y_2)}{\\Pr(y_3 = 0 | \\theta = 1) \\times \\Pr(\\theta = 1 | y_1, y_2) + \\Pr(y_3 = 0 | \\theta = 0) \\times \\Pr(\\theta = 0 | y_1, y_2)}\\] Plugging in the values: . \\[\\Pr(\\theta = 1 | y_1, y_2, y_3) = \\frac{0.5 \\times 0.20}{0.5 \\times 0.20 + 1 \\times 0.80} = \\frac{0.10}{0.10 + 0.80} = \\frac{0.10}{0.90} \\approx 0.111\\] Updating with an Affected Third Son . Conversely, if the third son is affected, the likelihood of him being affected given that his mother is a carrier (\\(\\theta = 1\\)) is again 0.5. However, the likelihood of him being affected given that his mother is not a carrier (\\(\\theta = 0\\)) is practically 0, considering the genetic condition is recessive and assuming we ignore the small mutation rate. The update for the Bayesian analysis would therefore be: . \\[\\Pr(\\theta = 1 | y_1, y_2, y_3) = \\frac{\\Pr(y_3 = 1 | \\theta = 1) \\times \\Pr(\\theta = 1 | y_1, y_2)}{\\Pr(y_3 = 1 | \\theta = 1) \\times \\Pr(\\theta = 1 | y_1, y_2) + \\Pr(y_3 = 1 | \\theta = 0) \\times \\Pr(\\theta = 0 | y_1, y_2)}\\] Given the values: . \\[\\Pr(\\theta = 1 | y_1, y_2, y_3) = \\frac{0.5 \\times 0.20}{0.5 \\times 0.20 + 0 \\times 0.80} = \\frac{0.10}{0.10 + 0.00} = 1\\] This result reflects the significant impact of observing an affected child in a scenario where the mother’s carrier status was uncertain. If the third son is affected, it conclusively indicates that the woman is a carrier of the hemophilia gene. The probability of the woman being a carrier thus becomes 1, showcasing the power of Bayesian updating when integrating decisive new evidence. ",
    "url": "/pages/bayesian_genetic_carrier.html#adding-more-data",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#adding-more-data"
  },"64": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Dataset and visualisation",
    "content": "library(ggplot2) # Data for two unaffected sons all_probs &lt;- data.frame( Theta = c(\"θ = 1 (Carrier)\", \"θ = 0 (Not Carrier)\"), Probability = c(0.20, 0.80), # Posterior probabilities after observing two unaffected sons Scenario = \"Two Unaffected Sons\" ) # Data for three unaffected sons - update these probabilities based on your extended analysis all_probs_three_sons &lt;- data.frame( Theta = c(\"θ = 1 (Carrier)\", \"θ = 0 (Not Carrier)\"), Probability = c(0.111, 0.889), # Update based on your analysis Scenario = \"Three Unaffected Sons\" ) # Data for third son affected - hypothetical scenario where the third son is affected all_probs_affected_third_son &lt;- data.frame( Theta = c(\"θ = 1 (Carrier)\", \"θ = 0 (Not Carrier)\"), Probability = c(1, 0), # Hypothetical update if the third son is affected Scenario = \"Affected Third Son\" ) # Combine all data frames for unified plotting later all_data &lt;- rbind( all_probs, all_probs_three_sons, all_probs_affected_third_son ) all_data$Scenario &lt;- factor(all_data$Scenario, levels = c(\"Two Unaffected Sons\", \"Three Unaffected Sons\", \"Affected Third Son\")) . | Theta | Probability | Scenario | . | θ = 1 (Carrier) | 0.200 | Two Unaffected Sons | . | θ = 0 (Not Carrier) | 0.800 | Two Unaffected Sons | . | θ = 1 (Carrier) | 0.111 | Three Unaffected Sons | . | θ = 0 (Not Carrier) | 0.889 | Three Unaffected Sons | . | θ = 1 (Carrier) | 1.000 | Affected Third Son | . | θ = 0 (Not Carrier) | 0.000 | Affected Third Son | . all_data - Probabilities table . plot_bayesian_updates &lt;- function(data) { ggplot(data, aes(x = Scenario, y = Probability, fill = Theta)) + geom_bar(stat = \"identity\", position = \"fill\") + scale_y_continuous(labels = scales::percent_format()) + labs(x = \"Scenario\", y = \"Probability\", fill = \"Carrier Status\") + ggtitle(\"Bayesian Updates for Different Scenarios\") + theme_minimal() } # Plotting the combined data plot_bayesian_updates(all_data) . ",
    "url": "/pages/bayesian_genetic_carrier.html#dataset-and-visualisation",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html#dataset-and-visualisation"
  },"65": {
    "doc": "Bayesian discrete probability example in genetics",
    "title": "Bayesian discrete probability example in genetics",
    "content": " ",
    "url": "/pages/bayesian_genetic_carrier.html",
    
    "relUrl": "/pages/bayesian_genetic_carrier.html"
  },"66": {
    "doc": "Bayes MCMC samplers",
    "title": "The Markov-chain Monte Carlo (MCMC) sampler gallery",
    "content": "This page is cloned from Chi Feng at https://github.com/chi-feng/mcmc-demo to have an important references for MCMC samplers. The following demo shows a multimodal dataset with Gibbs sampling. Your browser does not support iframes. Click on an algorithm below to view interactive demo: . | Random Walk Metropolis Hastings | Adaptive Metropolis Hastings [1] | Hamiltonian Monte Carlo [2] | No-U-Turn Sampler [2] | Metropolis-adjusted Langevin Algorithm (MALA) [3] | Hessian-Hamiltonian Monte Carlo (H2MC) [4] | Gibbs Sampling | Stein Variational Gradient Descent (SVGD) [5] | Nested Sampling with RadFriends (RadFriends-NS) [6] | Differential Evolution Metropolis (Z) [7] | Microcanonical Hamiltonian Monte Carlo [8] | . View the source code on GitHub: https://github.com/chi-feng/mcmc-demo. ",
    "url": "/pages/bayesian_mcmc_samplers.html#the-markov-chain-monte-carlo-mcmc-sampler-gallery",
    
    "relUrl": "/pages/bayesian_mcmc_samplers.html#the-markov-chain-monte-carlo-mcmc-sampler-gallery"
  },"67": {
    "doc": "Bayes MCMC samplers",
    "title": "References",
    "content": ". | [1] H. Haario, E. Saksman, and J. Tamminen, An adaptive Metropolis algorithm (2001) | [2] M. D. Hoffman, A. Gelman, The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo (2011) | [3] G. O. Roberts, R. L. Tweedie, Exponential Convergence of Langevin Distributions and Their Discrete Approximations (1996) | [4] Li, Tzu-Mao, et al. Anisotropic Gaussian mutations for metropolis light transport through Hessian-Hamiltonian dynamics ACM Transactions on Graphics 34.6 (2015): 209. | [5] Q. Liu, et al. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm Advances in Neural Information Processing Systems. 2016. | [6] J. Buchner A statistical test for Nested Sampling algorithms Statistics and Computing. 2014. | [7] Cajo J. F. ter Braak &amp; Jasper A. Vrugt Differential Evolution Markov Chain with snooker updater and fewer chains Statistics and Computing. 2008. | [8] Jakob Robnik, G. Bruno De Luca, Eva Silverstein, Uroš Seljak Microcanonical Hamiltonian Monte Carlo | . ",
    "url": "/pages/bayesian_mcmc_samplers.html#references",
    
    "relUrl": "/pages/bayesian_mcmc_samplers.html#references"
  },"68": {
    "doc": "Bayes MCMC samplers",
    "title": "Bayes MCMC samplers",
    "content": "Last update: 20241204 . ",
    "url": "/pages/bayesian_mcmc_samplers.html",
    
    "relUrl": "/pages/bayesian_mcmc_samplers.html"
  },"69": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Bayesian: Multiparameter binomial regression and grid sampling for bioassay data",
    "content": "Last update: . ## [1] \"2024-12-04\" . This doc was built with: rmarkdown::render(\"bayesian_demo3_6.Rmd\", output_file = \"../pages/bayesian_multiparameter_bioassay.md\") . This example is described in the textbook: Bayesian Data Analysis, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. Third edition, (BDA3), http://www.stat.columbia.edu/~gelman/book/, chapter 3.7 (BDA3 p. 74-). The code is based on a version by Aki Vehtari. library(ggplot2); theme_set(theme_bw()) library(gridExtra) library(tidyr) library(dplyr) library(purrr) . ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#bayesian-multiparameter-binomial-regression-and-grid-sampling-for-bioassay-data",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#bayesian-multiparameter-binomial-regression-and-grid-sampling-for-bioassay-data"
  },"70": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Introduction",
    "content": "This document explores the Bayesian approach for analysing bioassay data, focusing on estimating the toxicity of chemical compounds through a two-parameter logistic regression model. This analysis typifies non-conjugate scenarios in multiparameter problems where exact solutions are impractical, using a discretised approximation for the posterior distribution. The Bayesian approach enhances LD50 estimation, the key toxicological benchmark indicating a 50% fatality risk. Unlike traditional methods that rely on single-point estimates, this method employs a probabilistic framework, yielding a detailed distribution of potential outcomes. This not only provides a more nuanced understanding of toxicity but also aids regulatory bodies in establishing more precise and dependable safety thresholds. ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#introduction",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#introduction"
  },"71": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "The scientific problem and the data",
    "content": "Bioassay experiments are crucial in determining the lethal dosage of drugs and chemicals. The data structure for such experiments involves dose-response information where each observation unit corresponds to a different dose level administered to groups of subjects, recording the number of fatalities. Data overview . | x = dose_level (drug) | n = n_animals (subjects) | y = n_deaths (bad outcomes) | . df1 &lt;- data.frame( x = c(-0.86, -0.30, -0.05, 0.73), n = c(5, 5, 5, 5), y = c(0, 1, 3, 5) ) print(df1) . ## x n y ## 1 -0.86 5 0 ## 2 -0.30 5 1 ## 3 -0.05 5 3 ## 4 0.73 5 5 . Plot data . ggplot(df1, aes(x=x, y=y)) + geom_point(size=4, color='red') + ggrepel::geom_text_repel(aes(label=x), vjust=-0.5, hjust=1.5, color='black') + labs(title = 'Bioassay deaths and drug dosage', x = 'Dose (log g/ml)', y = 'Number of deaths') . Model specification . The responses within each dose group are modelled as binomially distributed, reflecting the probabilistic nature of the observed outcomes. Each response probability, \\(\\theta_i\\), depends on the dose, reinforcing the assumption that the likelihood of a positive outcome follows a binomial distribution conditioned on the dose level and response probability. ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#the-scientific-problem-and-the-data",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#the-scientific-problem-and-the-data"
  },"72": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Modeling the dose–response relation",
    "content": "In this analysis, we model the dose-response relationship using a logistic regression framework, which assumes that the log odds of death, \\(\\log\\left(\\frac{\\theta_i}{1-\\theta_i}\\right)\\), are linearly related to the dose levels. This approach allows the response probabilities to vary between 0 and 1, accommodating the binary nature of the outcomes (e.g., survival or death). The relationship is defined mathematically as: . \\[\\text{logit}(\\theta_i) = \\alpha + \\beta x_i\\] where \\(\\theta_i\\) is the probability of death given dose \\(x_i\\), and \\(\\alpha\\) and \\(\\beta\\) are parameters to be estimated. Likelihood specification . The likelihood for each group \\(i\\) under the model is defined as follows: . \\[p(y_i | \\alpha, \\beta, n_i, x_i) \\propto [\\text{logit}^{-1}(\\alpha + \\beta x_i)]^{y_i} [1 - \\text{logit}^{-1}(\\alpha + \\beta x_i)]^{n_i - y_i}\\] where \\(y_i\\) is the number of deaths among \\(n_i\\) subjects at dose level \\(x_i\\). This specification assumes that the outcomes within each group are independent and identically distributed, following a binomial distribution. Prior distribution and independence assumptions . We adopt a noninformative prior distribution for the parameters, uniform over the parameter space, expressed as \\(p(\\alpha, \\beta) \\propto 1\\). This choice reflects a lack of prior knowledge about the parameters and facilitates an objective analysis based primarily on the data. Furthermore, the assumption of independence among the responses within each group supports the use of the binomial model. This assumption is reasonable unless there is evidence of interaction effects among subjects within the same dose group, such as contagion in the event of disease transmission. Model justification and adjustments . Given that \\(\\alpha\\) and \\(\\beta\\) directly influence \\(\\theta_i\\) through the logistic function, it is crucial that the model accounts for the bounded nature of probabilities. The logistic transformation ensures that the estimated probabilities remain within the [0,1] interval, regardless of the values taken by the linear combination \\(\\alpha + \\beta x_i\\). This modeling framework effectively captures the systematic variation in death probabilities as a function of dose, providing a robust tool for analyzing bioassay data. The analysis includes fixing the sample sizes \\(n_i\\) and dose levels \\(x_i\\) as constants, focusing the inference solely on the parameters \\(\\alpha\\) and \\(\\beta\\). By employing this structured approach, we can interpret the parameters’ effects on the probability of death and make informed decisions about safe dose levels based on the computed LD50, the dose at which the death probability is 50%. ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#modeling-the-doseresponse-relation",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#modeling-the-doseresponse-relation"
  },"73": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Computational Setup for Bayesian Inference (grid for posterior computation)",
    "content": "Here, we set up a grid over the parameter space and calculate the posterior distribution across this grid. To perform Bayesian inference, we establish a grid over the logistic model parameters, systematically evaluating the posterior distribution across a spectrum of values. This grid approach is essential for approximating the posterior where analytical solutions are unattainable. Compute the posterior density in grid. | usually should be computed in logarithms! | with alternative prior, check that range and spacing of A and B are sensible | . A = seq(-4, 8, length.out = 50) B = seq(-10, 40, length.out = 50) . Making vectors that contain all pairwise combinations of A and B . cA &lt;- rep(A, each = length(B)) cB &lt;- rep(B, length(A)) . We define a helper function to calculate the log-likelihood, then apply it across our dataset to estimate the joint likelihood over the grid. This helper function will calculate the log likelihood given a dataframe with x, y, and n and evaluation points a and b. For the likelihood see BDA3 p. 75 log1p(x) computes log(x+1) in numerically more stable way. logl &lt;- function(df, a, b) df['y']*(a + b*df['x']) - df['n']*log1p(exp(a + b*df['x'])) . Calculating likelihood for each combination of parameters: apply logl function for each observation ie. each row of data frame of x, n and y . p &lt;- apply(df1, 1, logl, cA, cB) %&gt;% # sum the log likelihoods of observations # and exponentiate to get the joint likelihood rowSums() %&gt;% exp() # Convert log-likelihoods to likelihoods and normalise to obtain posterior probabilities . ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#computational-setup-for-bayesian-inference-grid-for-posterior-computation",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#computational-setup-for-bayesian-inference-grid-for-posterior-computation"
  },"74": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Sampling from the posterior distribution",
    "content": "We approximate the Bayesian posterior by sampling from the grid, with each sample weighted by its likelihood. Adding random jitter to these samples enhances the continuity of our estimates, facilitating more robust inference from our discrete approximation. We follow a detailed procedure to sample from the grid, ensuring that each step contributes to an accurate representation of the posterior distribution: . | Compute Marginal Posterior Distributions: We start by calculating the marginal posterior distribution for \\(\\alpha\\) by numerically summing over all possible values of \\(\\beta\\) in the grid. This step aggregates the contributions of each \\(\\beta\\) value to the probability of \\(\\alpha\\), forming a comprehensive view of \\(\\alpha\\)’s likelihood given the data. | Sampling Steps: . | For each of the 1000 iterations (s = 1 to 1000): . | a. Draw \\(\\alpha_s\\): Sample \\(\\alpha\\) values from the discretely computed marginal distribution \\(p(\\alpha | y)\\). This sampling can be seen as a discrete analogue of the inverse cumulative distribution function (CDF) method, ensuring that our samples are drawn according to their computed probabilities. | b. Draw \\(\\beta_s\\): For each \\(\\alpha_s\\) drawn, sample \\(\\beta_s\\) from the conditional distribution \\(p(\\beta | \\alpha, y)\\), reflecting the dependency of \\(\\beta\\) on the sampled values of \\(\\alpha\\). | c. Add Jitter: To each sampled pair \\((\\alpha_s, \\beta_s)\\), add a uniform random jitter centered at zero. The jitter width is set to half the spacing between grid points, smoothing the transition between discrete grid points and creating a more continuous distribution. | . | . | . Sample from the grid (with replacement) . Sample from the grid, with replacement, (the posterior distribution) to approximate Bayesian estimates and adding jitter for plot continuity. nsamp &lt;- 1000 samp_indices &lt;- sample(length(p), size = nsamp, replace = T, prob = p/sum(p)) samp_A &lt;- cA[samp_indices[1:nsamp]] samp_B &lt;- cB[samp_indices[1:nsamp]] . Add random jitter, see BDA3 p. 76 . samp_A &lt;- samp_A + runif(nsamp, (A[1] - A[2])/2, (A[2] - A[1])/2) samp_B &lt;- samp_B + runif(nsamp, (B[1] - B[2])/2, (B[2] - B[1])/2) head(samp_A) . ## [1] 1.8361911 0.5603304 0.6314775 1.2997661 1.9234143 0.1217990 . head(samp_B) . ## [1] 20.861506 9.061151 10.383111 10.018550 12.449213 8.442394 . Analysing LD50 and its challenges . We create a dataframe for plots and visualising the logistic regression curves. This contains the sampled parameter values and corresponding LD50 calculations. A critical measure in bioassay studies is the LD50, the dose level at which the probability of death is 50%. We compute this from the model parameters and address the interpretative challenges when the dose-response relationship might not be monotonically increasing, particularly when \\(\\beta \\leq 0\\). samps &lt;- data_frame(ind = 1:nsamp, alpha = samp_A, beta = samp_B) %&gt;% mutate(ld50 = - alpha/beta) head(samps) . ## # A tibble: 6 × 4 ## ind alpha beta ld50 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.84 20.9 -0.0880 ## 2 2 0.560 9.06 -0.0618 ## 3 3 0.631 10.4 -0.0608 ## 4 4 1.30 10.0 -0.130 ## 5 5 1.92 12.4 -0.155 ## 6 6 0.122 8.44 -0.0144 . ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#sampling-from-the-posterior-distribution",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#sampling-from-the-posterior-distribution"
  },"75": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Plot draws of logistic curves",
    "content": "We compile various visualisations, including logistic regression curves and the distribution of LD50 estimates, to interpret the model’s effectiveness and the implications of the parameters. These visualisations underscore the variability and uncertainty in the parameter estimates, providing a comprehensive view of the Bayesian inference process. invlogit &lt;- plogis xr &lt;- seq(-1.5, 1.5, length.out = 100) dff &lt;- pmap_df(samps[1:100,], ~ data_frame(x = xr, id=..1, f = invlogit(..2 + ..3*x))) ppost &lt;- ggplot(df1, aes(x=x, y=y/n)) + geom_line(data=dff, aes(x=x, y=f, group=id), linetype=1, color='blue', alpha=0.2) + geom_point(size=2, color='red') + # scale_x_continuous(breaks = df1$x, minor_breaks=NULL, limits = c(-1.5, 1.5)) + # scale_y_continuous(breaks = seq(0,1,length.out=3), minor_breaks=NULL) + labs(title = 'Bioassay', x = 'Dose (log g/ml)', y = 'Proportion of deaths') + ggrepel::geom_text_repel(aes(label=x), vjust=-0.5, hjust=1.5, color='black') + labs(title = 'Bioassay deaths and LD50 sampling', x = 'Dose (log g/ml)', y = 'Number of deaths') . add 50% deaths line and LD50 dots . ppost + geom_hline(yintercept = 0.5, linetype = 'dashed', color = 'grey20') + geom_point(data=samps[1:100,], aes(x=ld50, y=0.5), color='black', alpha=0.5) + annotate(geom = \"text\", x = 1.25, y = 0.54, label = \"LD50\") . Create a plot of the posterior density . # limits for the plots xl &lt;- c(-2, 8) yl &lt;- c(-2, 40) pos &lt;- ggplot(data = data.frame(cA ,cB, p), aes(cA, cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(title = 'Posterior density evaluated in grid', x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) . Plot of the samples . sam &lt;- ggplot(data = samps) + geom_point(aes(alpha, beta), color = 'blue') + coord_cartesian(xlim = xl, ylim = yl) + labs(title = 'Posterior draws', x = 'alpha', y = 'beta') . Combine the plots . grid.arrange(pos, sam, nrow=2) . ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#plot-draws-of-logistic-curves",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#plot-draws-of-logistic-curves"
  },"76": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Plot of the histogram of LD50",
    "content": "his &lt;- ggplot(data = samps) + geom_histogram(aes(ld50), binwidth = 0.02, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.5, 0.5)) + labs(x = 'LD50 = -alpha/beta') his . ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#plot-of-the-histogram-of-ld50",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#plot-of-the-histogram-of-ld50"
  },"77": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Posterior summaries and decision making",
    "content": "Interpreting LD50 estimates . The LD50, or the dose at which there is a 50% chance of a fatal outcome, is a crucial metric in toxicology. It serves as a benchmark for regulatory bodies to set safe exposure levels. By computing LD50 from our Bayesian model, we provide a probabilistic assessment of toxicity, which is more informative than single-point estimates derived from frequentist methods. In this analysis, LD50 values are derived from the distribution of the sampled parameters, alpha and beta. These parameters are estimated from the logistic regression model, reflecting how the probability of death changes with dosage. The variability in these estimates, illustrated in our histograms and scatter plots, reflects the uncertainty inherent in experimental data and model assumptions. Decision making implications . The distribution of LD50 values aids regulatory decision-making by quantifying the risk associated with different dosage levels. Regulators can use this information to determine safe dosage limits, balancing efficacy against potential harm. For instance, if the posterior probability of LD50 being below a certain threshold is high, it may suggest that the drug can be considered relatively safe up to that threshold. Conversely, a wide spread in the LD50 estimates may indicate the need for additional studies or more conservative dosage recommendations. Practical considerations . The Bayesian approach offers several advantages for decision-making: - Probabilistic nature: It provides a full probability distribution of outcomes, allowing for a nuanced understanding of risk. - Incorporation of prior information: Prior clinical data or expert opinion can be formally incorporated, enhancing the robustness of the inference. - Flexibility in model specifications: Bayesian models can be easily extended to include more complex relationships or hierarchical structures, which can be crucial for multi-phase trials. Regulatory and ethical considerations . Regulatory decisions based on LD50 estimates must also consider ethical implications, especially in terms of acceptable risk levels. The Bayesian framework’s flexibility allows for the explicit inclusion of ethical considerations into the model, such as different weights to outcomes based on severity. This comprehensive approach ensures that drug approval and dosage recommendations are based on a thorough understanding of both statistical evidence and practical implications, leading to safer and more effective therapeutic interventions. ",
    "url": "/pages/bayesian_multiparameter_bioassay.html#posterior-summaries-and-decision-making",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html#posterior-summaries-and-decision-making"
  },"78": {
    "doc": "Bayes multiparameter bioassay demo",
    "title": "Bayes multiparameter bioassay demo",
    "content": " ",
    "url": "/pages/bayesian_multiparameter_bioassay.html",
    
    "relUrl": "/pages/bayesian_multiparameter_bioassay.html"
  },"79": {
    "doc": "Bayes multiparameter models",
    "title": "Bayes multiparameter models",
    "content": "Last update: . ## [1] \"2024-11-30\" . This doc was built with: rmarkdown::render(\"bayesian_demo3_1_4.Rmd\", output_file = \"../pages/bayesian_multiparameter_models.md\") . This example is described in the textbook: Bayesian Data Analysis, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. Third edition, (BDA3), http://www.stat.columbia.edu/~gelman/book/, chapter 3. The code is based on a version by Aki Vehtari and Markus Paasiniemi. | Section 3.2: Discusses the normal distribution with unknown mean and variance. | Sections 3.4 and 3.5: Address inference for the multinomial and multivariate normal distributions, the simplest models for discrete and continuous multivariate data, respectively. | Conclusion: The chapter concludes with an examination of a nonconjugate logistic regression model, implemented via numerical computation of the posterior density on a grid. | . ",
    "url": "/pages/bayesian_multiparameter_models.html",
    
    "relUrl": "/pages/bayesian_multiparameter_models.html"
  },"80": {
    "doc": "Bayes multiparameter models",
    "title": "Introduction to nuisance parameters in Bayesian statistics",
    "content": "In the realm of statistics, nearly every practical problem encompasses multiple unknown or unobservable quantities. The simplicity of the Bayesian approach shines when managing such complexities, offering clear advantages over alternative methods of inference. Often, although a problem may present several parameters of interest, attention is usually focused on just one or a few. Definitions . Marginal posterior distribution: The probability distribution of the parameters of interest, obtained by integrating out the nuisance parameters from the joint posterior distribution. Nuisance parameters: Parameters included in the model to enhance its realism and validity, though they are not the primary focus of analysis. Joint posterior distribution: The probability distribution representing the likelihood of all parameter values given the observed data. Aim of Bayesian analysis . The primary goal of Bayesian analysis is to determine the marginal posterior distribution of the parameters of interest. This is achieved through a two-step process: . | Obtaining the joint posterior distribution: Initially, the complete joint posterior distribution encompassing all unknowns is required. | Integrating over nuisance parameters: Subsequently, this distribution is integrated over the parameters that are not of immediate interest, yielding the desired marginal distribution. | . \\[p(\\theta_1 \\mid y) = \\int p(\\theta_1, \\theta_2 \\mid y) \\, d\\theta_2\\] Alternatively, by using simulation: . | Simulation sampling: Samples are drawn from the joint posterior distribution, concentrating on the parameters of interest and disregarding the values of other unknowns. | . Nuisance parameters . Frequently in statistical problems, many parameters, although essential for constructing a realistic model, are not the focus of inference. These parameters are typically referred to as nuisance parameters. Example: An exemplary instance of nuisance parameters is the scale of random errors in measurement problems. ",
    "url": "/pages/bayesian_multiparameter_models.html#introduction-to-nuisance-parameters-in-bayesian-statistics",
    
    "relUrl": "/pages/bayesian_multiparameter_models.html#introduction-to-nuisance-parameters-in-bayesian-statistics"
  },"81": {
    "doc": "Bayes multiparameter models",
    "title": "Averaging over ‘Nuisance Parameters’",
    "content": "Explanation of symbols . \\(\\theta\\): Total parameters of interest, split into two components \\(\\theta_1\\) and \\(\\theta_2\\). \\(\\theta_1\\) and \\(\\theta_2\\): \\(\\theta_1\\) represents parameters of primary interest, while \\(\\theta_2\\) represents nuisance parameters. \\(y\\): Observed data. \\(\\mu\\) and \\(\\sigma^2\\): Parameters commonly used in normal distribution models, where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance. \\(p(\\theta_1 \\mid y)\\): Marginal posterior distribution of \\(\\theta_1\\) given data \\(y\\). \\(p(\\theta_1, \\theta_2 \\mid y)\\): Joint posterior distribution of \\(\\theta_1\\) and \\(\\theta_2\\) given data \\(y\\). \\(\\mathcal{N}(\\mu, \\sigma^2)\\): Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). To express the ideas of joint and marginal posterior distributions mathematically, suppose \\(\\theta\\) has two parts, each of which can be a vector, \\(\\theta = (\\theta_1, \\theta_2)\\), and further suppose that we are only interested (at least for the moment) in inference for \\(\\theta_1\\), so \\(\\theta_2\\) may be considered a ‘nuisance’ parameter. For instance, in the simple example, . \\[y \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2),\\] in which both \\(\\mu\\) (equivalent to \\(\\theta_1\\)) and \\(\\sigma^2\\) (equivalent to \\(\\theta_2\\)) are unknown, interest commonly centers on \\(\\mu\\). We seek the conditional distribution of the parameter of interest given the observed data; in this case, \\(p(\\theta_1 \\mid y)\\). This is derived from the joint posterior density, . \\[p(\\theta_1, \\theta_2 \\mid y) \\propto p(y \\mid \\theta_1, \\theta_2)p(\\theta_1, \\theta_2),\\] by averaging over \\(\\theta_2\\): . \\[p(\\theta_1 \\mid y) = \\int p(\\theta_1, \\theta_2 \\mid y) \\, d\\theta_2.\\] Alternatively, the joint posterior density can be factored to yield . \\[\\begin{equation} p(\\theta_1 \\mid y) = \\int Z p(\\theta_1 \\mid \\theta_2, y) p(\\theta_2 \\mid y) d\\theta_2, \\tag{1}\\label{eq:one} \\end{equation}\\] which shows that the posterior distribution of interest, \\(p(\\theta_1 \\mid y)\\), is a mixture of the conditional posterior distributions given the nuisance parameter, \\(\\theta_2\\), where \\(p(\\theta_2 \\mid y)\\) is a weighting function for the different possible values of \\(\\theta_2\\). The weights depend on the posterior density of \\(\\theta_2\\) and thus on a combination of evidence from data and prior model. The averaging over nuisance parameters \\(\\theta_2\\) can be interpreted generally; for example, \\(\\theta_2\\) can include a discrete component representing different possible sub-models. We rarely evaluate the integral (eqn \\ref{eq:one}) explicitly, but it suggests an important practical strategy for both constructing and computing with multiparameter models. Posterior distributions can be computed by marginal and conditional simulation, first drawing \\(\\theta_2\\) from its marginal posterior distribution and then \\(\\theta_1\\) from its conditional posterior distribution, given the drawn value of \\(\\theta_2\\). In this way, the integration embodied in (eqn \\ref{eq:one}) is performed indirectly. A canonical example of this form of analysis is provided by the normal model with unknown mean and variance, to which we now turn. Explanation of averaging over nuisance parameters . Let’s break down the equation to show how the joint posterior distribution is constructed from the likelihood and the prior distribution. Objective . To derive the marginal posterior distribution of \\(\\theta_1\\) by integrating out \\(\\theta_2\\), considered a nuisance parameter, from the joint posterior distribution. Step-by-step breakdown . | Joint posterior distribution: . | Equation: \\(p(\\theta_1, \\theta_2 \\mid y) \\propto p(y \\mid \\theta_1, \\theta_2) \\times p(\\theta_1, \\theta_2)\\) | Explanation: This formula shows the joint posterior distribution as a product of the likelihood of the data given the parameters and the prior beliefs about these parameters. The term “proportional to” indicates that after considering the likelihood and prior, the result must be normalised to ensure it sums to one across all possible values of \\(\\theta_1\\) and \\(\\theta_2\\). | . | Visual breakdown: . | Prior layer: Represents initial beliefs about the parameters before seeing the data. | Likelihood layer: Adjusts beliefs based on how likely the observed data is under various parameter values. | Normalisation: Ensures the total probability across all parameter values equals one. | . | Marginal posterior distribution: . | Equation: \\(p(\\theta_1 \\mid y) = \\int p(\\theta_1, \\theta_2 \\mid y) \\, d\\theta_2\\) | Explanation: By integrating out \\(\\theta_2\\), this operation focuses solely on \\(\\theta_1\\), summing over all potential influences of \\(\\theta_2\\) to isolate the effect on \\(\\theta_1\\). This represents averaging over all possible values of \\(\\theta_2\\), which adjusts \\(\\theta_1\\)’s distribution to reflect its combined impact. | . | Factorisation and practical computation: . | Equation: \\(p(\\theta_1 \\mid y) = \\int Z p(\\theta_1 \\mid \\theta_2, y) p(\\theta_2 \\mid y) d\\theta_2\\) | Components: . | \\(p(\\theta_1 \\mid \\theta_2, y)\\): The conditional posterior of \\(\\theta_1\\) given \\(\\theta_2\\) and data. | \\(p(\\theta_2 \\mid y)\\): Acts as a weighting function in the integration, representing the marginal posterior of \\(\\theta_2\\). | . | Explanation: This factorisation views the marginal posterior as a weighted average of conditional posteriors, emphasising a practical simulation-based method to handle complex integrations. | . | Simulation as a practical solution: . | Procedure: . | Draw \\(\\theta_2\\): From its marginal posterior distribution. | Draw \\(\\theta_1\\): From the conditional distribution given the sampled \\(\\theta_2\\). | . | Explanation: This simulation effectively performs the integration by sampling, which is useful when direct computation is infeasible or impractical. | . | . To visualise how these components come together, you can think of it as layers: . | Base layer (Prior distribution): Start with your initial beliefs about the parameters (before seeing the data). | Middle layer (Likelihood): Adjust these beliefs by the likelihood of the observed data under various parameter values. | Top layer (Normalisation): Scale the result so that the total probability across all parameter values equals 1. | . Step-by-step visualization . The goal of these visualizations is to explore the effects of the nuisance parameter \\(\\theta_2\\) on the distribution of \\(\\theta_1\\). We first observe the overall distribution of \\(\\theta_2\\) and then focus on how specific values influence \\(\\theta_1\\). Step 1: Visualizing the distribution of \\(\\theta_2\\) . We start by drawing a histogram to understand the typical range and distribution of \\(\\theta_2\\). This visualization helps identify the common values and the spread of \\(\\theta_2\\). For detailed analysis, we specifically highlight the bar at \\(x = 1\\) to observe its frequency and to mark it for further scrutiny. # Simulate some data set.seed(123) data &lt;- data.frame( theta1 = rnorm(10000), theta2 = rnorm(10000) ) . Step 2: Examining the effect of a highlighted \\(\\theta_2\\) on \\(\\theta_1\\) . Next, we demonstrate how varying \\(\\theta_2\\) affects \\(\\theta_1\\) by creating conditional histograms. We highlight the range around \\(x = 1\\) to see how \\(\\theta_1\\) behaves when \\(\\theta_2\\) is near this specific value. This step visualizes the conditional effects of \\(\\theta_2\\) on \\(\\theta_1\\) across different slices, with a particular focus on the highlighted region. Integrating out \\(\\theta_2\\) in Bayesian analysis is about marginalizing over this parameter to refine our estimates for \\(\\theta_1\\). This involves averaging over all possible values of \\(\\theta_2\\), where each value is weighted by its likelihood: . | Highlighting an example value: By highlighting \\(x = 1\\) in the visualizations, we’re not limiting our analysis to this point but rather using it to exemplify how specific values within the range of \\(\\theta_2\\) can influence the outcome for \\(\\theta_1\\). It serves as a focal point for comparison. | Complete integration: The full integration over \\(\\theta_2\\) considers every part of its distribution. Each histogram slice contributes to a comprehensive understanding of how \\(\\theta_1\\) is influenced by the entire spectrum of \\(\\theta_2\\). | . ",
    "url": "/pages/bayesian_multiparameter_models.html#averaging-over-nuisance-parameters",
    
    "relUrl": "/pages/bayesian_multiparameter_models.html#averaging-over-nuisance-parameters"
  },"82": {
    "doc": "Bayes multiparameter models",
    "title": "Normal model with unknown mean and variance (BDA3 section 3.2 on p. 64).",
    "content": "Multivariate joint distribution, conditional distribution, marginal distribution, marginalization and posterior predictive distribution . ggplot2, grid, and gridExtra are used for plotting, tidyr for manipulating data frames . Generic part common for Demos 3.1-3.4 . Data . y &lt;- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114) . Sufficient statistics . n &lt;- length(y) s2 &lt;- var(y) my &lt;- mean(y) . Factorize the joint posterior p(mu,sigma2|y) to p(sigma2|y)p(mu|sigma2,y) Sample from the joint posterior using this factorization . # helper functions to sample from and evaluate # scaled inverse chi-squared distribution rsinvchisq &lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...) dsinvchisq &lt;- function(x, nu, s2){ exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x) } . Sample 1000 random numbers from p(sigma2|y) . ns &lt;- 1000 sigma2 &lt;- rsinvchisq(ns, n-1, s2) . Sample from p(mu|sigma2,y) . mu &lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2)) . Create a variable sigma and sample from predictive distribution p(ynew|y) for each draw of (mu, sigma) . sigma &lt;- sqrt(sigma2) ynew &lt;- rnorm(ns, mu, sigma) . For mu, sigma and ynew compute the density in a grid ranges for the grids . t1l &lt;- c(90, 150) t2l &lt;- c(10, 60) nl &lt;- c(50, 185) t1 &lt;- seq(t1l[1], t1l[2], length.out = ns) t2 &lt;- seq(t2l[1], t2l[2], length.out = ns) xynew &lt;- seq(nl[1], nl[2], length.out = ns) . Compute the exact marginal density of mu . # multiplication by 1./sqrt(s2/n) is due to the transformation of # variable z=(x-mean(y))/sqrt(s2/n), see BDA3 p. 21 pm &lt;- dt((t1-my) / sqrt(s2/n), n-1) / sqrt(s2/n) . Estimate the marginal density using samples and ad hoc Gaussian kernel approximation . pmk &lt;- density(mu, adjust = 2, n = ns, from = t1l[1], to = t1l[2])$y . ",
    "url": "/pages/bayesian_multiparameter_models.html#normal-model-with-unknown-mean-and-variance-bda3-section-32-on-p64",
    
    "relUrl": "/pages/bayesian_multiparameter_models.html#normal-model-with-unknown-mean-and-variance-bda3-section-32-on-p64"
  },"83": {
    "doc": "Benchmarking pipeline output",
    "title": "Benchmarking pipeline output",
    "content": "To compare the output of GATK and DeepVariant, we process the data to first look at a subset of known variants for which we have “ground truth” Sanger sequencing confirmatation of which subjectsare carriers and genotype. ",
    "url": "/pages/benchmark_pipelines.html#benchmarking-pipeline-output",
    
    "relUrl": "/pages/benchmark_pipelines.html#benchmarking-pipeline-output"
  },"84": {
    "doc": "Benchmarking pipeline output",
    "title": "Script: 17_get_momic_known_subset.sh",
    "content": "This script is designed to process VCF files by reading a list of variants at specific to genomic positions, known to carry disease-causing variants, using various command-line tools to extract and index the data. | Environment Setup and Loading Modules: The script starts by setting up the environment to stop on errors (set -e) and loading necessary modules such as vcftools, bcftools, and vcflib for handling VCF files. | Defining Known Variant List: A list of known variant positions is sourced, which is used to filter down and focus the analysis on specific genomic locations that are of interest. Format: No header and two columns for chromosome and position: chr1 11790681. | Copying and Indexing VCF Files: The script copies VCF files from a shared directory and indexes them using bcftools index, allowing efficient querying of the VCF data. | Extracting Chromosome Numbers and Looping: It extracts unique chromosome identifiers from the position list and processes each chromosome separately. This ensures that the analysis is streamlined and efficiently uses resources. | Defining Input Paths for Each Source: Multiple input sources for VCF files are defined, including unfiltered GATK, VQSR processed files, and files processed using BCFTOOL_QC and DeepVariant. These represent different stages or types of quality control and variant calling. | Filtering VCF for Known Positions: For each chromosome and each data source, the script uses bcftools view to filter VCF files to only include variants from the known positions list. The results are outputted in both VCF and TSV formats. | Indexing and Converting VCF to TSV: The filtered VCF files are re-indexed and converted to TSV format using vcf2tsv, facilitating easier data manipulation in subsequent analyses, typically done in environments like R. | Logging and Error Handling: Throughout the script, there are numerous echo statements used to log the process, which is useful for debugging and tracking the progress of the script execution. | . ",
    "url": "/pages/benchmark_pipelines.html#script-17_get_momic_known_subsetsh",
    
    "relUrl": "/pages/benchmark_pipelines.html#script-17_get_momic_known_subsetsh"
  },"85": {
    "doc": "Benchmarking pipeline output",
    "title": "Script: 18_momic_known_subset_interpret.R",
    "content": "The R script uses libraries such as dplyr, ggplot2, and patchwork to analyze the filtered data. The main focus of this script is on counting overlaps between datasets and visualizing these overlaps to understand how different variant calling methods compare. | Data Loading and Preparation: The script loads data from TSV files, ensuring correct data types and selecting relevant columns. This preparation is crucial for accurate downstream analysis. | Combining Data: Data from different sources are combined into a single dataframe, which is then used to identify shared and unique variants. | Analyzing Overlaps: The script identifies variants that are shared across different data sources and those that are unique to each source. It summarizes these findings both in text and visually using histograms and Venn diagrams. | Visualizing Data: Using ggplot2 and patchwork, the script creates visual summaries of the overlaps and differences between the data sources, providing insights into the consistency and discrepancies of variant calling methods. | . ",
    "url": "/pages/benchmark_pipelines.html#script-18_momic_known_subset_interpretr",
    
    "relUrl": "/pages/benchmark_pipelines.html#script-18_momic_known_subset_interpretr"
  },"86": {
    "doc": "Benchmarking pipeline output",
    "title": "Summary",
    "content": "The Bash script efficiently prepares and filters genomic data for analysis, focusing specifically on a subset of known disease-related variants across different sources and chromosome segments. The R script then takes this prepared data to perform a comparative analysis, visualizing overlaps and unique findings in an interpretable manner. This two-step approach allows for rigorous analysis of genomic data while focusing on specific areas of interest, such as disease-related genetic variants. ",
    "url": "/pages/benchmark_pipelines.html#summary",
    
    "relUrl": "/pages/benchmark_pipelines.html#summary"
  },"87": {
    "doc": "Benchmarking pipeline output",
    "title": "Benchmarking pipeline output",
    "content": "Last update: 20240819 . ",
    "url": "/pages/benchmark_pipelines.html",
    
    "relUrl": "/pages/benchmark_pipelines.html"
  },"88": {
    "doc": "BeviMed",
    "title": "BeviMed",
    "content": ". | BeviMed . | Talks on the topics | Introduction | Material and Methods . | Model Specification | Inference | . | . | References | . This page is a work in progress and thus contains content quoted directly from: Greene, Daniel et al. “A Fast Association Test for Identifying Pathogenic Variants Involved in Rare Diseases.” The American Journal of Human Genetics, Volume 101, Issue 1, pages 104 - 114. http://dx.doi.org/10.1016/j.ajhg.2017.05.015. ",
    "url": "/pages/bevimed.html#bevimed",
    
    "relUrl": "/pages/bevimed.html#bevimed"
  },"89": {
    "doc": "BeviMed",
    "title": "Talks on the topics",
    "content": ". | https://www.youtube.com/watch?v=0SFGjWeGI0I | https://www.youtube.com/watch?v=9f4BOQvN_E4 | . ",
    "url": "/pages/bevimed.html#talks-on-the-topics",
    
    "relUrl": "/pages/bevimed.html#talks-on-the-topics"
  },"90": {
    "doc": "BeviMed",
    "title": "Introduction",
    "content": "In principle, Bayesian inference lends itself well to rare variant association analysis because it provides a coherent framework for sharing information across variants and provides a natural way of incorporating prior information on variant pathogenicity. The variational Bayes discrete mixture method (vbdm),1 the Bayesian risk index,2 and the Bayesian rare variant detector (BRVD)3 all model a mixture of pathogenic and non-pathogenic variants in a locus, but they employ additive models of disease risk or severity more suited to complex rather than rare diseases caused by dominant or recessive inheritance of one or two pathogenic alleles. Here we present a Bayesian model in which disease risk depends on the genotypes at rare variants in a locus, a latent mode of inheritance, and a latent partition of variants into pathogenic and non-pathogenic subsets. Different modes of inheritance are modeled by conditioning the probability of case status on the number of pathogenic alleles and the ploidy for each individual at the variants. Thus, disease risk due to compound heterozygosity or X-linked inheritance is explicitly accommodated. Prior knowledge concerning variant pathogenicity can be incorporated in the form of shifts in the log odds of pathogenicity relative to a global mean. By placing a vague prior distribution on the scale of these shifts, the usefulness or otherwise of these co-data are accounted for flexibly to maximize power. For a given set of variants, inference is performed by comparing the model described above with a baseline model in which disease risk is independent of the genotypes. The mode of inheritance and the pathogenicity of each variant, conditional on an association, can be inferred through the posterior distributions of parameters in the model. Particular classes of variants in a locus may be the only ones that confer disease risk. For example, only variants in the 50 UTR region or only high-impact coding variants may be involved. Our method can compare models fitted to different classes of variants in order to infer which ones are responsible for disease. Typically the inference process would be repeated over many sets of variants selected from different loci throughout the genome. The procedures are implemented in an efficient R package called BeviMed, which stands for Bayesian evaluation of variant involvement in Mendelian disease. ",
    "url": "/pages/bevimed.html#introduction",
    
    "relUrl": "/pages/bevimed.html#introduction"
  },"91": {
    "doc": "BeviMed",
    "title": "Material and Methods",
    "content": "Model Specification . Let \\(y\\) be a binary vector of length \\(N\\) indicating whether individual \\(i\\) is a case (\\(y_i = 1\\)) or a control (\\(y_i = 0\\)) subject with respect to a particular disease. Suppose \\(k\\) rare variants are under consideration (typically in a particular genomic region) and the genotype for individual \\(i\\) at variant \\(j\\) is coded in the \\(i\\)th row and \\(j\\)th column of the genotype matrix \\(G\\). A genotype of 0 or 2 denotes homozygosity for the common or minor allele, respectively, and a genotype of 1 denotes heterozygosity. Under a baseline model, labeled \\(\\gamma = 0\\), \\(y\\) is independent of \\(G\\) and all individuals have a probability of being a case \\(\\tau_0\\). Under the association model, labeled \\(\\gamma = 1\\), individuals either have or do not have a pathogenic configuration of alleles and have probabilities of being a case subject \\(\\pi\\) and \\(\\tau\\), respectively. Whether or not an individual has a pathogenic configuration of alleles depends on a function \\(f\\) of the genotypes \\(G_i\\) of that individual, a latent binary vector \\(z\\) indicating which of the \\(k\\) variants are pathogenic, a value \\(s_i\\) equal to the ploidy of the individual at the variant sites, and a variable \\(m\\) representing the mode of inheritance governing the disease etiology though the \\(k\\) variants: . \\[\\gamma = 0 : \\mathbb{P}(y_i = 1) = \\tau_0,\\] \\[\\gamma = 1 : \\mathbb{P}(y_i = 1) = \\begin{cases} \\tau &amp; \\text{if } f(G_{i \\cdot}, z, s_i, m) = 2, \\\\ \\pi &amp; \\text{if } f(G_{i \\cdot}, z, s_i, m) = 1. \\end{cases}\\] The function \\(f\\) can represent a dominant inheritance model or a recessive inheritance model that accounts for sex-dependent differences in ploidy on the X chromosome (i.e., X-linked recessive inheritance), depending on variable \\(m \\in \\{m_{\\text{dom}}, m_{\\text{rec}}\\}\\): . \\[f(G_{i \\cdot}, z, s_i, m_{\\text{dom}}) = 1 \\left( \\sum_j G_{ij} z_j \\geq 1 \\right),\\] \\[f(G_{i \\cdot}, z, s_i, m_{\\text{rec}}) = 1 \\left( \\sum_j G_{ij} z_j \\geq s_i \\right).\\] Thus, the interpretation of \\(z\\) depends on the mode of inheritance. In order to have a pathogenic allele configuration, individual \\(i\\) requires at least one allele at a variant for which \\(z_j = 1\\) under a dominant model, but \\(s_i\\) alleles under a recessive model. If genotypes are phased, then a requirement that the \\(s_i\\) pathogenic alleles are on different haplotypes can be imposed. Recent relatedness is a potential confounder because it is correlated with both case/control status and genotype and, therefore, only unrelated individuals should be included in the model. We place beta priors on all three parameters representing risk of disease: . \\[\\tau_0 \\sim \\text{Beta}(\\alpha_0, \\beta_0),\\] \\[\\tau \\sim \\text{Beta}(\\alpha_\\tau, \\beta_\\tau),\\] \\[\\pi \\sim \\text{Beta}(\\alpha_\\pi, \\beta_\\pi).\\] The mean risk of disease for individuals without a pathogenic combination of alleles in the variants under consideration is uncertain under both models, and thus we place uniform priors on \\(\\tau_0\\) and \\(\\tau\\) by default. However, as pathogenic combinations of alleles typically confer a high disease risk, we suggest setting the hyperparameters for \\(\\pi\\) to \\(\\alpha_\\pi = 6\\) and \\(\\beta_\\pi = 1\\) (i.e., with a mean of \\(\\frac{6}{7}\\)). However, the prior mean could be adapted, for example, to reflect the consistency with which the disease manifests within families. We adopt a logistic regression framework for the prior probability that the variants are pathogenic. The logit of the prior probabilities are shrunk toward a common mean, \\(\\omega\\). If prior information that discriminates between the likely pathogenicity of variants is available, it can be incorporated in the form of a covariate \\(c\\) with regression coefficient \\(\\phi\\) in the regression equation: . \\[z_j \\sim \\text{Bernoulli}(p_j),\\] \\[\\text{logit}(p_j) = \\omega + \\phi c_j.\\] One would typically place a Gaussian prior on the intercept \\(\\omega\\) but, for computational purposes, we prefer to use a logit-beta prior with hyperparameters \\(\\alpha_\\omega\\) and \\(\\beta_\\omega\\) (see Appendix A). The prior mean of \\(\\omega\\) should reflect the expected proportion of variants that are pathogenic, conditional on an association, and may depend on the filtering procedures used to select the variants to include in the model. By default, . \\[p(\\omega) = 0.20\\] reflects a prior expectation that 20% of variants are pathogenic and a prior probability of only 0.01 that the proportion of pathogenic variants exceeds 0.54. This prior is well suited to missense variants but a distribution with a higher mean should be specified if most variants are expected to be pathogenic. This would be the case if the variants under consideration are all protein truncating and thought to be functionally equivalent to each other. To ensure that \\(\\omega\\) can be interpreted as the global mean log odds of pathogenicity, the \\(c\\) are required to sum to zero. Thus, any user-supplied weights, . \\[c_j = \\tilde{c}_j - \\left(\\frac{1}{k}\\right) \\sum_{l} \\tilde{c}_l\\] are centered such that. We place a log-normal prior on the regression coefficient \\(\\phi\\) to force the effects of the \\(c_j\\) to be the same as their signs. The prior mean of \\(\\phi\\) is set to 1 so that the \\(c_j\\) are interpretable as prior shifts in the log odds of pathogenicity relative to the mean. A prior variance on \\(\\phi\\) of 0.35 ensures that the effect of the co-data can be diminished if the co-data are not informative and increased if they improve the model fit. Finally, the prior probability on the mode of inheritance parameter \\(m\\) and the model indicator parameter \\(\\gamma\\) need to be specified. By default, we set the prior probabilities for each mode of inheritance given an association to be the same, i.e., . \\[\\mathbb{P}(m = m_{\\text{dom}} | \\gamma = 1) = 0.5,\\] and we assume that there is only a 1% chance a priori of an association, i.e., . \\[\\mathbb{P}(\\gamma = 1) = 0.01.\\] However, for a particular set of variants, the choice of values for these parameters could be based on the scientific literature or reference variant databases, for example. Inference . The principal quantity of interest is the posterior probability of the model indicator \\(\\gamma\\), which can be derived from a Bayes factor comparing the two models and \\(\\mathbb{P}(\\gamma)\\). The Bayes factor has two components: the evidence under \\(\\gamma = 0\\) and the evidence under \\(\\gamma = 1\\). A closed-form expression exists for the evidence under either model and can be computed rapidly under \\(\\gamma = 0\\), irrespective of \\(y\\). However, the expression for the evidence under \\(\\gamma = 1\\) contains a sum over every possible value of \\(z\\), of which there are \\(2^k\\), and \\(k\\) is usually large enough to render this sum computationally intractable. To tackle this problem, we reviewed various methods for estimating the evidence of a model4 and chose the method of power posteriors5, which enables the evidence to be estimated by Markov chain Monte Carlo (MCMC) sampling. In this method, the MCMC is tempered, which is helpful in a variable selection setting such as ours because it makes exploration of the space of sets of pathogenic variants more efficient. Samples are drawn from a series of related distributions called power posteriors. Each power posterior has a temperature \\(t\\) between 0 and 1 and is proportional to the likelihood of the parameters to the power of \\(t\\) times the prior. These samples can be combined to obtain an estimate of the integrated likelihood (see Appendix A). Sampling for our model can be done very efficiently because an MCMC update to \\(z_j\\) entails changes only in \\(f(G_{i \\cdot}, z, s_i, m)\\) for individuals for whom \\(G_{ij} &gt; 0\\). For convenience, we estimate the evidence conditional on \\(m\\) but we can integrate over it through simple summation. Once the MCMC samples have been collected, the marginal posterior probability of \\(z\\) given \\(\\gamma\\) and \\(m\\) can be obtained directly and used for ranking variants by their likely pathogenicity. The estimated number of pathogenic variants and the expected posterior number of case subjects explained by the pathogenic variants, given \\(\\gamma = 1\\), can also be computed (see Appendix A). The posterior probability of \\(\\gamma\\) provides a natural means of ranking sets of variants from different loci across the genome. The model above assumes that the prior probabilities of variant pathogenicity are conditionally independent. However, particular classes of variants in a locus may confer disease risk, while others may be benign. We can impose a prior correlation structure on the \\(z\\) reflecting these competing hypotheses by fitting a different association model for each class of variant. If one of the hypotheses matches the true etiology of disease, then this modeling approach can improve model fit and thus increase power. Let \\(\\gamma \\in \\{1, 2, \\ldots, g\\}\\) index the association models and let \\(I_{uv}\\) indicate whether variant \\(v\\) is included in association model \\(u\\). Then, we can compute the probability of association across the competing models as: . \\[\\mathbb{P}(\\gamma &gt; 0 | y, G, c, I) = \\frac{\\sum_{u=1}^g \\mathbb{P}(y | \\gamma = u, G^{(u)}, c^{(u)}) \\mathbb{P}(\\gamma = u)}{\\sum_{u=0}^g \\mathbb{P}(y | \\gamma = u, G^{(u)}, c^{(u)}) \\mathbb{P}(\\gamma = u)},\\] where \\(G^{(u)} = G \\cdot \\{v : I_{uv} = 1\\}\\) and \\(c^{(u)} = c \\{v : I_{uv} = 1\\}\\). The prior on the model indicator, \\(\\mathbb{P}(\\gamma)\\), can be informed by external data. For example, if a gene has a high probability of loss-of-function intolerance6, then the prior corresponding to a model of high-impact variants in that gene could be up-weighted relative to competing models. We can also compute the posterior probability of variant pathogenicity averaged over all association models using the following expression: . \\[\\mathbb{P}(z | \\gamma &gt; 0, y, G, c, I) = \\frac{\\sum_{u=1}^g \\mathbb{P}(z | \\gamma = u, y, G^{(u)}, c^{(u)}) \\mathbb{P}(\\gamma = u | y, G^{(u)}, c^{(u)})}{\\sum_{u=1}^g \\mathbb{P}(\\gamma = u | y, G^{(u)}, c^{(u)})}.\\] Other quantities of interest, such as the expected posterior number of cases explained by pathogenic variants, can be averaged over models in the same way. ",
    "url": "/pages/bevimed.html#material-and-methods",
    
    "relUrl": "/pages/bevimed.html#material-and-methods"
  },"92": {
    "doc": "BeviMed",
    "title": "References",
    "content": "https://cran.r-project.org/web/packages/BeviMed/ . | Logsdon, B.A., Dai, J.Y., Auer, P.L., et al., NHLBI GO Exome Sequencing Project. A variational Bayes discrete mixture test for rare variant association. Genet. Epidemiol. 2014; 38:21-30. &#8617; . | Quintana, M.A., Berstein, J.L., Thomas, D.C., et al. Incorporating model uncertainty in detecting rare variants: the Bayesian risk index. Genet. Epidemiol. 2011; 35:638-649. &#8617; . | Liang, F., Xiong, M. Bayesian detection of causal rare variants under posterior consistency. PLoS ONE. 2013; 8:e69633. &#8617; . | Friel, N., Wyse, J. Estimating the evidence–a review. Stat. Neerl. 2012; 66:288-308. &#8617; . | Friel, N., Pettitt, A.N. Marginal likelihood estimation via power posteriors. J. R. Stat. Soc. Series B Stat. Methodol. 2008; 70:589-607. &#8617; . | Lek, M., Karczewski, K.J., Minikel, E.V., et al., Exome Aggregation Consortium. Analysis of protein-coding genetic variation in 60,706 humans. Nature. 2016; 536:285-291. &#8617; . | . ",
    "url": "/pages/bevimed.html#references",
    
    "relUrl": "/pages/bevimed.html#references"
  },"93": {
    "doc": "BeviMed",
    "title": "BeviMed",
    "content": "Last update: 20241104 . ",
    "url": "/pages/bevimed.html",
    
    "relUrl": "/pages/bevimed.html"
  },"94": {
    "doc": "BioMedIT",
    "title": "Working on BioMedIT",
    "content": "Last update: 20250101 . ",
    "url": "/pages/biomedit.html#working-on-biomedit",
    
    "relUrl": "/pages/biomedit.html#working-on-biomedit"
  },"95": {
    "doc": "BioMedIT",
    "title": "UZH VPN",
    "content": "To access BioMedIT sciCORE (Uni Basel) a whitelisted IP address is required. | Directions for vpn: https://www.zi.uzh.ch/en/support/network/vpn_ISAC.html | Client provided is: Ivanti Secure Access | Using UZH 2-FA | . ",
    "url": "/pages/biomedit.html#uzh-vpn",
    
    "relUrl": "/pages/biomedit.html#uzh-vpn"
  },"96": {
    "doc": "BioMedIT",
    "title": "BioMedIT overall",
    "content": "If you have a project on the BioMedIT network you can check the portal and the gitlab for project code. | BioMedIT portal: https://portal.dcc.sib.swiss | gitlab https://git.dcc.sib.swiss/ | . ",
    "url": "/pages/biomedit.html#biomedit-overall",
    
    "relUrl": "/pages/biomedit.html#biomedit-overall"
  },"97": {
    "doc": "BioMedIT",
    "title": "sciCORE",
    "content": "On the BioMedIT network, one cluster is the Uni Basel sciCORE Center for Scientific Computing. The project will have it’s own tenant with the project name. This will be listed in the BioMedIT portal. One on a whitelisted network, and all credential steps are complete, you can access via SSH or via the browser. | project name: x_hidden_x | username: x_hidden_x | node: sciCOREmed (scicore), University of Basel | browser access: https://sphn-momic-ondemand.scicoreplus.unibas.ch/ | ssh: ssh username@sphn-momic-login.scicore.unibas.ch | workspace: /project/home/user/ | Support email: scicore-admin@unibas.ch referencing the tenant name | support tickets: https://support.scicore.unibas.ch/servicedesk/customer/user/requests?status=open | . ",
    "url": "/pages/biomedit.html#scicore",
    
    "relUrl": "/pages/biomedit.html#scicore"
  },"98": {
    "doc": "BioMedIT",
    "title": "BioMedIT",
    "content": " ",
    "url": "/pages/biomedit.html",
    
    "relUrl": "/pages/biomedit.html"
  },"99": {
    "doc": "Bookmarks",
    "title": "Bookmarks",
    "content": "Last update: 20230621 . An unstructured log of bookmarks that we use for guidance . | GATK https://gatk.broadinstitute.org/hc/en-us | Genomics England: bioinformatics and data science https://www.genomicsengland.co.uk/bioinformatics | Genomics England: technical documents https://re-docs.genomicsengland.co.uk/further_reading/ | Genomics England: model report http://gelreportmodels.genomicsengland.co.uk/ | NIH Biowulf HPC https://hpc.nih.gov | Fastq https://en.wikipedia.org/wiki/FASTQ_format | Fastq https://knowledge.illumina.com/software/general/software-general-reference_material-list/000002211 | Fastq https://help.basespace.illumina.com/files-used-by-basespace/fastq-files | . ",
    "url": "/pages/bookmarks.html",
    
    "relUrl": "/pages/bookmarks.html"
  },"100": {
    "doc": "BWA",
    "title": "BWA",
    "content": "Last update: 20230727 . | GATK: (How to) Map reads to a reference with alternate contigs like GRCH38 https://gatk.broadinstitute.org/hc/en-us/articles/360037498992--How-to-Map-reads-to-a-reference-with-alternate-contigs-like-GRCH38 | . SMOC data arrives in multiple batches to ensure read depth, etc., resulting in multiple sets of FASTQ files per sample all of which should have distinct read group IDs (RGID). Therefore at some points we must compile all reads for a single subject. The following is from GATK, which we will also perform: . (like at the) Broad Institute, we run the initial steps of the pre-processing workflow (mapping and sorting) separately on each individual read group. Then we merge the data to produce a single BAM file for each sample (aggregation); this is done conveniently at the same time that we do the duplicate marking, by running Mark Duplicates on all read group BAM files for a sample at the same time. Then we run Base Recalibration on the aggregated per-sample BAM files. See the worked-out example below for more details. | GATK: How should I pre-process data from multiplexed sequencing and multi-library designs? https://gatk.broadinstitute.org/hc/en-us/articles/360035889471-How-should-I-pre-process-data-from-multiplexed-sequencing-and-multi-library-designs- | GATK: Read groups https://gatk.broadinstitute.org/hc/en-us/articles/360035890671 | . ",
    "url": "/pages/bwa.html",
    
    "relUrl": "/pages/bwa.html"
  },"101": {
    "doc": "BWA",
    "title": "Read groups",
    "content": "Meaning of the read group fields required by GATK . | ID = Read group identifier . | This tag identifies which read group each read belongs to, so each read group’s ID must be unique. It is referenced both in the read group definition line in the file header (starting with @RG) and in the RG:Z tag for each read record. Note that some Picard tools have the ability to modify IDs when merging SAM files in order to avoid collisions. In Illumina data, read group IDs are composed using the flowcell name and lane number, making them a globally unique identifier across all sequencing data in the world. | Use for BQSR: ID is the lowest denominator that differentiates factors contributing to technical batch effects: therefore, a read group is effectively treated as a separate run of the instrument in data processing steps such as base quality score recalibration (unless you have PU defined), since they are assumed to share the same error model. | . | PU = Platform Unit . | The PU holds three types of information, the {FLOWCELL_BARCODE}.{LANE}.{SAMPLE_BARCODE}. The {FLOWCELL_BARCODE} refers to the unique identifier for a particular flow cell. The {LANE} indicates the lane of the flow cell and the {SAMPLE_BARCODE} is a sample/library-specific identifier. Although the PU is not required by GATK but takes precedence over ID for base recalibration if it is present. In the example shown earlier, two read group fields, ID and PU, appropriately differentiate flow cell lane, marked by .4, a factor that contributes to batch effects. | . | SM = Sample . | The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample, and this is also the name that will be used for the sample column in the VCF file. Therefore it is critical that the SM field be specified correctly. When sequencing pools of samples, use a pool name instead of an individual sample name. | . | PL = Platform/technology used to produce the read . | This constitutes the only way to know what sequencing technology was used to generate the sequencing data. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO. | . | LB = DNA preparation library identifier . | MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes. | . | . ",
    "url": "/pages/bwa.html#read-groups",
    
    "relUrl": "/pages/bwa.html#read-groups"
  },"102": {
    "doc": "BWA",
    "title": "Our method for creating read group info",
    "content": "NovaSeq SMPC fastq filename . | &lt;SAMPLE_ID&gt;_&lt;NGS_ID&gt;_&lt;POOL_ID&gt;_&lt;S#&gt;_&lt;LANE&gt;_&lt;R1|R2&gt;.fastq.gz | . NovaSeq SMOC fastq header . | @&lt;instrument&gt;:&lt;run number&gt;:&lt;flowcell ID&gt;:&lt;lane&gt;:&lt;tile&gt;:&lt;x-pos&gt;:&lt;y-pos&gt; &lt;read&gt;:&lt;is filtered&gt;:&lt;control number&gt;:&lt;sample number&gt; | . Read group definitions . | ID = Read group identifier | SM = Sample | PL = Platform/technology used to produce the read | PU = Platform Unit | LB = DNA preparation library identifier | . Read group sources . | ID = sample_id , e.g. AAA073_NGS000033312_NA_S20_L004 | SM = sample_id regex field 1, e.g. AAA073 | PL = Machine and library method, e.g. “NovaSeq6000_WGS_TruSeq”, hardcoded - automate later | PU = {FLOWCELL_BARCODE}.{LANE}.{SAMPLE_BARCODE} which we can derive from fastq header 1 regex fields: 3.4., and sample_id regex field 1-2. | LB = sample_id regex field 1-2 | . sm=$(echo ${sample_id} | awk -F '_' '{print $1}') pu=$(zcat ${FILE1} | awk 'NR==1 {split($1,a,\":\"); print a[3] \".\" a[4] \".\" \"'$sm'\"}') lb=$(echo ${sample_id} | awk -F '_' '{print $1 \"_\" $2}') pl=\"NovaSeq6000_WGS_TruSeq\" echo \"ID = ${sample_id}\" echo \"SM = ${sm}\" echo \"PL = ${pl}\" echo \"PU = ${pu}\" echo \"LB = ${lb}\" `rg=\"@RG\\tID:${sample_id}\\tSM:${sm}\\tPL:${pl}\\tPU:${pu}\\tLB:${lb}\"` bwa mem \\ ${REF} \\ ${FILE1} \\ ${FILE2} \\ -R $rg \\ -v 1 -M -t 8 |\\ samtools view --threads 8 -O BAM -o ${output_file} . Check read group e.g.samtools view -H file.bam | grep '^@RG'. We can also use logs to see if we have any read group collision which should be unique. ",
    "url": "/pages/bwa.html#our-method-for-creating-read-group-info",
    
    "relUrl": "/pages/bwa.html#our-method-for-creating-read-group-info"
  },"103": {
    "doc": "Causal inference stats",
    "title": "Causal inference stats Pearl review",
    "content": "Last update: 20250101 . This document is based on [1] “J. Pearl, “Causal inference in statistics: An overview,” Statistics Surveys, 2009, link to pdf. Here is a link to the author’s homepage where you will find the key work highlighted. The complete textbook is [2] “Causality - Models, Reasoning, and Inference”, Second edition, Judea Pearl, link to author page. The same topics are introduced for a general audience in [3] “The Book of Why”, Judea Pearl and Dana Mackenzie, 2018, link to Wikipedia. This topic is widely ignored. The techincal papers are sometimes verbose and opinionated and thus are difficult to read. An alternative approach is to read the general audience introduction (link 3 above) followed by hands-on work as demonstrated in the tutorial page on causal inference in R. ",
    "url": "/pages/causal_inference_stats.html#causal-inference-stats-pearl-review",
    
    "relUrl": "/pages/causal_inference_stats.html#causal-inference-stats-pearl-review"
  },"104": {
    "doc": "Causal inference stats",
    "title": "Introduction",
    "content": "Background: The field of causal inference has advanced significantly, driven by developments in: . | Nonparametric structural equations. | Graphical models. | Integration of counterfactual and graphical methods. | . Importance: These advances are crucial for empirical sciences to address causal questions in health, social, and behavioral sciences. Objective: To summarize key advances in causal inference and illustrate their application in empirical research. ",
    "url": "/pages/causal_inference_stats.html#introduction",
    
    "relUrl": "/pages/causal_inference_stats.html#introduction"
  },"105": {
    "doc": "Causal inference stats",
    "title": "From Associational to Causal Analysis: Distinctions and Barriers",
    "content": "2.1 Basic Distinction: . | Statistical Analysis: Infers parameters from a distribution to describe static conditions. | Causal Analysis: Aims to understand data generation processes, enabling predictions about changes due to interventions or external factors. | Key Principle: Associations cannot substantiate causal claims; causal analysis requires specific causal assumptions. | . 2.2 Formulating the Distinction: . | Associational Concepts: Defined by relationships within a joint distribution (e.g., correlation, regression). | Causal Concepts: Involve relationships that extend beyond observable distributions (e.g., randomization, effect, confounding). | . 2.3 Ramifications: . | Associational vs. Causal: Confounding, often misconstrued as associational, is inherently causal as it involves discrepancies between observed associations and those under experimental conditions. | Notational Challenges: Traditional statistical methods and language are insufficient for expressing causal relationships. | . 2.4 Barrier of Untested Assumptions: . | Associational assumptions are potentially testable; causal assumptions are not without experimental control. | The clarity of causal assumptions invites scrutiny and requires a shift in how assumptions are conceptualized and communicated. | . 2.5 New Notation: . | Introduction of specific causal notation is necessary to clearly distinguish between statistical associations and causal relationships. | Counterfactual notation (e.g., \\(Y_x(u)\\) or \\(P(Y=y \\mid \\text{do}(X=x))\\)) helps express causal concepts that cannot be captured by traditional probability calculus. | . ",
    "url": "/pages/causal_inference_stats.html#from-associational-to-causal-analysis-distinctions-and-barriers",
    
    "relUrl": "/pages/causal_inference_stats.html#from-associational-to-causal-analysis-distinctions-and-barriers"
  },"106": {
    "doc": "Causal inference stats",
    "title": "The Language of Diagrams and Structural Equations",
    "content": "3.1 Linear Structural Equation Models . Historical context: Wright (1921) combined equations and diagrams to illustrate causal relationships, e.g., disease (X) causing a symptom (Y): \\(y = \\beta x + u\\) Equation (3.1) illustrates the level of disease (x) influencing the level of symptom (y) with u representing external factors. Path diagrams: Diagrams clarify causal directions, preventing misinterpretations of equations like reversing (3.1) to suggest symptoms cause diseases. Model interpretation: In path diagrams, causal influence is indicated by arrows, with their absence denoting no direct causal effect. The model includes exogenous variables (U, V) representing unexplained factors influencing endogenous variables (X, Y). 3.2 From Linear to Nonparametric Models . Evolution of Structural Equation Modeling (SEM): Originally developed for linear relationships, SEM has been extended to nonparametric and nonlinear models. Reconceptualizing effect: Effects in SEM are redefined as general capacities to transmit changes among variables, beyond algebraic coefficients. Nonparametric SEM: The new approach uses structural equations to simulate hypothetical interventions: \\(z = f_Z(w)\\) \\(x = f_X(z, v)\\) \\(y = f_Y(x, u)\\) These functions assume that variables are structurally autonomous, invariant to changes in other equations’ forms. Intervention simulation: The “do” operator (do(x)) represents interventions by setting variables to constants in models, affecting the distribution of other variables: \\(x = x_0\\) This changes the joint distribution to reflect controlled conditions, providing insights into treatment effects: \\(E(Y|do(x'0)) - E(Y|do(x0))\\) Equation (3.7) calculates the difference in expected outcomes under different treatment conditions. Identifiability and Markovian models: Identification in causal analysis is feasible under Markovian conditions, where: \\(P(v1, v2, ..., vn) = \\prod P(vi|pai)\\) Equation (3.9) expresses the joint distribution as a product of conditional distributions determined by the graph structure. Truncated factorization: After interventions, distributions are re-factorized to exclude affected variables: \\(P(v1, v2, ..., vk|do(x0)) = \\prod_{i|Vi \\notin X} P(vi|pai)|_{x=x0}\\) Equation (3.11) reflects distributions post-intervention, essential for deriving causal effects from observational data. These sections outline how causal relationships are modeled using equations and diagrams, and how these models adapt to non-linear and nonparametric forms to estimate effects of interventions in complex systems. ",
    "url": "/pages/causal_inference_stats.html#the-language-of-diagrams-and-structural-equations",
    
    "relUrl": "/pages/causal_inference_stats.html#the-language-of-diagrams-and-structural-equations"
  },"107": {
    "doc": "Causal inference stats",
    "title": "3.3 Counterfactual Analysis in Structural Models",
    "content": "Conceptual Framework: Counterfactual analysis explores causal relationships not expressible by experimental setups or \\(P(y|do(x))\\), using counterfactual notation \\(Y_x(u) = y\\) to examine potential outcomes in hypothetical scenarios. Interpretation of Counterfactuals: The approach modifies the original model by replacing variables with constants to reflect hypothetical conditions, thus formalizing counterfactual queries within structural equation models. For example, in a modified model: \\(Y_{x_0}(u,v,w) = Y_{x_0}(u) = f_Y(x_0,u)\\) This represents the potential response \\(Y\\) to a hypothetical treatment \\(x_0\\). Attribution and Susceptibility: Counterfactuals allow calculation of effects like change due to treatment, expressed as: \\(Y_{x_1} = \\beta(x_1 - x_0) + y_0\\) This signifies how the outcome \\(Y\\) would change if \\(X\\) were \\(x_1\\) instead of \\(x_0\\). ",
    "url": "/pages/causal_inference_stats.html#33-counterfactual-analysis-in-structural-models",
    
    "relUrl": "/pages/causal_inference_stats.html#33-counterfactual-analysis-in-structural-models"
  },"108": {
    "doc": "Causal inference stats",
    "title": "3.4 An Example: Non-Compliance in Clinical Trials",
    "content": "Formulating the Assumptions . Model Setup: Considers a clinical trial model with variables \\(Z\\) (randomized treatment assignment), \\(X\\) (treatment received), and \\(Y\\) (response), influenced by unobserved factors \\(U\\) and \\(V\\). Diagram of a Clinical Trial with Imperfect Compliance: . | Figure 5(a): Shows the causal relationships, including compliance and response pathways. | Exclusion Restriction: Assumes \\(Z\\) influences \\(Y\\) only through \\(X\\), not directly. | Randomization Assumption: \\(Z\\) is independent of \\(U\\) and \\(V\\), ensuring no common cause affecting both. | . Estimating Causal Effects . Non-identifiability: The causal effect of treatment on outcome is not identifiable directly due to unobserved confounders \\(V\\) and \\(U\\), without further assumptions. Instrumental Variables Approach: Linear case: Uses correlation to determine causal effect, with: \\(\\beta = \\frac{Cov(Z, Y)}{Cov(Z, X)}\\) Equation (3.22) shows the use of instrumental variables to estimate causal effects when direct measurement or control of confounders is not possible. Bounding Causal Effects . Bounds Determination: When exact causal effects can’t be identified, the analysis seeks to establish bounds on these effects based on observed data. Instrumental Inequality: Ensures any variable \\(Z\\) serves as an instrument only if it meets specific conditions, reflected by the inequality: \\(\\max \\left[ \\max P(x, y|z) \\right] \\leq 1\\) Equation (3.25) imposes constraints on the observed distribution to maintain the validity of the instrumental assumptions. Testable Implications . Detection of Violations: Specific inequalities derived from the model help detect violations of assumptions regarding the absence of direct effects of \\(Z\\) on \\(Y\\) or hidden biases. Non-compliance and Side Effects: The model assesses non-compliance impacts and side effects through observational inequalities, aiding in understanding the limitations and robustness of causal inferences in clinical trials with partial compliance. ",
    "url": "/pages/causal_inference_stats.html#34-an-example-non-compliance-in-clinical-trials",
    
    "relUrl": "/pages/causal_inference_stats.html#34-an-example-non-compliance-in-clinical-trials"
  },"109": {
    "doc": "Causal inference stats",
    "title": "4 The Language of Potential Outcomes and Counterfactuals",
    "content": "4.1 Formulating Assumptions . | Concept: The potential outcome framework utilises Yx(u), representing the outcome Y for unit u if X were x, handling this as a random variable Yx. | Axiomatic Framework: The framework postulates a comprehensive probability function covering both real and hypothetical events. | Consistency Constraints: It asserts that if X equals x, then Yx must match the observed Y. \\(X = x \\Rightarrow Yx = Y\\) Equation (4.1) aligns the potential outcome with the observed outcome when the intervention aligns with the actual treatment. | . 4.2 Performing Inferences . Conditional Ignorability: Assumes conditional independence of Yx and X given Z (Yx ⊥ X | Z), allowing for straightforward calculation of causal effects using potential outcomes: \\(P(Yx = y) = \\sum_z P(Y = y | X = x, z)P(z)\\) Equation (4.5) effectively aligns with standard covariate adjustment formulas, leveraging observed data to infer counterfactual scenarios. ",
    "url": "/pages/causal_inference_stats.html#4-the-language-of-potential-outcomes-and-counterfactuals",
    
    "relUrl": "/pages/causal_inference_stats.html#4-the-language-of-potential-outcomes-and-counterfactuals"
  },"110": {
    "doc": "Causal inference stats",
    "title": "4.3 Combining Graphs and Algebra",
    "content": ". | Translation of Assumptions: Graphs convey causal assumptions, translating into counterfactual notation to express and verify causal relationships. | Parent Sets and Exclusion Restrictions: Define relationships and independence among variables, using the structure to ensure interventions only affect relevant pathways. | Graph-Assisted Analysis: Graphs help validate and explore dependencies and independencies essential for deriving causal inferences, using criteria like d-separation for testing conditional independencies. | . ",
    "url": "/pages/causal_inference_stats.html#43-combining-graphs-and-algebra",
    
    "relUrl": "/pages/causal_inference_stats.html#43-combining-graphs-and-algebra"
  },"111": {
    "doc": "Causal inference stats",
    "title": "5 Conclusions",
    "content": "Structural Equation Models (SEMs): Presented as a formal language for articulating causal assumptions, enabling clear representation of complex causal relationships. Integration of Methods: Combining SEMs with potential outcomes enriches the statistical toolkit, allowing for robust empirical research through a unified approach that encapsulates randomization, intervention, effects, and confounding in a coherent framework. This synthesis of graphical models and algebraic potential outcomes facilitates a deeper understanding and application of causal inference techniques in statistical research, enhancing both theoretical and practical approaches to exploring causality in complex systems. ",
    "url": "/pages/causal_inference_stats.html#5-conclusions",
    
    "relUrl": "/pages/causal_inference_stats.html#5-conclusions"
  },"112": {
    "doc": "Causal inference stats",
    "title": "Causal inference stats",
    "content": " ",
    "url": "/pages/causal_inference_stats.html",
    
    "relUrl": "/pages/causal_inference_stats.html"
  },"113": {
    "doc": "Causal inference mosquito nets",
    "title": "1 Causal inference - mosquito nets and malaria",
    "content": "Last update: . ## [1] \"2024-12-31\" . This doc was built with: rmarkdown::render(\"causal_inference_whole_game.Rmd\", output_file = \"../pages/causal_inference_whole_game.md\") . This text is largely copied directly from the following source while we build an example closer to our needs. Please see the original source as follows. This example is described in the textbook: Causal Inference in R, by Malcolm Barrett, Lucy D’Agostino McGowan, and Travis Gerke. https://www.r-causal.org, chapter 2. We will use simulated data to answer a more specific question: Does using insecticide-treated bed nets compared to no nets decrease the risk of contracting malaria after 1 year? Data simulated by Dr. Andrew Heiss: . …researchers are interested in whether using mosquito nets decreases an individual’s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them. This data includes a variable that measures the likelihood of contracting malaria, something we wouldn’t likely have in real life. This let’s us know the actual effect size to understand the methods better. The simulated data is in net_data from the {causalworkshop} package, which includes ten variables: . | id : an ID variable | net and net_num : a binary variable indicating if the participant used a net (1) or didn’t use a net (0) | malaria_risk : risk of malaria scale ranging from 0-100 | income : weekly income, measured in dollars | health : a health score scale ranging from 0–100 | household : number of people living in the household | eligible : a binary variable indicating if the household is eligible for the free net program. | temperature : the average temperature at night, in Celsius | resistance : Insecticide resistance of local mosquitoes. A scale of 0–100, with higher values indicating higher resistance. | . The distribution of malaria risk appears to be quite different by net usage. # devtools::install_github(\"r-causal/causalworkshop\") . library(tidyverse) library(causalworkshop) net_data |&gt; ggplot(aes(malaria_risk, fill = net)) + geom_density(color = NA, alpha = .8) . Figure 1.1: A density plot of malaria risk for those who did and did not use nets. The risk of malaria is lower for those who use nets. In figure 1.1, the density of those who used nets is to the left of those who did not use nets. The mean difference in malaria risk is about 16.4, suggesting net use might be protective against malaria. net_data |&gt; group_by(net) |&gt; summarize(malaria_risk = mean(malaria_risk)) . ## # A tibble: 2 × 2 ## net malaria_risk ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 43.9 ## 2 TRUE 27.5 . And that’s what we see with simple linear regression, as well, as we would expect. library(broom) net_data |&gt; lm(malaria_risk ~ net, data = _) |&gt; tidy() . ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 43.9 0.377 116. 0 ## 2 netTRUE -16.4 0.741 -22.1 1.10e-95 . ",
    "url": "/pages/causal_inference_whole_game.html#1-causal-inference---mosquito-nets-and-malaria",
    
    "relUrl": "/pages/causal_inference_whole_game.html#1-causal-inference---mosquito-nets-and-malaria"
  },"114": {
    "doc": "Causal inference mosquito nets",
    "title": "1.1 Draw our assumptions using a causal diagram",
    "content": "The problem that we face is that other factors may be responsible for the effect we’re seeing. In this example, we’ll focus on confounding: a common cause of net usage and malaria will bias the effect we see unless we account for it somehow. One of the best ways to determine which variables we need to account for is to use a causal diagram. These diagrams, also called causal directed acyclic graphs (DAGs), visualize the assumptions that we’re making about the causal relationships between the exposure, outcome, and other variables we think might be related. Here’s the DAG that we’re proposing for this question. Figure 1.2: A proposed causal diagram of the effect of bed net use on malaria. This directed acyclic graph (DAG) states our assumption that bed net use causes a reduction in malaria risk. It also says that we assume: malaria risk is impacted by net usage, income, health, temperature, and insecticide resistance; net usage is impacted by income, health, temperature, eligibility for the free net program, and the number of people in a household; eligibility for the free net programs is impacted by income and the number of people in a household; and health is impacted by income. We’ll explore how to create and analyze DAGs in @sec-dags. In DAGs, each point represents a variable, and each arrow represents a cause. In other words, this diagram declares what we think the causal relationships are between these variables. In figure 1.2, we’re saying that we believe: . | Malaria risk is causally impacted by net usage, income, health, temperature, and insecticide resistance. | Net usage is causally impacted by income, health, temperature, eligibility for the free net program, and the number of people in a household. | Eligibility for the free net programs is determined by income and the number of people in a household. | Health is causally impacted by income. | . You may agree or disagree with some of these assertions. That’s a good thing! Laying bare our assumptions allows us to consider the scientific credibility of our analysis. Another benefit of using DAGs is that, thanks to their mathematics, we can determine precisely the subset of variables we need to account for if we assume this DAG is correct. Assembling DAGs In this exercise, we’re providing you with a reasonable DAG based on knowledge of how the data were generated. In real life, setting up a DAG is a challenge requiring deep thought, domain expertise, and (often) collaboration between several experts. The chief problem we’re dealing with is that, when we analyze the data we’re working with, we see the impact of net usage on malaria risk and of all these other relationships. In DAG terminology, we have more than one open causal pathway. If this DAG is correct, we have eight causal pathways: the path between net usage and malaria risk and seven other confounding pathways. Figure 1.3: In the proposed DAG, there are eight open pathways that contribute to the causal effect seen in the naive regression: the true effect (in green) of net usage on malaria risk and seven other confounding pathways (in orange). The naive estimate is wrong because it is a composite of all these effects. When we calculate a naive linear regression that only includes net usage and malaria risk, the effect we see is incorrect because the seven other confounding pathways in figure 1.3 distort it. In DAG terminology, we need to block these open pathways that distort the causal estimate we’re after. (We can block paths through several techniques, including stratification, matching, weighting, and more. We’ll see several methods throughout the book.) Luckily, by specifying a DAG, we can precisely determine the variables we need to control for. For this DAG, we need to control for three variables: health, income, and temperature. These three variables are a minimal adjustment set, the minimum set (or sets) of variables you need to block all confounding pathways. We’ll discuss adjustment sets further in @sec-dags. ",
    "url": "/pages/causal_inference_whole_game.html#11-draw-our-assumptions-using-a-causal-diagram",
    
    "relUrl": "/pages/causal_inference_whole_game.html#11-draw-our-assumptions-using-a-causal-diagram"
  },"115": {
    "doc": "Causal inference mosquito nets",
    "title": "1.2 Model our assumptions",
    "content": "We’ll use a technique called inverse probability weighting (IPW) to control for these variables, which we’ll discuss in detail in @sec-using-ps. We’ll use logistic regression to predict the probability of treatment—the propensity score. Then, we’ll calculate inverse probability weights to apply to the linear regression model we fit above. The propensity score model includes the exposure—net use—as the dependent variable and the minimal adjustment set as the independent variables. Modeling the functional form Generally speaking, we want to lean on domain expertise and good modeling practices to fit the propensity score model. For instance, we may want to allow continuous confounders to be non-linear using splines, or we may want to add essential interactions between confounders. Because these are simulated data, we know we don’t need these extra parameters (so we’ll skip them), but in practice, you often do. We’ll discuss this more in @sec-using-ps. The propensity score model is a logistic regression model with the formula net ~ income + health + temperature, which predicts the probability of bed net usage based on the confounders income, health, and temperature. propensity_model &lt;- glm( net ~ income + health + temperature, data = net_data, family = binomial() ) # the first six propensity scores head(predict(propensity_model, type = \"response\")) . ## 1 2 3 4 5 6 ## 0.2464 0.2178 0.3230 0.2307 0.2789 0.3060 . We can use propensity scores to control for confounding in various ways. In this example, we’ll focus on weighting. In particular, we’ll compute the inverse probability weight for the average treatment effect (ATE). The ATE represents a particular causal question: what if everyone in the study used bed nets vs. what if no one in the study used bed nets? . To calculate the ATE, we’ll use the broom and propensity packages. broom’s augment() function extracts prediction-related information from the model and joins it to the data. propensity’s wt_ate() function calculates the inverse probability weight given the propensity score and exposure. For inverse probability weighting, the ATE weight is the inverse of probability of receiving the treatment you actually received. In other words, if you used a bed net, the ATE weight is the inverse of the probability that you used a net, and if you did not use a net, it is the the inverse of the probability that you did not use a net. library(broom) library(propensity) net_data_wts &lt;- propensity_model |&gt; augment(data = net_data, type.predict = \"response\") |&gt; # .fitted is the value predicted by the model # for a given observation mutate(wts = wt_ate(.fitted, net)) net_data_wts |&gt; select(net, .fitted, wts) |&gt; head() . ## # A tibble: 6 × 3 ## net .fitted wts ## &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE 0.246 1.33 ## 2 FALSE 0.218 1.28 ## 3 FALSE 0.323 1.48 ## 4 FALSE 0.231 1.30 ## 5 FALSE 0.279 1.39 ## 6 FALSE 0.306 1.44 . wts represents the amount each observation will be up-weighted or down-weighted in the outcome model we will soon fit. For instance, the 16th household used a bed net and had a predicted probability of 0.41. That’s a pretty low probability considering they did, in fact, use a net, so their weight is higher at 2.42. In other words, this household will be up-weighted compared to the naive linear model we fit above. The first household did not use a bed net; they’re predicted probability of net use was 0.25 (or put differently, a predicted probability of not using a net of 0.75). That’s more in line with their observed value of net, but there’s still some predicted probability of using a net, so their weight is 1.28. ",
    "url": "/pages/causal_inference_whole_game.html#12-model-our-assumptions",
    
    "relUrl": "/pages/causal_inference_whole_game.html#12-model-our-assumptions"
  },"116": {
    "doc": "Causal inference mosquito nets",
    "title": "1.3 Diagnose our models",
    "content": "The goal of propensity score weighting is to weight the population of observations such that the distribution of confounders is balanced between the exposure groups. Put another way, we are, in principle, removing the arrows between the confounders and exposure in the DAG, so that the confounding paths no longer distort our estimates. Here’s the distribution of the propensity score by group, created by geom_mirror_histogram() from the halfmoon package for assessing balance in propensity score models: . library(halfmoon) ggplot(net_data_wts, aes(.fitted)) + geom_mirror_histogram( aes(fill = net), bins = 50 ) + scale_y_continuous(labels = abs) + labs(x = \"propensity score\") . Figure 1.4: A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who did not use nets (bottom, orange). The range of propensity scores is similar between groups, with those who used nets slightly to the left of those who didn’t, but the shapes of the distribution are different. The weighted propensity score creates a pseudo-population where the distributions are much more similar: . ggplot(net_data_wts, aes(.fitted)) + geom_mirror_histogram( aes(group = net), bins = 50 ) + geom_mirror_histogram( aes(fill = net, weight = wts), bins = 50, alpha = .5 ) + scale_y_continuous(labels = abs) + labs(x = \"propensity score\") . Figure 1.5: A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who did not use nets (bottom, orange). The shaded region represents the unweighted distribution, and the colored region represents the weighted distributions. The ATE weights up-weight the groups to be similar in range and shape of the distribution of propensity scores. In this example, the unweighted distributions are not awful—the shapes are somewhat similar here, and they overlap quite a bit—but the weighted distributions in figure 1.4 are much more similar. Unmeasured confounding Propensity score weighting and most other causal inference techniques only help with observed confounders—ones that we model correctly, at that. Unfortunately, we still may have unmeasured confounding, which we’ll discuss below. Randomization is one causal inference technique that does deal with unmeasured confounding, one of the reasons it is so powerful. We might also want to know how well-balanced the groups are by each confounder. One way to do this is to calculate the standardized mean differences (SMDs) for each confounder with and without weights. We’ll calculate the SMDs with tidy_smd() then plot them with geom_love(). plot_df &lt;- tidy_smd( net_data_wts, c(income, health, temperature), .group = net, .wts = wts ) ggplot( plot_df, aes( x = abs(smd), y = variable, group = method, color = method ) ) + geom_love() . Figure 1.6: A love plot representing the standardized mean differences (SMD) between exposure groups of three confounders: temperature, income, and health. Before weighting, there are considerable differences in the groups. After weighting, the confounders are much more balanced between groups. A standard guideline is that balanced confounders should have an SMD of less than 0.1 on the absolute scale. 0.1 is just a rule of thumb, but if we follow it, the variables in figure 1.6 are well-balanced after weighting (and unbalanced before weighting). Before we apply the weights to the outcome model, let’s check their overall distribution for extreme weights. Extreme weights can destabilize the estimate and variance in the outcome model, so we want to be aware of it. We’ll also discuss several other types of weights that are less prone to this issue in @sec-estimands. net_data_wts |&gt; ggplot(aes(wts)) + geom_density(fill = \"#CC79A7\", color = NA, alpha = 0.8) . Figure 1.7: A density plot of the average treatment effect (ATE) weights. The plot is skewed, with higher values towards 8. This may indicate a problem with the model, but the weights aren’t so extreme to destabilize the variance of the estimate. The weights in figure 1.7 are skewed, but there are no outrageous values. If we saw extreme weights, we might try trimming or stabilizing them, or consider calculating an effect for a different estimand, which we’ll discuss in @sec-estimands. It doesn’t look like we need to do that here, however. ",
    "url": "/pages/causal_inference_whole_game.html#13-diagnose-our-models",
    
    "relUrl": "/pages/causal_inference_whole_game.html#13-diagnose-our-models"
  },"117": {
    "doc": "Causal inference mosquito nets",
    "title": "1.4 Estimate the causal effect",
    "content": "We’re now ready to use the ATE weights to (attempt to) account for confounding in the naive linear regression model. Fitting such a model is pleasantly simple in this case: we fit the same model as before but with weights = wts, which will incorporate the inverse probability weights. net_data_wts |&gt; lm(malaria_risk ~ net, data = _, weights = wts) |&gt; tidy(conf.int = TRUE) . ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Inte… 42.7 0.442 96.7 0 41.9 ## 2 netTR… -12.5 0.624 -20.1 5.50e-81 -13.8 ## # ℹ 1 more variable: conf.high &lt;dbl&gt; . The estimate for the average treatment effect is -12.5 (95% CI -13.8, -11.3). Unfortunately, the confidence intervals we’re using are wrong because they don’t account for the uncertainty in estimating the weights. Generally, confidence intervals for propensity score weighted models will be too narrow unless we account for this uncertainty. The nominal coverage of the confidence intervals will thus be wrong (they aren’t 95% CIs because their coverage is much lower than 95%) and may lead to misinterpretation. We’ve got several ways to address this problem, which we’ll discuss in detail in @sec-outcome-model, including the bootstrap, robust standard errors, and manually accounting for the estimation procedure with empirical sandwich estimators. For this example, we’ll use the bootstrap, a flexible tool that calculates distributions of parameters using re-sampling. The bootstrap is a useful tool for many causal models where closed-form solutions to problems (particularly standard errors) don’t exist or when we want to avoid parametric assumptions inherent to many such solutions; see @sec-appendix-bootstrap for a description of what the bootstrap is and how it works. We’ll use the rsample package from the tidymodels ecosystem to work with bootstrap samples. Because the bootstrap is so flexible, we need to think carefully about the sources of uncertainty in the statistic we’re calculating. It might be tempting to write a function like this to fit the statistic we’re interested in (the point estimate for netTRUE): . library(rsample) fit_ipw_not_quite_rightly &lt;- function(.split, ...) { # get bootstrapped data frame .df &lt;- as.data.frame(.split) # fit ipw model lm(malaria_risk ~ net, data = .df, weights = wts) |&gt; tidy() } . However, this function won’t give us the correct confidence intervals because it treats the inverse probability weights as fixed values. They’re not, of course; we just estimated them using logistic regression! We need to account for this uncertainty by bootstrapping the entire modeling process. For every bootstrap sample, we need to fit the propensity score model, calculate the inverse probability weights, then fit the weighted outcome model. library(rsample) fit_ipw &lt;- function(.split, ...) { # get bootstrapped data frame .df &lt;- as.data.frame(.split) # fit propensity score model propensity_model &lt;- glm( net ~ income + health + temperature, data = .df, family = binomial() ) # calculate inverse probability weights .df &lt;- propensity_model |&gt; augment(type.predict = \"response\", data = .df) |&gt; mutate(wts = wt_ate(.fitted, net)) # fit correctly bootstrapped ipw model lm(malaria_risk ~ net, data = .df, weights = wts) |&gt; tidy() } . Now that we know precisely how to calculate the estimate for each iteration let’s create the bootstrapped dataset with rsample’s bootstraps() function. The times argument determines how many bootstrapped datasets to create; we’ll do 1,000. bootstrapped_net_data &lt;- bootstraps( net_data, times = 1000, # required to calculate CIs later apparent = TRUE ) bootstrapped_net_data . ## # Bootstrap sampling with apparent sample ## # A tibble: 1,001 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1752/646]&gt; Bootstrap0001 ## 2 &lt;split [1752/637]&gt; Bootstrap0002 ## 3 &lt;split [1752/621]&gt; Bootstrap0003 ## 4 &lt;split [1752/630]&gt; Bootstrap0004 ## 5 &lt;split [1752/644]&gt; Bootstrap0005 ## 6 &lt;split [1752/650]&gt; Bootstrap0006 ## 7 &lt;split [1752/631]&gt; Bootstrap0007 ## 8 &lt;split [1752/627]&gt; Bootstrap0008 ## 9 &lt;split [1752/637]&gt; Bootstrap0009 ## 10 &lt;split [1752/631]&gt; Bootstrap0010 ## # ℹ 991 more rows . The result is a nested data frame: each splits object contains metadata that rsample uses to subset the bootstrap samples for each of the 1,000 samples. We actually have 1,001 rows because apparent = TRUE keeps a copy of the original data frame, as well, which is needed for some times of confidence interval calculations. Next, we’ll run fit_ipw() 1,001 times to create a distribution for estimate. At its heart, the calculation we’re doing is . fit_ipw(bootstrapped_net_data$splits[[n]]) . Where n is one of 1,001 indices. We’ll use purrr’s map() function to iterate across each split object. ipw_results &lt;- bootstrapped_net_data |&gt; mutate(boot_fits = map(splits, fit_ipw)) ipw_results . ## # Bootstrap sampling with apparent sample ## # A tibble: 1,001 × 3 ## splits id boot_fits ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; ## 1 &lt;split [1752/646]&gt; Bootstrap0001 &lt;tibble [2 × 5]&gt; ## 2 &lt;split [1752/637]&gt; Bootstrap0002 &lt;tibble [2 × 5]&gt; ## 3 &lt;split [1752/621]&gt; Bootstrap0003 &lt;tibble [2 × 5]&gt; ## 4 &lt;split [1752/630]&gt; Bootstrap0004 &lt;tibble [2 × 5]&gt; ## 5 &lt;split [1752/644]&gt; Bootstrap0005 &lt;tibble [2 × 5]&gt; ## 6 &lt;split [1752/650]&gt; Bootstrap0006 &lt;tibble [2 × 5]&gt; ## 7 &lt;split [1752/631]&gt; Bootstrap0007 &lt;tibble [2 × 5]&gt; ## 8 &lt;split [1752/627]&gt; Bootstrap0008 &lt;tibble [2 × 5]&gt; ## 9 &lt;split [1752/637]&gt; Bootstrap0009 &lt;tibble [2 × 5]&gt; ## 10 &lt;split [1752/631]&gt; Bootstrap0010 &lt;tibble [2 × 5]&gt; ## # ℹ 991 more rows . The result is another nested data frame with a new column, boot_fits. Each element of boot_fits is the result of the IPW for the bootstrapped dataset. For example, in the first bootstrapped data set, the IPW results were: . ipw_results$boot_fits[[1]] . ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 42.7 0.465 91.9 0 ## 2 netTRUE -11.8 0.657 -18.0 1.04e-66 . Now we have a distribution of estimates: . ipw_results |&gt; # remove original data set results filter(id != \"Apparent\") |&gt; mutate( estimate = map_dbl( boot_fits, # pull the `estimate` for `netTRUE` for each fit \\(.fit) .fit |&gt; filter(term == \"netTRUE\") |&gt; pull(estimate) ) ) |&gt; ggplot(aes(estimate)) + geom_histogram(fill = \"#D55E00FF\", color = \"white\", alpha = 0.8) . Figure 1.8: “A histogram of 1,000 bootstrapped estimates of the effect of net use on malaria risk. The spread of these estimates accounts for the dependency and uncertainty in the use of IPW weights.” . Figure figure 1.8 gives a sense of the variation in estimate, but let’s calculate 95% confidence intervals from the bootstrapped distribution using rsample’s int_t() : . boot_estimate &lt;- ipw_results |&gt; # calculate T-statistic-based CIs int_t(boot_fits) |&gt; filter(term == \"netTRUE\") boot_estimate . ## # A tibble: 1 × 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 netTRUE -13.4 -12.5 -11.7 0.05 student-t . Now we have a confounder-adjusted estimate with correct standard errors. The estimate of the effect of all households using bed nets versus no households using bed nets on malaria risk is -12.5 (95% CI -13.4, -11.7). Bed nets do indeed seem to reduce malaria risk in this study. ",
    "url": "/pages/causal_inference_whole_game.html#14-estimate-the-causal-effect",
    
    "relUrl": "/pages/causal_inference_whole_game.html#14-estimate-the-causal-effect"
  },"118": {
    "doc": "Causal inference mosquito nets",
    "title": "1.5 Conduct sensitivity analysis on the effect estimate",
    "content": "We’ve laid out a roadmap for taking observational data, thinking critically about the causal question we want to ask, identifying the assumptions we need to get there, then applying those assumptions to a statistical model. Getting the correct answer to the causal question relies on getting our assumptions more or less right. But what if we’re more on the less correct side? . Spoiler alert: the answer we just calculated is wrong. After all that effort! . When conducting a causal analysis, it’s a good idea to use sensitivity analyses to test your assumptions. There are many potential sources of bias in any study and many sensitivity analyses to go along with them (@sec-sensitivity); here, we’ll focus on the assumption of no confounding. Let’s start with a broad sensitivity analysis; then, we’ll ask questions about specific unmeasured confounders. When we have less information about unmeasured confounders, we can use tipping point analysis to ask how much confounding it would take to tip my estimate to the null. In other words, what would the strength of the unmeasured confounder have to be to explain our results away? The tipr package is a toolkit for conducting sensitivity analyses. Let’s examine the tipping point for an unknown, normally-distributed confounder. The tip_coef() function takes an estimate (a beta coefficient from a regression model, or the upper or lower bound of the coefficient). It further requires either the 1) scaled differences in means of the confounder between exposure groups or 2) effect of the confounder on the outcome. For the estimate, we’ll use conf.high, which is closer to 0 (the null), and ask: how much would the confounder have to affect malaria risk to have an unbiased upper confidence interval of 0? We’ll use tipr to calculate this answer for 5 scenarios, where the mean difference in the confounder between exposure groups is 1, 2, 3, 4, or 5. library(tipr) tipping_points &lt;- tip_coef(boot_estimate$.upper, exposure_confounder_effect = 1:5) tipping_points |&gt; ggplot(aes(confounder_outcome_effect, exposure_confounder_effect)) + geom_line(color = \"#009E73\", linewidth = 1.1) + geom_point(fill = \"#009E73\", color = \"white\", size = 2.5, shape = 21) + labs( x = \"Confounder-Outcome Effect\", y = \"Scaled mean differences in\\n confounder between exposure groups\" ) . Figure 1.9: A tipping point analysis under several confounding scenarios where the unmeasured confounder is a normally-distributed continuous variable. The line represents the strength of confounding necessary to tip the upper confidence interval of the causal effect estimate to 0. The x-axis represents the coefficient of the confounder-outcome relationship adjusted for the exposure and the set of measured confounders. The y-axis represents the scaled mean difference of the confounder between exposure groups. If we had an unmeasured confounder where the standardized mean difference between exposure groups was 1, the confounder would need to decrease malaria risk by about -11.7. That’s pretty strong relative to other effects, but it may be feasible if we have an idea of something we might have missed. Conversely, suppose the relationship between net use and the unmeasured confounder is very strong, with a mean scaled difference of 5. In that case, the confounder-malaria relationship only needs to be -2.3. Now we have to consider: which of these scenarios are plausible given our domain knowledge and the effects we see in this analysis? . Now let’s consider a much more specific sensitivity analysis. Some ethnic groups, such as the Fulani, have a genetic resistance to malaria [@arama2015]. Let’s say that in our simulated data, an unnamed ethnic group in the unnamed country shares this genetic resistance to malaria. For historical reasons, bed net use in this group is also very high. We don’t have this variable in net_data, but let’s say we know from the literature that in this sample, we can estimate at: . | People with this genetic resistance have, on average, a lower malaria risk by about 10. | About 26% of people who use nets in our study have this genetic resistance. | About 5% of people who don’t use nets have this genetic resistance. | . With this amount of information, we can use tipr to adjust the estimates we calculated for the unmeasured confounder. We’ll use adjust_coef_with_binary() to calculate the adjusted estimates. adjusted_estimates &lt;- boot_estimate |&gt; select(.estimate, .lower, .upper) |&gt; unlist() |&gt; adjust_coef_with_binary( exposed_confounder_prev = 0.26, unexposed_confounder_prev = 0.05, confounder_outcome_effect = -10 ) adjusted_estimates . ## # A tibble: 3 × 4 ## effect_adjusted effect_observed ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -10.4 -12.5 ## 2 -11.3 -13.4 ## 3 -9.63 -11.7 ## # ℹ 2 more variables: ## # exposure_confounder_effect &lt;dbl&gt;, ## # confounder_outcome_effect &lt;dbl&gt; . The adjusted estimate for a situation where genetic resistance to malaria is a confounder is -10.4 (95% CI -11.3, -9.6). In fact, these data were simulated with just such a confounder. The true effect of net use on malaria is about -10, and the true DAG that generated these data is: . Figure 1.10: The true causal diagram for `net_data`. This DAG is identical to the one we proposed with one addition: genetic resistance to malaria causally reduces the risk of malaria and impacts net use. It’s thus a confounder and a part of the minimal adjustment set required to get an unbiased effect estimate. In otherwords, by not including it, we’ve calculated the wrong effect. The unmeasured confounder in figure 1.10 is available in the dataset net_data_full as genetic_resistance. If we recalculate the IPW estimate of the average treatment effect of nets on malaria risk, we get -10.3 (95% CI -11.2, -9.3), much closer to the actual answer of -10. What do you think? Is this estimate reliable? Did we do a good job addressing the assumptions we need to make for a causal effect, mainly that there is no confounding? How might you criticize this model, and what would you do differently? Ok, we know that -10 is the correct answer because the data are simulated, but in practice, we can never be sure, so we need to continue probing our assumptions until we’re confident they are robust. We’ll explore these techniques and others in @sec-sensitivity. To calculate this effect, we: . | Specified a causal question (for the average treatment effect) | Drew our assumptions using a causal diagram (using DAGs) | Modeled our assumptions (using propensity score weighting) | Diagnosed our models (by checking confounder balance after weighting) | Estimated the causal effect (using inverse probability weighting) | Conducted sensitivity analysis on the effect estimate (using tipping point analysis) | . We can dive more deeply into propensity score techniques, explore other methods for estimating causal effects, and, most importantly, make sure that the assumptions we’re making are reasonable, even if we’ll never know for sure. ",
    "url": "/pages/causal_inference_whole_game.html#15-conduct-sensitivity-analysis-on-the-effect-estimate",
    
    "relUrl": "/pages/causal_inference_whole_game.html#15-conduct-sensitivity-analysis-on-the-effect-estimate"
  },"119": {
    "doc": "Causal inference mosquito nets",
    "title": "Causal inference mosquito nets",
    "content": " ",
    "url": "/pages/causal_inference_whole_game.html",
    
    "relUrl": "/pages/causal_inference_whole_game.html"
  },"120": {
    "doc": "Data concepts",
    "title": "Data concepts",
    "content": "Last update: 20230619 . | Data concepts . | How data is collected and stored | Database concepts | Availability | Specific genetic examples | Potential concepts | Examples . | Genomics England (GE) | OpenCB | Ontobee | . | . | . ",
    "url": "/pages/data_concepts.html",
    
    "relUrl": "/pages/data_concepts.html"
  },"121": {
    "doc": "Data concepts",
    "title": "How data is collected and stored",
    "content": ". | The Swiss Personalised Health Network (SPHN): collects and hosts omic data that we will ultimately use | Branch of SPHN: Data Coordination Center (DCC) | DCC is also a part of Swiss Institute of Bioinformatics (SIB) | The data is to be stored and processed on BioMedIT network | Our tenant on BioMedIT is called MOMIC and this is on the sciCORE infrastructure. | The database structure requires Resource Description Framework (RDF) Schema | . ",
    "url": "/pages/data_concepts.html#how-data-is-collected-and-stored",
    
    "relUrl": "/pages/data_concepts.html#how-data-is-collected-and-stored"
  },"122": {
    "doc": "Data concepts",
    "title": "Database concepts",
    "content": "The DCC is responsible for making research data suitable for database management. To do this they design the concepts. Training material available here, but is a DCC responsibility. Several concepts for the types of data that we use alredy exist. e.g. genetic data concepts. We also assist in generating new omics-related concepts for future expansion. This page will be updated to summarise the progress as these new concepts are defined. ",
    "url": "/pages/data_concepts.html#database-concepts",
    
    "relUrl": "/pages/data_concepts.html#database-concepts"
  },"123": {
    "doc": "Data concepts",
    "title": "Availability",
    "content": "The following information is copied from https://sphn.ch/2023/03/20/sphn-dataset-rdf-schema-2023-release/ . | The SPHN Dataset is openly available here: https://sphn.ch/document/sphn-dataset/ | The SPHN RDF Schema is browsable here: https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2 | The external terminologies in RDF are accessible on BioMedIT Portal: https://portal.dcc.sib.swiss/ | The Quality Assurance Framework is available here: https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/quality_assurance | Project templates are available here: https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/templates | A comprehensive documentation is openly accessible here: https://sphn-semantic-framework.readthedocs.io/en/latest/ | . ",
    "url": "/pages/data_concepts.html#availability",
    
    "relUrl": "/pages/data_concepts.html#availability"
  },"124": {
    "doc": "Data concepts",
    "title": "Specific genetic examples",
    "content": ". | https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#GeneticVariation | https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#SingleNucleotideVariation | https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#VariantDescriptor | https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#VariantNotation | . ",
    "url": "/pages/data_concepts.html#specific-genetic-examples",
    
    "relUrl": "/pages/data_concepts.html#specific-genetic-examples"
  },"125": {
    "doc": "Data concepts",
    "title": "Potential concepts",
    "content": ". . ",
    "url": "/pages/data_concepts.html#potential-concepts",
    
    "relUrl": "/pages/data_concepts.html#potential-concepts"
  },"126": {
    "doc": "Data concepts",
    "title": "Examples",
    "content": "Genomics England (GE) . GE represents one of the best national platforms so far and they have complete analysis of ~200’000 genomes for clinical use. | Homepage | Bioinformatics page | . GE model documentation shows the concepts in use: http://gelreportmodels.genomicsengland.co.uk/models.html#. Concept development could start with the GE 1.3.0-SNAPSHOT. For example, VariantMetadata is a good starting place since it includes “individual” (i.e. subject), “sample type”, “experiment”, etc., which are required in most genomic data scenarios. We could work through a prioritised list of concepts over several months-years for any new concept that matches user requirements. They have derived some logical structures derived from Ontobee, OpenCB, and probably other common sources. Example: metadata . Here are some examples which most users will require, from VariantMetadata: . |--VariantMetadata: |-- Cohort |-- Experiment |--- center, date, molecule, technique, library, libraryLayout, platform, description |-- Individual |-- id, family, father, mother, sex, phenotype, samples |-- Program |-- ... |-- Sample |-- ... |-- SampleSetType |-- CASE_CONTROL, CASE_SET, CONTROL_SET, PAIRED, TIME_SERIES, FAMILY, TRIO, MISCELLANEOUS, UNKNOWN |--Species |-- ... Example: variant Some of the most important concepts for our genomics needs are listed on the variant procol page, which come from org.opencb.biodata.models.variant.avro. These are beyond our current needs but this list of 55 entries likely cover most of the conceivable needs: . AdditionalAttribute, AlleleOrigin Enum, AllelesCode Enum, AlternateCoordinate, ClinVar, ClinicalSignificance Enum, Confidence Enum, ConsequenceType, ConsistencyStatus Enum, Cosmic, Cytoband, Drug, DrugResponseClassification Enum, EthnicCategory Enum, EvidenceEntry, EvidenceImpact Enum, EvidenceSource, EvidenceSubmission, ExonOverlap, Expression, ExpressionCall Enum, FeatureTypes Enum, FileEntry, GeneDrugInteraction, GeneTraitAssociation, GenomicFeature, Genotype, Gwas, HeritableTrait, ModeOfInheritance Enum, Penetrance Enum, PopulationFrequency, Property, ProteinFeature, ProteinVariantAnnotation, Repeat, Score, SequenceOntologyTerm, SomaticInformation, StructuralVariantType Enum, StructuralVariation, StudyEntry, TraitAssociation Enum, TumorigenesisClassification Enum, VariantAnnotation, VariantAvro, VariantClassification, VariantFunctionalEffect Enum, VariantHardyWeinbergStats, VariantStats, VariantTraitAssociation, VariantType Enum, Xref, . Let’s pick one example from that list which would come under the heading “variant interpretation” - ClinicalSignificance: . Example: variant interpretation - ClinicalSignificance . We use this exact example for our clinical genetics work: ClinicalSignificance. Mendelian variants classification with ACMG terminology as defined in Richards, S. et al. (2015). Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology. Genetics in Medicine, 17(5), 405?423. https://doi.org/10.1038/gim.2015.30. Classification for variants associated with disease, etc., based on the ACMG recommendations and ClinVar classification (https://www.ncbi.nlm.nih.gov/clinvar/docs/clinsig/). | benign_variant : Benign variants interpreted for Mendelian disorders | likely_benign_variant : Likely benign variants interpreted for Mendelian disorders with a certainty of at least 90% | pathogenic_variant : Pathogenic variants interpreted for Mendelian disorders | likely_pathogenic_variant : Likely pathogenic variants interpreted for Mendelian disorders with a certainty of at least 90% | uncertain_significance : Uncertain significance variants interpreted for Mendelian disorders. Variants with conflicting evidences should be classified as uncertain_significance | Enum symbols: . | benign, likely_benign, VUS, likely_pathogenic, pathogenic, uncertain_significance | . | . However, we do not restrict our use of ACMG standard for variant interpretation using only ClinicalSignificance, since there are a large number of other variant interpretation datasets which can be used to make the final determination. One method we use is the ACMG scoring sytem to score all variants based on the ACMG evidence categorisation method. Therefore, ACMG_score could be derived from ClinicalSignificance, but also from other sources. Perhaps EvidenceEntry is a major entry which includes most of these subtypes for interpretation evidence. OpenCB . | Used for some Genomics England concepts | OpenCB which provides scalable a storage engine framework with data and metadata catalogue. | https://github.com/opencb/opencga | http://docs.opencb.org | . Ontobee . | Used for some Genomics England concepts | Ontology data server with RDF source code. | Example GE concept for “Variants / AlleleOrigin / germline_variant” comes from : | https://ontobee.org/ontology/SO?iri=http://purl.obolibrary.org/obo/SO_0001762 | germline_variant RDF source code | . |-- sequence_attribute |-- variant_quality |-- variant_origin |-- maternal_variant |-- paternal_variant |-- somatic_variant |-- pedigree_specific_variant |-- population_specific_variant |-- de_novo_variant |-- germline_variant |-- RDF sourcode . ",
    "url": "/pages/data_concepts.html#examples",
    
    "relUrl": "/pages/data_concepts.html#examples"
  },"127": {
    "doc": "Data stream",
    "title": "Data stream",
    "content": "Last update: 20230616 . ",
    "url": "/pages/data_stream.html#data-stream",
    
    "relUrl": "/pages/data_stream.html#data-stream"
  },"128": {
    "doc": "Data stream",
    "title": "Omic data sources",
    "content": ". | The Swiss Multi-Omics Center (SMOC) . | http://smoc.ethz.ch. | . | . ",
    "url": "/pages/data_stream.html#omic-data-sources",
    
    "relUrl": "/pages/data_stream.html#omic-data-sources"
  },"129": {
    "doc": "Data stream",
    "title": "SMOC has 3 branches",
    "content": ". | CGAC - Genomics. | CPAC - Proteotyping. | CMAC - Metabolomics &amp; Lipidomics. | . ",
    "url": "/pages/data_stream.html#smoc-has-3-branches",
    
    "relUrl": "/pages/data_stream.html#smoc-has-3-branches"
  },"130": {
    "doc": "Data stream",
    "title": "Where the facilities are",
    "content": ". | CGAC is from Health2030 genome center Geneva. | CMAC is from ETHZ Metabolomics &amp; Biophysics Zurich. | CPAC is from ETHZProteomics Zurich. | . ",
    "url": "/pages/data_stream.html#where-the-facilities-are",
    
    "relUrl": "/pages/data_stream.html#where-the-facilities-are"
  },"131": {
    "doc": "Data stream",
    "title": "Naming convention",
    "content": ". | BiomedIT portal data transfer uses the names: . | CGAC = health_2030_genome_center | CMAC = phrt_cmac | CPAC = phrt_cpac | . | . ",
    "url": "/pages/data_stream.html#naming-convention",
    
    "relUrl": "/pages/data_stream.html#naming-convention"
  },"132": {
    "doc": "Data stream",
    "title": "Examples of omic tech and output",
    "content": "The following are place-holder links until we write up pages specfic to our usage. | https://lawlessgenomics.com/topic/omics-mc-hilic | https://lawlessgenomics.com/topic/omics-mc-lc | https://lawlessgenomics.com/topic/omics-proteomics | https://lawlessgenomics.com/topic/omics-rplc | https://lawlessgenomics.com/topic/omics-targeted | . ",
    "url": "/pages/data_stream.html#examples-of-omic-tech-and-output",
    
    "relUrl": "/pages/data_stream.html#examples-of-omic-tech-and-output"
  },"133": {
    "doc": "Data stream",
    "title": "BioMedIT",
    "content": "Read about the high-performance-computing (HPC) infrastructure here: HPC infrastructure . ",
    "url": "/pages/data_stream.html#biomedit",
    
    "relUrl": "/pages/data_stream.html#biomedit"
  },"134": {
    "doc": "Data stream",
    "title": "How data is collected and stored",
    "content": "Omic data that we will ultimately use is collected and hosted under the following hierarchy. | The Swiss Personalised Health Network (SPHN). | Branch of SPHN: Data Coordination Center (DCC). | DCC is also a part of Swiss Institute of Bioinformatics (SIB). | The data is to be stored and processed on BioMedIT network. | Our tenant on BioMedIT is called MOMIC and this is on the sciCORE infrastructure. | . While the SPHN organisation hierarchy is illustrated based on the final storage location for the omic data belonging to this project, there are many other branches of SPHN not shown. It is also worth noting that ethical and legal responsibilities of data access may rest on the ethics/consent form signatories, such as individual project leaders. SPHN and BioMedIT . The Swiss Personalized Health Network (SPHN) is a national initiative under the leadership of the Swiss Academy of Medical Sciences (SAMS). In collaboration with the SIB Swiss Institute of Bioinformatics it contributes to the development, implementation and validation of coordinated data infrastructures in order to make health-relevant data interoperable and shareable for research in Switzerland. BioMedIT is an integral part of SPHN and was developed along the needs of SPHN funded projects. SPHN and BioMedIT are funded by the Swiss Government through the ERI-dispatches (2017-2020 and 2021-2024). (from https://www.biomedit.ch) . BioMedIT structure . The three BioMedIT sites (nodes) form a shared security zone, such that once data is brought into the platform, it can be accessed and processed on any of the nodes. Projects from research consortia or single researchers wishing to use sensitive data are set up as research projects on one of the nodes in a BioMedIT project-specific environment, called a ‘B-space’. Research groups can access their B-spaces via the BioMedIT Portal, where they can manage users and encryption keys, initiate data transfers and access support and other tools and services offered by BioMedIT. Researchers securely access the system using two factor authentication, and extract only nonsensitive or aggregated research results while leaving the sensitive data in the platform. (from https://www.biomedit.ch) . ",
    "url": "/pages/data_stream.html#how-data-is-collected-and-stored",
    
    "relUrl": "/pages/data_stream.html#how-data-is-collected-and-stored"
  },"135": {
    "doc": "Data stream",
    "title": "Accessing BioMedIT",
    "content": "BioMedIT is open to all Swiss universities, research institutes, hospitals, service providers and other interested partners. In addition, and specifically within the framework of the two Swiss national initiatives SPHN and PHRT, the BioMedIT Network can be used for mono- and multi-site, individual and collaborative research projects. BioMedIT is a project of the SIB Swiss Institute of Bioinformatics (from https://www.biomedit.ch) . ",
    "url": "/pages/data_stream.html#accessing-biomedit",
    
    "relUrl": "/pages/data_stream.html#accessing-biomedit"
  },"136": {
    "doc": "Data stream",
    "title": "Data stream",
    "content": " ",
    "url": "/pages/data_stream.html",
    
    "relUrl": "/pages/data_stream.html"
  },"137": {
    "doc": "Design PCA SNV INDEL v1",
    "title": "Design PCA SNV INDEL v1",
    "content": "We prepare two datasets: . | (a) 1KG reference population: input ${THOUSAND_GENOMES}/phase3.chr\"${INDEX}\".GRCh38.GT.crossmap.vcf.gz | (b) Cohort: input ${PRE_ANNOTATION_DIR}/chr${INDEX}_bcftools_gatk_norm.bcf . | We prepare with bcftools and convert to bcf. | We then convert to PLINK format. | Prune variants : . | --maf 0.10, only retain SNPs with MAF greater than 10% | --indep [window size] [step size/variant count)] [Variance inflation factor (VIF) threshold] | e.g. indep 50 5 1.5, Generates a list of markers in approx. linkage equilibrium - takes 50 SNPs at a time and then shifts by 5 for the window. VIF (1/(1-r^2)) is the cut-off for linkage disequilibrium | . | Merge to single files | Merge (a) and (b) = (c) | PCA on each of (a-c) for individual datasets and plots | . Output: ${PCA_DATA}/prune/ . ",
    "url": "/pages/design_PCA_SNV_INDEL_V1.html#design-pca-snv-indel-v1",
    
    "relUrl": "/pages/design_PCA_SNV_INDEL_V1.html#design-pca-snv-indel-v1"
  },"138": {
    "doc": "Design PCA SNV INDEL v1",
    "title": "Additional pruning for consideration",
    "content": "https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD) . ",
    "url": "/pages/design_PCA_SNV_INDEL_V1.html#additional-pruning-for-consideration",
    
    "relUrl": "/pages/design_PCA_SNV_INDEL_V1.html#additional-pruning-for-consideration"
  },"139": {
    "doc": "Design PCA SNV INDEL v1",
    "title": "Design PCA SNV INDEL v1",
    "content": "Last update: 20241220 . ",
    "url": "/pages/design_PCA_SNV_INDEL_V1.html",
    
    "relUrl": "/pages/design_PCA_SNV_INDEL_V1.html"
  },"140": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Design DNA SNV INDEL v1",
    "content": " ",
    "url": "/pages/design_dna_snvindel_v1.html#design-dna-snv-indel-v1",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#design-dna-snv-indel-v1"
  },"141": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Germline short variant discovery (SNVs + Indels) and interpretation",
    "content": ". | Design DNA SNV INDEL v1 . | Germline short variant discovery (SNVs + Indels) and interpretation | Aims | Introduction | Protocol summary | Protocol steps | Metrics | Data release | . | . Protocol name: design_dna_snvindel_v1 . ",
    "url": "/pages/design_dna_snvindel_v1.html#germline-short-variant-discovery-snvs--indels-and-interpretation",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#germline-short-variant-discovery-snvs--indels-and-interpretation"
  },"142": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Aims",
    "content": "| Phase | Aim | Status | Task | . | Phase 2 | (1) | Complete | Process all WGS from the study cohort to a consensus format. | . | Phase 2 | (2) | 2 of 2 complete | Prepare qualifying variant (QV) sets for each downstream aim. | . | Phase 2 | (3) | v1 complete v2 in progress | Clinical genetics report per individual(i.e., baseline benchmark of known disease-causing). | . | Phase 2 | (4) | 2 experiment complete | GWAS: Statistical genomics to find new cohort-level associations with disease. | . | Phase 2 | (5) | 2 experiment complete | Gene-VSAT: Statistical genomics to find new cohort-level associations with disease. | . | Phase 2 | (6) | 1 experiment complete | Proteome-VSAT: Statistical genomics to find new cohort-level associations with disease (Proteom-VSAT). | . | Phase 2 | (7) | 1 experiment complete | ACAT: Statistical multiomics to find new cohort-level associations with disease.. | . | Phase 2 | (8) | In progress | New methods (ML/DL, causal inference) for individual and cohort-leveldiscovery. | . Figure 1: Summary of design DNA SNV INDEL v1 pipeline plan. ",
    "url": "/pages/design_dna_snvindel_v1.html#aims",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#aims"
  },"143": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Introduction",
    "content": "This protocol is designed to process DNA WGS data in FASTQ format into qualifying qariants (QV) based on consensus variables and thresholds (figure 1). The QV can then be used in multiple applications such as ML/DL to find disease-related variants or gene functions. Additionally, in the clinical genetic protocol further standardised filtering criteria are used to reach a single genetic determinant in a clinical genetics report for each subject. The design name Design DNA SNV INDEL v1 indicates that this protocol is tailored to single nucleotide variants (SNVs) and short insertion/deletions (INDELs) (e.g. GATK pipeline). We implement the genome analysis tool kit GATK best practices workflow for germline short variant discovery (open source licence here). This GATK workflow is designed to operate on a set of samples constituting a study cohort; specifically, a set of per-sample BAM files that have been pre-processed as described in the GATK Best Practices for data pre-processing. Single-variant and genomics-only analysis will be followed up to confirm if causal effects are identified in RNA and protein layers. Joint-multiomic analysis will include all layers in a single statistical model. ",
    "url": "/pages/design_dna_snvindel_v1.html#introduction",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#introduction"
  },"144": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Protocol summary",
    "content": ". | Process all raw WGS into an analysis-ready format - geonmic VCF (gVCF). | The first goal is to process all raw whole genome sequencing (WGS) data into analysis-ready formats, specifically into joint cohort Variant Call Format (VCF) using the emit-ref-confidence (ERC) gVCF mode. This involves using a reference model to emit data with condensed non-variant blocks, adhering to the gVCF format. gVCF is split per chromosome. | . | The joint cohort chromosome level gVCF are filtered into qualifying variants (QV). | The QV sets are used individually or mixed to produce the main analysis results: . | QV set 1 for clinical genetics (known disease-causing) report for each individual | QV set 2 for statistical genomics (new associations with established methods) for cohort level discovery | QV set 1 or 2 for other methods (ML/DL, causal inference) (new methods) for individual and cohort level discovery | . | Release data. | If not already included in an analysis model, the candidate causal variants will be followed up to confirm if causal effects are identified in RNA and protein layers. | . ",
    "url": "/pages/design_dna_snvindel_v1.html#protocol-summary",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#protocol-summary"
  },"145": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Protocol steps",
    "content": "The major processing steps in sequential order are: . | Aim 1 . | FASTQ QC - see DNA QC | FASTP: QC, check adapters, trimming, filtering, splitting/merging. | Genome alignment with BWA with reference genome GCA_000001405.15_GRCh38_no_alt_analysis_set | GATK Duplicates | GATK BQSR | GATK Haplotype caller | GATK Genomic db import | GATK Genotyping gVCFs | GATK VQSR | GATK Genotype refine | VCF QC - see DNA QC | . | Aim 2 . | Design QV SNV INDEL V1 Qualifying variants (variables and thresholds) SNV INDEL v1 | Pre-annotation processing: data conversion for simpler handling | Pre-annotation MAF: filtering to remove noise | DNA annotation: annotate known effects, biological function, associations | . | Aim 3 use a selective mixture of the remaining methods depending on the QV set . | DNA interpretation | ACMG criteria: Standardised scoring for interpreting variant pathogenicity | Clinical genetics reports - not documented here | . | Aim 4-7 use a selective mixture of the remaining methods depending on the QV set . | Design Statistical genomics v1 | ML/DL projects - not documented here | . | Release . | See design_dna_snvindel_v1_release | . | . Figure 2: Extended methods of figure 1 DNA germline short variant discovery pipeline plan. ",
    "url": "/pages/design_dna_snvindel_v1.html#protocol-steps",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#protocol-steps"
  },"146": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Metrics",
    "content": "Study book data: . | CollectWgsMetrics: 03b_collectwgsmetrics.sh -&gt; study_book/qc_summary_stats mapping, depth, and more. See metrics_collectwgsmetrics. | bcftools stats and plot-vcfstats: 07c_qc_summary_stats.sh -&gt; study_book/qc_summary_stats gVCF summary after HC. See metrics_bcftoolsstats. | . ",
    "url": "/pages/design_dna_snvindel_v1.html#metrics",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#metrics"
  },"147": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Data release",
    "content": "The private internal data release: design_dna_snvindel_v1_release . ",
    "url": "/pages/design_dna_snvindel_v1.html#data-release",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html#data-release"
  },"148": {
    "doc": "Design DNA SNV INDEL v1",
    "title": "Design DNA SNV INDEL v1",
    "content": "Last update: 20241217 . ",
    "url": "/pages/design_dna_snvindel_v1.html",
    
    "relUrl": "/pages/design_dna_snvindel_v1.html"
  },"149": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Design release DNA SNV INDEL v1",
    "content": "Protocol name: design_dna_snv_indel_v1_relsease (this document) for design_dna_snvindel_v1 (see design_dna_snvindel_v1). | Design release DNA SNV INDEL v1 . | About | Contents | Data synchronisation process | Data Availability | Current version script | Checksums | Stats | . | . Release information . Current release version: v1 Location temp: /project/data/shared_all/release_dna_snv_indel_v1 Location stable: /project/data/shared/release_dna_snv_indel_v1 . Status . Status: design_dna_snv_indel_v2_relsease is currently recommended. We have added new annotation settings affecting which QV are reported. Any modifications due to error will be notified and a new release ID will be issued (e.g. design_dna_snv_indel_v1.2_relsease). ",
    "url": "/pages/design_dna_snvindel_v1_release.html#design-release-dna-snv-indel-v1",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#design-release-dna-snv-indel-v1"
  },"150": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "About",
    "content": "If you are reading this then you are interested in using the release data from the design_dna_snvindel_v1 pipeline. This pipeline produces a multi-use dataset via the qualifying variants 1 (QV1) protocol. Incremental additions with new data for this release will be added. Potential uses are suggested as the end-points in figure 1 (e.g. statistical genomics, etc.) . The protocol used was: . | design_dna_snvindel_v1 . | design_PCA_SNV_INDEL_v1 | design_qv_snvindel_v1 | . | . Figure 1: Summary of design DNA SNV INDEL v1 pipeline plan. ",
    "url": "/pages/design_dna_snvindel_v1_release.html#about",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#about"
  },"151": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Contents",
    "content": "See the README.md for this release to find files: . dna_snv_indel_v1_release.README.md . ",
    "url": "/pages/design_dna_snvindel_v1_release.html#contents",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#contents"
  },"152": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Data synchronisation process",
    "content": ". | Sync release data to shared_all. | Log and verify data during sync to shared_all. | Data manager syncs from shared_all to shared. | The final release is locked in shared but allows incremental additions. | Labelled as release_dna_snv_indel_v1. | . ",
    "url": "/pages/design_dna_snvindel_v1_release.html#data-synchronisation-process",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#data-synchronisation-process"
  },"153": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Data Availability",
    "content": "All raw and processed data are accessible to all internal researchers. Processed/intermediate data might change during projects, so released data is recommended for downstream projects. Release data should have detailed supporting documentation following the protocol. Essential data is curated to minimise storage use. Non-release data is accessible as usual from user directories but may be overwritten or updated without notice. Data paths are listed in the release script, generally read from a master list of variables. ",
    "url": "/pages/design_dna_snvindel_v1_release.html#data-availability",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#data-availability"
  },"154": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Current version script",
    "content": "release_dna_snv_indel_v1.sh is run from the user src directory. ",
    "url": "/pages/design_dna_snvindel_v1_release.html#current-version-script",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#current-version-script"
  },"155": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Checksums",
    "content": "All files are logged with their md5 checksum. To verify integrity run: md5sum -c checksums.md5 . $ cat checksums.md5 457dc93749669bed628575dfe551bdac ./release_dna_snv_indel_v1.sh.log d35670b5ac26302b19f07c4cf3a96977 ./release_dna_snv_indel_v1.out . $ md5sum -c checksums.md5 ./release_dna_snv_indel_v1.sh.log: OK ./release_dna_snv_indel_v1.out: OK . ",
    "url": "/pages/design_dna_snvindel_v1_release.html#checksums",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#checksums"
  },"156": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Stats",
    "content": "Chromosome 1 Number of samples: 180 Number of SNPs: 1543167 Number of INDELs: 385213 Number of MNPs: 0 Number of others: 0 Number of sites: 1972696 Chromosome 2 Number of samples: 180 Number of SNPs: 1620394 Number of INDELs: 394968 Number of MNPs: 0 Number of others: 0 Number of sites: 2061648 Chromosome 3 Number of samples: 180 Number of SNPs: 1316780 Number of INDELs: 324975 Number of MNPs: 0 Number of others: 0 Number of sites: 1673128 Chromosome 4 Number of samples: 180 Number of SNPs: 1337064 Number of INDELs: 316959 Number of MNPs: 0 Number of others: 0 Number of sites: 1689336 Chromosome 5 Number of samples: 180 Number of SNPs: 1196261 Number of INDELs: 293793 Number of MNPs: 0 Number of others: 0 Number of sites: 1520849 Chromosome 6 Number of samples: 180 Number of SNPs: 1214390 Number of INDELs: 303459 Number of MNPs: 0 Number of others: 0 Number of sites: 1552036 Chromosome 7 Number of samples: 180 Number of SNPs: 1145244 Number of INDELs: 277881 Number of MNPs: 0 Number of others: 0 Number of sites: 1460406 Chromosome 8 Number of samples: 180 Number of SNPs: 1020218 Number of INDELs: 240781 Number of MNPs: 0 Number of others: 0 Number of sites: 1284467 Chromosome 9 Number of samples: 180 Number of SNPs: 858094 Number of INDELs: 198839 Number of MNPs: 0 Number of others: 0 Number of sites: 1080126 Chromosome 10 Number of samples: 180 Number of SNPs: 946539 Number of INDELs: 237079 Number of MNPs: 0 Number of others: 0 Number of sites: 1212448 Chromosome 11 Number of samples: 180 Number of SNPs: 910215 Number of INDELs: 221252 Number of MNPs: 0 Number of others: 0 Number of sites: 1154276 Chromosome 12 Number of samples: 180 Number of SNPs: 903797 Number of INDELs: 242211 Number of MNPs: 0 Number of others: 0 Number of sites: 1173553 Chromosome 13 Number of samples: 180 Number of SNPs: 683880 Number of INDELs: 169236 Number of MNPs: 0 Number of others: 0 Number of sites: 872669 Chromosome 14 Number of samples: 180 Number of SNPs: 618962 Number of INDELs: 156463 Number of MNPs: 0 Number of others: 0 Number of sites: 792576 Chromosome 15 Number of samples: 180 Number of SNPs: 609207 Number of INDELs: 141670 Number of MNPs: 0 Number of others: 0 Number of sites: 768672 Chromosome 16 Number of samples: 180 Number of SNPs: 653752 Number of INDELs: 151086 Number of MNPs: 0 Number of others: 0 Number of sites: 826293 Chromosome 17 Number of samples: 180 Number of SNPs: 521864 Number of INDELs: 161194 Number of MNPs: 0 Number of others: 0 Number of sites: 700410 Chromosome 18 Number of samples: 180 Number of SNPs: 534571 Number of INDELs: 130405 Number of MNPs: 0 Number of others: 0 Number of sites: 680157 Chromosome 19 Number of samples: 180 Number of SNPs: 455618 Number of INDELs: 136628 Number of MNPs: 0 Number of others: 0 Number of sites: 610238 Chromosome 20 Number of samples: 180 Number of SNPs: 427477 Number of INDELs: 108222 Number of MNPs: 0 Number of others: 0 Number of sites: 547588 Chromosome 21 Number of samples: 180 Number of SNPs: 333850 Number of INDELs: 67464 Number of MNPs: 0 Number of others: 0 Number of sites: 413496 Chromosome 22 Number of samples: 180 Number of SNPs: 332138 Number of INDELs: 74089 Number of MNPs: 0 Number of others: 0 Number of sites: 418913 Chromosome X Number of samples: 180 Number of SNPs: 678392 Number of INDELs: 183618 Number of MNPs: 0 Number of others: 0 Number of sites: 881808 Chromosome Y Number of samples: 180 Number of SNPs: 71640 Number of INDELs: 1289 Number of MNPs: 0 Number of others: 0 Number of sites: 77666 . ",
    "url": "/pages/design_dna_snvindel_v1_release.html#stats",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html#stats"
  },"157": {
    "doc": "Design release DNA SNV INDEL v1",
    "title": "Design release DNA SNV INDEL v1",
    "content": "Last update: 20250102 . ",
    "url": "/pages/design_dna_snvindel_v1_release.html",
    
    "relUrl": "/pages/design_dna_snvindel_v1_release.html"
  },"158": {
    "doc": "Design release DNA SNV INDEL v2",
    "title": "Design release DNA SNV INDEL v2",
    "content": "Protocol name: design_dna_snv_indel_v2_relsease (this document) for design_dna_snvindel_v2 (see design_dna_snvindel_v1). | Design release DNA SNV INDEL v2 . | Contents | . | . Release information . Current release version: v2 Location temp: /project/data/shared_all/release_dna_snv_indel_v2 Location stable: /project/data/shared/release_dna_snv_indel_v2 . Status . Status: design_dna_snv_indel_v2_relsease is currently recommended. Any modifications due to error will be notified and a new release ID will be issued (e.g. design_dna_snv_indel_v2.2_relsease). ",
    "url": "/pages/design_dna_snvindel_v2_release.html#design-release-dna-snv-indel-v2",
    
    "relUrl": "/pages/design_dna_snvindel_v2_release.html#design-release-dna-snv-indel-v2"
  },"159": {
    "doc": "Design release DNA SNV INDEL v2",
    "title": "Contents",
    "content": "See the README.md for this release to find files: . dna_snv_indel_v2_release.README.md . ",
    "url": "/pages/design_dna_snvindel_v2_release.html#contents",
    
    "relUrl": "/pages/design_dna_snvindel_v2_release.html#contents"
  },"160": {
    "doc": "Design release DNA SNV INDEL v2",
    "title": "Design release DNA SNV INDEL v2",
    "content": "Last update: 202500301 . ",
    "url": "/pages/design_dna_snvindel_v2_release.html",
    
    "relUrl": "/pages/design_dna_snvindel_v2_release.html"
  },"161": {
    "doc": "Design documents",
    "title": "Index page for design documents",
    "content": "Last update: 20220701 . This pages list the protocols used in our analysis. The protcols are compiled from modular components which may be resused from shared documentation. Completed design documents are listed below in the table of contents. ",
    "url": "/pages/design_doc.html#index-page-for-design-documents",
    
    "relUrl": "/pages/design_doc.html#index-page-for-design-documents"
  },"162": {
    "doc": "Design documents",
    "title": "Planned designs",
    "content": ". | Design DNA SNV INDEL v1 (germline short variant discovery (SNVs + Indels) and interpretation) | DNA structural variation | RNA quantitative expression | Metabolomics and proteomics | Clinical/pheno/covariate features | . ",
    "url": "/pages/design_doc.html#planned-designs",
    
    "relUrl": "/pages/design_doc.html#planned-designs"
  },"163": {
    "doc": "Design documents",
    "title": "Design documents",
    "content": " ",
    "url": "/pages/design_doc.html",
    
    "relUrl": "/pages/design_doc.html"
  },"164": {
    "doc": "Design QV SNV INDEL v1",
    "title": "Design QV SNV INDEL v1",
    "content": " ",
    "url": "/pages/design_qv_snvindel_v1.html#design-qv-snv-indel-v1",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#design-qv-snv-indel-v1"
  },"165": {
    "doc": "Design QV SNV INDEL v1",
    "title": "Qualifying variant protocol for SNV INDEL v1",
    "content": ". | Design QV SNV INDEL v1 . | Qualifying variant protocol for SNV INDEL v1 | Introduction | Variables and thresholds | Variables file | The use of QV in analysis plan | . | References | . Protocol name: design_qv_snvindel_v1 . ",
    "url": "/pages/design_qv_snvindel_v1.html#qualifying-variant-protocol-for-snv-indel-v1",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#qualifying-variant-protocol-for-snv-indel-v1"
  },"166": {
    "doc": "Design QV SNV INDEL v1",
    "title": "Introduction",
    "content": "This pages summarises the qualifying variant (QV) settings used for QV SNV INDEL v1. QV defines those variants which remain after filtering for use in downstream analysis. In general, most choices are based on well established best practices. For any subjective choices of thesholds, we will rely on evidence-based reasoning. However, it is important for users to understand whether their QV is suited to a particular context. For example, a GWAS analysis typically relies on the presence of common shared variants while a clinical genetics report might focus on rare novel variants. Conversely, cohort-level a genetic relationship matrix for use in statistical analysis might rely on some but not all of the QV steps (such as dropping some filters so that haplotypes can be correcly represented). You can read about QV in this review 1 https://www.nature.com/articles/s41576-019-0177-4/. Examples of QV sets are: (1) QC-only QV resulting in a very large remaining variant dataset, e.g. &gt;500’000 variants per subject. (2) Rare disease QV resulting in a small dataset with strict filters, e.g. 10’000 variants per subject. (3) Flexible QV resulting in disease-causing candidate variants with modest filtering to balance QC and false positives, e.g. &lt;100’000 variants per subject. Several QV protocols can be piped together to create increasingly filtered datasets to match the needs at a certain stage of analysis. It is also typical that different analysis from QVs sets are used and the final results from each step are merged to cover multiple scenarios. For example, SNV/INDEL QV + CNV QV + rare disease known QV + statitcal assossiation QC may be merged to reach the final combined analysis of (1) single case-level known disease causing results with (2) newly identified cohort-level genes associated with disease. The following figure 1 from Hillman et. al. 2 is a typical illustration of QV workflow. We do not use this reference for any analysis. The figure is simply shown for illustration. Figure 2 from Povysil et al. 1 provides an example of the application of QV in an analysis. Figure 1: From Hillman et. al. 2. Variant filtering and annotation workflow with summative case and control variant and gene counts. Top. General workflow and filtering parameters applied to data. Bottom. Table summarizing total qualifying variant and gene counts by ethnicity and in total for cases and controls. Variant counts represent the number of unique qualifying variant loci identified in each ethnicity and network. Gene counts represent the number of genes containing at least one qualifying variant in each network and ethnicity. AMR, American Latino; CADD, combined annotation dependent depletion; dbNSFP, database of non-synonymous single-nucleotide variant functional predictions; EA, European Americans; FOCM, Folate and One-Carbon Metabolism; GATK, Genome Analysis Toolkit; GHOS, Glucose Homeostasis and Oxidative Stress; gnomAD, genome aggregation database; MA, Mexican Americans; NFE, Non-Finnish Europeans; VCF, variant call format. ",
    "url": "/pages/design_qv_snvindel_v1.html#introduction",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#introduction"
  },"167": {
    "doc": "Design QV SNV INDEL v1",
    "title": "Variables and thresholds",
    "content": "This protocol shown here (SNV INDEL v1) is considered as the flexible format. QC-only QV . | [QC] 01_fastp.sh The tool fastp is used for QC. FASTQ that fail are investigated or removed. See fastp for more. | [QC] 03b_collectwgsmetrics.sh BAMs that fail are investigated or removed. See metrics_CollectWgsMetrics for more. | [QC] 05_rmdup_merge.sh is used to mark optical duplicates. See GATK Duplicates for more. | [QC] 07_haplotype_caller.sh used -ERC GVCF mode. This does not remove variants but unlike BP_RESOLUTION, GVCF mode condenses non-variant blocks which could be misunderstood later as missing if not recongnised by the user, for example in a genotype matrix which has been merged with other cohorts. See VCF and VCF and gVCF for more. | [QC] 07c_qc_summary_stats.sh is used to log QC. This implements bcftools stats (https://samtools.github.io/bcftools/bcftools.html#stats) and subsequently the bcftools plot-vcfstats (https://samtools.github.io/bcftools/bcftools.html#plot-vcfstats) using python -m venv envQCplot with matplotlib. Subjects fail are investigated or removed. See metrics_bcftoolsstats. | [QC] !Not used Large cohort hard-filter. The following step is generllay useful before VQSR (shown later), however since our cohort is likely to contain some large populations of unrelated patients with the same causal genetic variants, we skip it to avoid loosing them. (https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering) [A] Hard-filtter a large cohort callset on ExcessHet (https://gatk.broadinstitute.org/hc/en-us/articles/360036726851-ExcessHet) using VariantFiltration (https://gatk.broadinstitute.org/hc/en-us/articles/360036350452-VariantFiltration). ExcessHet filtering applies only to callsets with a large number of samples, e.g. hundreds of unrelated samples. Small cohorts should not trigger ExcessHet filtering as values should remain small. Note cohorts of consanguinous samples will inflate ExcessHet, and it is possible to limit the annotation to founders for such cohorts by providing a pedigree file during variant calling. For example, \"ExcessHet &gt; 54.69\" produces a VCF callset where any record with ExcessHet greater than 54.69 is filtered with the ExcessHet label in the FILTER column. The phred-scaled 54.69 corresponds to a z-score of -4.5. If a record lacks the ExcessHet annotation, it will pass filters. If run, this step would be followed by MakeSitesOnlyVcf (https://gatk.broadinstitute.org/hc/en-us/articles/360036346712-MakeSitesOnlyVcf-Picard-) to speed up analysis. | [QC] 10_vqsr.sh applies the Variant Quality Score Recalibration (VQSR) method in GATK, a sophisticated approach used to assess variant quality and filter out sequencing and data processing artifacts. Our settings are shown in table below; they include priors for SNPs from HapMap=15.0, Omni=12.0, 1000G=10.0, and Truth Sensitivity = 99.7, for INDELS Mills 1KG=12.0, dbSNP=2.0 and Truth Sensitivity=95 and across all, the annotations QD, MQRankSum, ReadPosRankSum, FS, SOR. This two-step procedure begins with building a recalibration model that evaluates the relationship between variant annotations, such as Quality by Depth (QD), Mapping Quality (MQ), and Read Position Rank Sum Test (ReadPosRankSum), against the likelihood that a variant is genuine. Key resources like HapMap and Omni 2.5M SNP chip array data are leveraged to train this model adaptively. By employing an adaptive error model and a Gaussian mixture model, VQSR provides a score known as VQSLOD (Variant Quality Score Log Odds), which indicates the likelihood of each variant being true. The VQSLOD scores are incorporated into the variant call file (VCF), allowing for highly precise and flexible filtering. The overall aim of VQSR is to allocate a calibrated probability to each variant, surpassing traditional methods that rely strictly on fixed annotation value thresholds. This technique is especially crucial in contexts where accuracy in variant filtering significantly impacts subsequent analyses and conclusions, particularly in medical genomics research. Population resource files are publically available at https://console.cloud.google.com/storage/browser/broad-references/hg38/v0/. Further reading about techniques here https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering and here https://gatk.broadinstitute.org/hc/en-us/articles/360035531612-Variant-Quality-Score-Recalibration-VQSR?id=1259. | [QC] see 10b_qc_summary_stats for plink logs ${QC_SUMMARY_STATS}/vqsr/${QV_MODEL}_chr${INDEX}_summary_stat.log | [QC] Optional CollectVariantCallingMetrics. An example is shown here https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering and documentation here https://gatk.broadinstitute.org/hc/en-us/articles/360036363592-CollectVariantCallingMetrics-Picard. | [QC] Optional Other Picard metrics which are not in use: https://broadinstitute.github.io/picard/picard-metric-definitions.html. | [QC] 11_genotype_refinement.sh Genotype Refinement workflow is a collection of steps https://gatk.broadinstitute.org/hc/en-us/articles/360035531432-Genotype-Refinement-workflow-for-germline-short-variants. We will continue to add steps to our current implementation. For example, when we have trio studies we can provide more nuanced variant confidence methods. We currently use (1) CalculateGenotypePosteriors: This tool refines genotype probabilities using family data or population allele frequencies. We use gnomAD af-only-gnomad.hg38.vcf.gz. It enhances the initial genotype likelihoods from variant callers, ensuring accuracy especially when incorporating trio and population data. This step reduces false positives and improves genotype precision. https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors. (2) VariantFiltration: This stage applies filters. We use GQ &lt; 20 on genotype quality scores to flag lower confidence genotypes. It refines variant call quality by annotating individual genotypes based on specified criteria, helping to isolate high-confidence calls from potential errors. https://gatk.broadinstitute.org/hc/en-us/articles/360041850471-VariantFiltration. | [QC] The same stats as 10b_qc_summary_stats for plink logs are run in step 11_genotype_refinement.sh. | [Filter] 12_pre_annotation_processing.sh. We apply the following bcftools filter with (1) QUAL&gt;=30: Filters for variants where the quality score is 30 or higher. The quality score (QUAL) represents the confidence in the correctness of the variant call. (2) INFO/DP&gt;=20: Filters for variants where the total depth of quality base calls (DP in the INFO field of a VCF file) is 20 or more. This is a measure of the total amount of sequencing data that supports the variants in the file. (3) FORMAT/DP&gt;=10: Applies a filter to each individual sample in the VCF file, ensuring that the depth of reads for each genotype call is 10 or more. This ensures that each genotype call in every sample has sufficient data supporting it. (4) FORMAT/GQ&gt;=20: Ensures that the genotype quality (GQ) for each sample’s call is 20 or above. The genotype quality score expresses the confidence in the assignment of the genotype at that specific locus. (5) Then GATK SelectVariants is subsequently applied with --exclude-filtered, --exclude-non-variants, and --remove-unused-alternates. | [QC] 12_pre_annotation_processing.sh also contains the steps using vt (variant tool) https://genome.sph.umich.edu/wiki/Vt to run (1) variant decompose https://genome.sph.umich.edu/wiki/Vt#Decompose a complex operation to ensure that multiallelic variants (a feature of VCF format) are split into distinct observations to remove errors in downstream analysis and (2) variant normalisation https://genome.sph.umich.edu/wiki/Variant_Normalization a proof-bacjed procedure to ensure that variant representations conform to a consensus format. See Tan et. al 3. A variant is parsimonious if and only if it is represented in as few nucleotides as possible without an allele of length 0. Futher reading here https://genome.sph.umich.edu/wiki/Vt#Normalization | [Filter] 13_pre_annotation_MAF.sh runs data preparation steps including a filter with vcftools --max-maf using the source variable MAF_value=\"0.4\". This step is simply for reducing the data size to remove highly common variants. We do not expect more than 40% of the cohort to share a relavent variant. | . Rare disease QV (run after QC-only QV) . | ACMG filtering ACMG criteria and scoring | Variant interpretation in disease. This final step is used in a case-by-case basis to assess whether automated analysis agrees with the consensus clinical genetics approach. | . Table 1. VQSR settings . | VQSR Settings | Explanation | . | SNP Mode |   | . | - HapMap: known=false, training=true, truth=true, prior=15.0 | Used as a high-confidence reference set for training the recalibration model. | . | - Omni: known=false, training=true, truth=false, prior=12.0 | Provides additional training data derived from Omni genotyping arrays. | . | - 1000G: known=false, training=true, truth=false, prior=10.0 | Utilizes data from the 1000 Genomes Project to inform the model on common SNP variations. | . | - Annotations: QD, MQ, MQRankSum, ReadPosRankSum, FS, SOR | Annotations are metrics used to predict the likelihood of a variant being a true genetic variation versus a sequencing artifact. They include quality by depth, mapping quality, mapping quality rank sum test, read position rank sum test, Fisher’s exact test for strand bias, and symmetric odds ratio of strand bias. | . | - Truth Sensitivity Filter Level: 99.7 | Specifies the percentage of true variants to retain at a given VQSLOD score threshold, set here to capture 99.7% of true variants. | . |   |   | . | INDEL Mode |   | . | - Mills: known=false, training=true, truth=true, prior=12.0 | Utilizes the Mills and 1000G gold standard indel dataset for high-accuracy recalibration of indels. | . | - dbSNP: known=true, training=false, truth=false, prior=2.0 | Includes known indel sites from the dbSNP database to enhance the detection and filtering process. | . | - Annotations: QD, MQRankSum, ReadPosRankSum, FS, SOR | Same as for SNPs, these annotations are critical for assessing the likelihood of indels being true genetic variations rather than errors. | . | - Truth Sensitivity Filter Level: 95 | This setting defines the percentage of true indels to retain, aiming to capture 95% of true indels at the specified VQSLOD threshold. | . ",
    "url": "/pages/design_qv_snvindel_v1.html#variables-and-thresholds",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#variables-and-thresholds"
  },"168": {
    "doc": "Design QV SNV INDEL v1",
    "title": "Variables file",
    "content": "During analysis we source the following variables.sh file. We will continue to update this so that all QV variables and thresholds are included here. In future releases, we plan to have each version of the variables file indentified with sensible names like variables_dna_snvindel_v1 etc. # This script is meant to be sourced, not executed. To use the variables # defined in this script, use the command `source variables.sh` in your # own scripts. # main paths HOME=\"/project/home/lawless\" DATA=\"${HOME}/data/wgs\" PROD=\"/project/data/prod\" # processed data STUDYBOOK_DIR=\"${DATA}/study_book_data\" FASTP_DIR=\"${DATA}/fastp\" FASTP_METRICS_DIR=\"${DATA}/fastp_reports\" BAM_DIR=\"${DATA}/bam\" SAMSORT_DIR=\"${DATA}/samsort\" RMDUP_DIR=\"${DATA}/rmdup\" RMDUP_MERGE_DIR=\"${DATA}/rmdup_merge\" BQSR_DIR=\"${DATA}/bqsr\" BQSR_DIR_HOLD=\"${DATA}/bqsr_hold\" KNOWN_SITES=\"${HOME}/data/db/known_sites\" HC_DIR=\"${DATA}/hc\" HC_RENAMED_DIR=\"${DATA}/hc_renamed\" GENOMICS_DB_IMPORT_DIR=\"${DATA}/genomics_db_import\" GENOTYPE_GVCF_DIR=\"${DATA}/genotype_gvcf\" VQSR_DIR=\"${DATA}/vqsr\" GENOTYPE_REFINEMENT_DIR=\"${DATA}/genotype_refine\" PRE_ANNOTATION_DIR=\"${DATA}/pre_annotation\" PRE_ANNOTATION_MAF_DIR=\"${DATA}/pre_annotation_maf\" ANNOTATION_DIR=\"${DATA}/annotation\" ANNOTATION_GNOMAD_DIR=\"${DATA}/annotation_gnomad\" POSTANNOSPLIT=\"${DATA}/annotation_split\" POSTVCF2TABLE=\"${DATA}/vcf2table\" VIRTUAL_PANEL_DIR=\"${DATA}/virtual_panel\" VCFLIB_DIR=\"${DATA}/vcflib_out\" VARLEVEL_DIR=\"${DATA}/variant_level\" VAL_DIR=\"${DATA}/validation\" MAF_value=\"0.4\" GNOMAD_value=\"0.002\" DATABASE_DIR=\"${HOME}/data/db\" dbNSFP_grch38=\"${DATABASE_DIR}/dbnsfp/dbNSFP4.4a_grch38_v3.gz\" THOUSAND_GENOMES=\"/project/home/lawless/data/ref/1000genomes/\" PCA_BIPLOT_1KG=\"${DATA}/pca_biplot_1kg\" SMOOVE=\"${DATA}/smoove\" VSAT=\"${DATA}/plink_skat_vsat\" QC_SUMMARY_STATS=\"${DATA}/qc_summary_stats\" # src PROJECT=\"${HOME}/wgs\" SRC=\"${PROJECT}/src\" # ref REF=\"/project/home/lawless/data/ref/bgzip_fasta/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz\" REF_NONZIP=\"/project/home/lawless/data/ref/bgzip_fasta/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna\" REF_SHARED=\"/project/data/shared/processed_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna\" SV_EXCLUDE_BED=\"/project/home/lawless/data/ref/bed_annotaions/exclude.cnvnator_100bp.GRCh38.20170403.bed\" VEP_VERSION=\"95\" VEP_CACHE_DIR=\"${DATABASE_DIR}/vep${VEP_VERSION}/cache\" VEP_PLUGIN_DIR=\"${DATABASE_DIR}/vep${VEP_VERSION}/plugins\" . ",
    "url": "/pages/design_qv_snvindel_v1.html#variables-file",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#variables-file"
  },"169": {
    "doc": "Design QV SNV INDEL v1",
    "title": "The use of QV in analysis plan",
    "content": "QV are a logical necessity of data cleaning and preperation. Labelling then the single umbrella of QV is useful for simplicity. In reality the steps of QV can be sepated and result from a mixture of different steps or sources. Figure 2 from Povysil et al. 1 provides an example of the application of QV in an analysis. Figure 2. From Povysil et. al 1: Outline of the standard collapsing analysis approach. First, cases and matching controls are selected (part Aa), and the same sample-level quality control (QC) is performed for cases and controls (part Ab). The sample-pruning level comprises relatedness pruning (part Ba) and the removal of population outliers based on principal components (PCs) (part Bb). All the remaining samples are used to perform coverage harmonization (part C), in which sites and therefore variants that show coverage differences between cases and controls are pruned. All remaining variants are used for qualifying variant (QV) selection (part D), in which various filters, including internal and external minor allele frequency (MAF) filters, are applied. The selected QVs are used to build the gene-by-individual collapsing matrix (part E), which indicates the presence of at least one QV. Finally, each gene is tested for an association between QV status and the phenotype of interest (part Fa), and the results can be evaluated by means of a quantile–quantile (QQ) plot (part Fb). ",
    "url": "/pages/design_qv_snvindel_v1.html#the-use-of-qv-in-analysis-plan",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#the-use-of-qv-in-analysis-plan"
  },"170": {
    "doc": "Design QV SNV INDEL v1",
    "title": "References",
    "content": ". | Povysil, G. et al., 2019. Rare-variant collapsing analyses for complex traits: guidelines and applications. Nature Reviews Genetics, 20(12), pp.747-759. DOI: 10.1038/s41576-019-0177-4. &#8617; &#8617;2 &#8617;3 &#8617;4 . | Hillman, P., Baker, C., Hebert, L., Brown, M., Hixson, J., Ashley-Koch, A., Morrison, A.C., Northrup, H. and Au, K.S. (2020), Identification of novel candidate risk genes for myelomeningocele within the glucose homeostasis/oxidative stress and folate/one-carbon metabolism networks. Mol Genet Genomic Med, 8: e1495. https://doi.org/10.1002/mgg3.1495 &#8617; &#8617;2 . | Adrian Tan, Gonçalo R. Abecasis, Hyun Min Kang, Unified representation of genetic variants, Bioinformatics, Volume 31, Issue 13, July 2015, Pages 2202–2204, https://doi.org/10.1093/bioinformatics/btv112 &#8617; . | . ",
    "url": "/pages/design_qv_snvindel_v1.html#references",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html#references"
  },"171": {
    "doc": "Design QV SNV INDEL v1",
    "title": "Design QV SNV INDEL v1",
    "content": "Last update: 20241217 . ",
    "url": "/pages/design_qv_snvindel_v1.html",
    
    "relUrl": "/pages/design_qv_snvindel_v1.html"
  },"172": {
    "doc": "Design statistical genomics v1",
    "title": "Design statistical genomics v1",
    "content": ". | Design statistical genomics v1 . | Aims | . | . Protocol name: design_statistical_genomics_v1 . Statistical genomics to find new cohort-level associations with disease. This documention is an incomplete placeholder. This protocol is made up of several steps which together make the joint analysis. Pages on individual methods include VSAT, setID, ACAT, GWAS. This set of analysis use Design DNA SNV INDEL v1 followed by QV SNV INDEL v1 as input. The following aims are then used to reach the joint analysis. ",
    "url": "/pages/design_statistical_genomics_v1.html#design-statistical-genomics-v1",
    
    "relUrl": "/pages/design_statistical_genomics_v1.html#design-statistical-genomics-v1"
  },"173": {
    "doc": "Design statistical genomics v1",
    "title": "Aims",
    "content": "| Phase | Aim | Status | Task | . | Phase 2 | (0) | Complete | QC, PCA, clinical covariates, outcomes | . | Phase 2 | (1) | 2 experiment complete in progress | Single variant association test (SVAT) GWAS | . | Phase 2 | (2) | 2 experiment complete in progress | Variant set association test (VSAT) gene-level | . | Phase 2 | (3) | Complete | ProteoMCLusteR to generate pathway setID | . | Phase 2 | (4) | 1 experiment complete in progress | Variant set association test (VSAT) pathway-level | . | Phase 2 | (6) | 1 experiment complete in progress | Multiomic ACAT | . ",
    "url": "/pages/design_statistical_genomics_v1.html#aims",
    
    "relUrl": "/pages/design_statistical_genomics_v1.html#aims"
  },"174": {
    "doc": "Design statistical genomics v1",
    "title": "Design statistical genomics v1",
    "content": "Last update: 20241217 . ",
    "url": "/pages/design_statistical_genomics_v1.html",
    
    "relUrl": "/pages/design_statistical_genomics_v1.html"
  },"175": {
    "doc": "DNA annotation",
    "title": "DNA annotation",
    "content": ". | DNA annotation | . Variant annotation is a critical step in clinical and statistical genetics. Popular tools for applying annotation data to VCF format genetic data include: . | Variant Effect Predictor (VEP) link: VEP | NIRVANA link: NIRVANA | ANNOVAR link: ANNOVAR | . We are using VEP with Conda, but we are likely to test additional methods (licence). Additionally, these tools must be paired with a set of data sources containing the annotation information which will be applied to each variant. | View our list of approx. 160 databases. | . The variant consequence may be one of the defining criteria by which variants can be included in analysis since they are interpretable or of ostensibly known significance. The consequences provided by VEP can provide a simple reference example to understand its function. For example, HIGH impact variants might be a likely consequence for identifying candidates disease-causing: Ensembl Variation - Calculated variant consequences.\\ . You may have observed cases in literature where clinical research reporting relied on variant effect consequence alone for known disease-related genes, but this practice is likely to introduce spurious results. It is important to use established criteria for selecting consequences of interest combined with additional filtering methods to define evidence thresholds. See the ACMG interpretation standards for examples. ",
    "url": "/pages/dna_annotation.html#dna-annotation",
    
    "relUrl": "/pages/dna_annotation.html#dna-annotation"
  },"176": {
    "doc": "DNA annotation",
    "title": "DNA annotation",
    "content": "Last update: 20241216 . ",
    "url": "/pages/dna_annotation.html",
    
    "relUrl": "/pages/dna_annotation.html"
  },"177": {
    "doc": "DNA interpretation",
    "title": "DNA Interpretation",
    "content": ". | DNA Interpretation | . We will perform a range of interpretation steps for: . | Generalised single-case clinical variant classification | Cohort-level classification | . For example, we will perform interpretation of variants by ACMG standards and guidelines. Extensive annotation is applied during our genomics analysis. Interpretation of genetic determinants of disease is based on many evidence sources. One important source of interpretation comes from the Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology [richards2015standards], full text at doi: 10.1038/gim.2015.30. Check the tables linked here: temporary link. These are provided as they appear in the initial steps of our filtering protocol for the addition of ACMG-standardised labels to candidate causal variants. | Criteria for classifications | Caveats implementing filters | . Implementing the guidelines for interpretation of annotation requires multiple programmatic steps. The number of individual caveat checks indicate the number of bioinformatic filter functions used. Unnumbered caveat checks indicate that only a single filter function is required during reference to annotation databases. However, each function depends on reference to either one or several evidence source databases (approximately 160 sources). For reference, alternative public implementations of ACMG guidelines can be found in [li2017intervar] and [xavier2019tapes]; please note these tools have not implemented here nor is any assertion of their quality offered. Examples of effective variant filtering and expected candidate variant yield in studies of rare human disease are provided by [pedersen2021effective]. We plan to use our tools built for these requirements which are currently in review: . | ACMGuru for automated clinical genetics evidence interpretation. | ProteoMCLustR for unbiased whole-genome pathway clustering; | SkatRbrain for statistical sequence kernel association testing with variant collapse. | UntangleR for pathway visualisation. | AutoDestructR for protein structure variant mapping. All tools were designed for modular automated high-performance computing. | . ",
    "url": "/pages/dna_interpretation.html#dna-interpretation",
    
    "relUrl": "/pages/dna_interpretation.html#dna-interpretation"
  },"178": {
    "doc": "DNA interpretation",
    "title": "DNA interpretation",
    "content": "Last update: 20241216 . ",
    "url": "/pages/dna_interpretation.html",
    
    "relUrl": "/pages/dna_interpretation.html"
  },"179": {
    "doc": "DNA QC",
    "title": "DNA quality control (QC)",
    "content": ". | DNA quality control (QC) | . We use a number of tools such as github.com/OpenGene/fastp for comprehensive quality profiling for both before and after filtering data. | fastp (page coming) | Bcftools vcf-stats (page coming) | . ",
    "url": "/pages/dna_qc.html#dna-quality-control-qc",
    
    "relUrl": "/pages/dna_qc.html#dna-quality-control-qc"
  },"180": {
    "doc": "DNA QC",
    "title": "DNA QC",
    "content": "Last update: 20241216 . ",
    "url": "/pages/dna_qc.html",
    
    "relUrl": "/pages/dna_qc.html"
  },"181": {
    "doc": "Docker with singularity",
    "title": "Downloading Docker images using Singularity",
    "content": "The following method is used on BioMedIT sciCORE for running docker images. You can use Singularity to download Docker images from public Docker registries. This is the list of accessible Docker registries: . | Docker Hub | Quay.io | GCR.io | Google Container Registry | . Those Docker registries are accessible using a proxy so you should modify your pull command to use the internal proxy. See some examples below: . ",
    "url": "/pages/docker_singularity.html#downloading-docker-images-using-singularity",
    
    "relUrl": "/pages/docker_singularity.html#downloading-docker-images-using-singularity"
  },"182": {
    "doc": "Docker with singularity",
    "title": "Download Docker Image from Dockerhub",
    "content": "The command to execute in an environment with unrestricted internet access to download the freesurfer image from Dockerhub would be “singularity pull docker://freesurfer/freesurfer:7.3.1” but we have to modify the pull command to add our proxy url (bm-soft.scicore.unibas.ch:444). See the example below: . singularity pull docker://bm-soft.scicore.unibas.ch:444/freesurfer/freesurfer:7.3.1 . ",
    "url": "/pages/docker_singularity.html#download-docker-image-from-dockerhub",
    
    "relUrl": "/pages/docker_singularity.html#download-docker-image-from-dockerhub"
  },"183": {
    "doc": "Docker with singularity",
    "title": "Download Biocontainer Docker Images",
    "content": "The Biocontainers project provides a curated list of container images with many bioinformatics tools. You can download the biocontainer images like this: . singularity pull docker://bm-soft.scicore.unibas.ch:444/biocontainers/blast:2.2.31 . ",
    "url": "/pages/docker_singularity.html#download-biocontainer-docker-images",
    
    "relUrl": "/pages/docker_singularity.html#download-biocontainer-docker-images"
  },"184": {
    "doc": "Docker with singularity",
    "title": "Download Big Docker Images",
    "content": "It can happen that your container image is too big to fit in local disk when pulling it. If that’s the case you can use the environment variables SINGULARITY_TMPDIR and SINGULARITY_CACHEDIR e.g. mkdir $HOME/tmp/ mkdir $HOME/tmp/cache SINGULARITY_TMPDIR=$HOME/tmp/ SINGULARITY_CACHEDIR=$HOME/tmp/cache singularity pull docker://bm-soft.scicore.unibas.ch:444/freesurfer/freesurfer:7.3.1 . ",
    "url": "/pages/docker_singularity.html#download-big-docker-images",
    
    "relUrl": "/pages/docker_singularity.html#download-big-docker-images"
  },"185": {
    "doc": "Docker with singularity",
    "title": "Docker with singularity",
    "content": "Last update: 20240828 . ",
    "url": "/pages/docker_singularity.html",
    
    "relUrl": "/pages/docker_singularity.html"
  },"186": {
    "doc": "Documentation log",
    "title": "Documentation log count",
    "content": "Last update: . ## [1] \"2025-01-08\" . This doc was built with: rmarkdown::render(\"documentation_log.Rmd\", output_file = \"../pages/documentation_log.md\") . Below is the latest count of documentation pages. This figure is updated sporadically to track the current page count and estimated growth rate. This count only considers the main sources in ./pages but ignores other code pages, images, etc. ## Regression Summary: ## ## Call: ## lm(formula = FileCount ~ DateNumeric, data = df) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.9994 6.8523 -12.8702 -9.5141 -11.0321 -0.8189 26.3837 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.705e+03 4.977e+02 -3.427 0.0187 * ## DateNumeric 8.742e-02 2.505e-02 3.490 0.0175 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 14.98 on 5 degrees of freedom ## Multiple R-squared: 0.709, Adjusted R-squared: 0.6508 ## F-statistic: 12.18 on 1 and 5 DF, p-value: 0.01746 ## Estimated Annual Growth Rate (pages/year): 31.91 ## Current count: 77 . ",
    "url": "/pages/documentation_log.html#documentation-log-count",
    
    "relUrl": "/pages/documentation_log.html#documentation-log-count"
  },"187": {
    "doc": "Documentation log",
    "title": "Change log",
    "content": "See generate_gitlog.sh for usage. | View Git Log 2025 | View Git Log 2024 | View Git Log 2023 | . (current year log may not be up-to-date) . ",
    "url": "/pages/documentation_log.html#change-log",
    
    "relUrl": "/pages/documentation_log.html#change-log"
  },"188": {
    "doc": "Documentation log",
    "title": "2025",
    "content": "Wed Jan 8 10:28:21 2025 +0100 - DylanLawless: pca gwas tutorial link . Wed Jan 8 10:14:38 2025 +0100 - DylanLawless: multiblock data fusion methods . Tue Jan 7 14:51:50 2025 +0100 - DylanLawless: PCA RNA de examples and reading . Tue Jan 7 14:34:03 2025 +0100 - DylanLawless: poplation structure . Tue Jan 7 13:37:20 2025 +0100 - DylanLawless: pca king ibd . Mon Jan 6 18:19:10 2025 +0100 - DylanLawless: pca features . Thu Jan 2 14:12:02 2025 +0100 - DylanLawless: release docs . Thu Jan 2 10:49:11 2025 +0100 - DylanLawless: causal inference stats . Git log for 2025 contains 7 entries and is saved to gitlog_2025.txt . ",
    "url": "/pages/documentation_log.html#2025",
    
    "relUrl": "/pages/documentation_log.html#2025"
  },"189": {
    "doc": "Documentation log",
    "title": "2024",
    "content": "Fri Dec 20 16:34:53 2024 +0100 - DylanLawless: pca page . Fri Dec 20 00:03:46 2024 +0100 - DylanLawless: metrics . Wed Dec 18 17:07:52 2024 +0100 - DylanLawless: slurm notes . Tue Dec 17 15:36:35 2024 +0100 - DylanLawless: design docs naviation and content linked . Tue Dec 17 13:07:00 2024 +0100 - DylanLawless: metrics pages . Mon Dec 16 15:53:57 2024 +0100 - DylanLawless: qv page . Mon Dec 16 10:07:32 2024 +0100 - dylan: design docs DNA v1 . Sat Dec 14 11:46:50 2024 +0100 - DylanLawless: vcf gvcf and QV protocol started . Tue Dec 10 19:02:52 2024 +0100 - DylanLawless: search function . Tue Dec 10 18:53:01 2024 +0100 - DylanLawless: host error . Tue Dec 10 18:18:22 2024 +0100 - DylanLawless: ref page update . Thu Dec 5 15:01:01 2024 +0100 - dylan: annotation table . Thu Dec 5 14:05:24 2024 +0100 - dylan: acmg criteria scoring . Wed Dec 4 20:01:58 2024 +0100 - DylanLawless: conflicts . Wed Dec 4 19:59:11 2024 +0100 - DylanLawless: bayes mutiparam biassay for ld50 . Wed Dec 4 17:51:33 2024 +0100 - DylanLawless: mcmc samplers with demo . Mon Dec 2 12:33:31 2024 +0100 - DylanLawless: sofa score . Sat Nov 30 15:22:58 2024 +0100 - dylan: bayes multiparam . Sat Nov 30 10:37:36 2024 +0100 - dylan: plotted example of bayes multiparam nuicence param . Fri Nov 29 22:16:17 2024 +0100 - dylan: gemfile fix . Fri Nov 29 22:07:15 2024 +0100 - dylan: bayes multiparam . Fri Nov 29 16:57:19 2024 +0100 - DylanLawless: unfinished bayes multiparam . Fri Nov 29 14:46:41 2024 +0100 - DylanLawless: index page image . Fri Nov 29 14:42:56 2024 +0100 - DylanLawless: documentation log page . Fri Nov 29 13:44:41 2024 +0100 - DylanLawless: altman plot path update . Fri Nov 29 12:46:14 2024 +0100 - DylanLawless: several altman and bland methods as stats pages . Fri Nov 29 10:22:31 2024 +0100 - dylan: discretete prob bayes example . Thu Nov 28 14:17:55 2024 +0100 - DylanLawless: remove trash pages . Wed Nov 27 14:29:27 2024 +0100 - DylanLawless: phoenix values . Wed Nov 27 14:18:11 2024 +0100 - DylanLawless: phoenix figures . Wed Nov 27 13:58:17 2024 +0100 - DylanLawless: phoenix score . Wed Nov 20 12:42:40 2024 +0100 - DylanLawless: slurm manager note . Wed Nov 20 12:36:14 2024 +0100 - DylanLawless: slurm manage . Wed Nov 20 12:34:09 2024 +0100 - DylanLawless: slurm manage . Sun Nov 17 19:33:37 2024 +0100 - dylan: bayesian 2 . Wed Nov 13 16:40:02 2024 +0100 - DylanLawless: Baysian example BDA3 . Wed Nov 13 16:32:35 2024 +0100 - DylanLawless: Baysian example BDA3 . Wed Nov 6 18:40:46 2024 +0100 - DylanLawless: download links . Wed Nov 6 17:49:20 2024 +0100 - DylanLawless: added downloads . Wed Nov 6 17:35:34 2024 +0100 - DylanLawless: semantic evidence network plots . Mon Nov 4 09:40:44 2024 +0100 - DylanLawless: bevimed . Fri Nov 1 17:49:11 2024 +0100 - DylanLawless: complete sequencing_assay concepts . Tue Oct 22 11:29:20 2024 +0200 - Dylan: updated variant concepts . Mon Sep 23 14:34:22 2024 +0200 - dylan: SPHN concept merge updates . Mon Sep 23 14:13:58 2024 +0200 - dylan: SPHN concept merge updates . Fri Sep 20 12:25:53 2024 +0200 - DylanLawless: aims to RDF . Fri Sep 20 11:59:01 2024 +0200 - DylanLawless: download paths . Fri Sep 20 11:46:43 2024 +0200 - DylanLawless: variant to RDF mapping prep . Wed Aug 28 12:59:25 2024 +0200 - dylan: docker with singularity . Wed Aug 28 11:06:12 2024 +0200 - dylan: acat images . Wed Aug 28 10:50:32 2024 +0200 - dylan: acat pngs . Wed Aug 28 10:31:48 2024 +0200 - dylan: acat . Tue Aug 27 13:13:34 2024 +0200 - DylanLawless: guru . Tue Aug 27 13:00:30 2024 +0200 - DylanLawless: acmg criteria . Tue Aug 27 11:19:06 2024 +0200 - DylanLawless: mathjax . Mon Aug 26 17:21:49 2024 +0200 - dylan: vsat skat . Mon Aug 26 14:59:32 2024 +0200 - dylan: filter vcf with bcftools . Fri Aug 23 15:36:13 2024 +0200 - Dylan: vsat setid . Tue Aug 20 17:36:10 2024 +0200 - Dylan: sv docs, sbatch, variables . Mon Aug 19 15:50:11 2024 +0200 - dylan: typos . Mon Aug 19 15:47:11 2024 +0200 - dylan: benchmarking . Mon Aug 19 15:22:31 2024 +0200 - dylan: synth data . Mon Aug 19 15:18:13 2024 +0200 - dylan: synth data . Mon Aug 19 14:59:14 2024 +0200 - dylan: synth data . Mon Aug 19 14:54:23 2024 +0200 - dylan: synthetic data . Wed Aug 7 15:57:54 2024 +0200 - dylan: 1kg pca . Mon Jul 29 09:48:51 2024 +0200 - Dylan: Merge branch ‘main’ of github.com:SwissPedHealth-PipelineDev/docs into main . Mon Jul 29 09:48:34 2024 +0200 - Dylan: gwas page . Wed Jul 24 15:08:01 2024 +0200 - DylanLawless: virtual gene panels . Fri Jul 19 17:03:57 2024 +0200 - DylanLawless: benchmark errors . Wed Jul 17 14:44:33 2024 +0200 - dylan: benchmark . Wed Jul 17 14:22:03 2024 +0200 - dylan: benchmark . Wed Jul 17 10:09:34 2024 +0200 - dylan: whitepaper link . Wed Jul 17 10:00:47 2024 +0200 - dylan: precision med . Wed Jul 17 09:46:21 2024 +0200 - dylan: .nojekyll to prevent github from building . Wed Jul 17 09:43:01 2024 +0200 - dylan: gem . Wed Jul 17 09:40:37 2024 +0200 - dylan: config . Wed Jul 17 09:35:21 2024 +0200 - dylan: Merge branch ‘main’ of github.com:SwissPedHealth-PipelineDev/docs into main . Wed Jul 17 09:35:08 2024 +0200 - dylan: pmu . Tue Jul 16 17:26:36 2024 +0200 - dylan: pmu page . Wed Jul 10 13:39:26 2024 +0200 - DylanLawless: Merge branch ‘main’ of dylanlawless.github.com:SwissPedHealth-PipelineDev/docs . Tue Jul 9 14:10:39 2024 +0200 - Dylan: quick start index . Tue Jul 9 13:46:03 2024 +0200 - Dylan: gem . Tue Jul 9 13:35:31 2024 +0200 - Dylan: ref path . Thu Jun 13 12:32:41 2024 +0200 - Dylan: read group with bwa . Tue Jun 11 14:33:05 2024 +0200 - Dylan: Merge branch ‘main’ of github.com:SwissPedHealth-PipelineDev/docs into main . Tue Jun 11 14:18:41 2024 +0200 - Dylan: aggregate multiplex read group . Fri Jun 7 13:30:52 2024 +0200 - DylanLawless: Merge branch ‘main’ of dylanlawless.github.com:SwissPedHealth-PipelineDev/docs . Sun Jun 2 11:14:29 2024 +0200 - dylan: test . Fri May 31 16:50:17 2024 +0200 - Dylan: gatk pages . Fri Mar 1 15:28:53 2024 +0100 - DylanLawless: Merge branch ‘main’ of dylanlawless.github.com:SwissPedHealth-PipelineDev/docs . Git log for 2024 contains 90 entries and is saved to gitlog_2024.txt . ",
    "url": "/pages/documentation_log.html#2024",
    
    "relUrl": "/pages/documentation_log.html#2024"
  },"190": {
    "doc": "Documentation log",
    "title": "2023",
    "content": "Wed Nov 8 15:30:44 2023 +0100 - Dylan: rnaseq . Wed Aug 23 11:45:54 2023 +0200 - Dylan: design docs . Tue Aug 1 10:07:14 2023 +0200 - Dylan: git . Thu Jul 27 10:51:04 2023 +0200 - Dylan: fastp . Wed Jul 26 08:05:42 2023 +0200 - Dylan: rna design start . Tue Jul 25 13:22:57 2023 +0200 - Dylan: fastq . Sun Jul 9 09:21:02 2023 +0200 - Dylan: progress notes . Sun Jul 2 19:16:01 2023 +0200 - Dylan: presentation template . Wed Jun 21 16:26:30 2023 +0200 - DylanLawless: bookmark . Wed Jun 21 11:22:46 2023 +0200 - DylanLawless: concept examples . Mon Jun 19 18:44:59 2023 +0200 - DylanLawless: revert missing annotation table head code . Mon Jun 19 18:14:49 2023 +0200 - DylanLawless: concepts . Fri Jun 16 17:37:07 2023 +0200 - DylanLawless: data stream image . Fri Jun 16 17:33:11 2023 +0200 - DylanLawless: pages: hpc, data stream, concepts . Thu Jun 15 19:09:10 2023 +0200 - DylanLawless: data stream page . Thu Jun 15 18:22:38 2023 +0200 - DylanLawless: data stream page . Thu Jun 8 11:09:30 2023 +0200 - DylanLawless: layout.md . Fri Jun 2 11:57:01 2023 +0200 - DylanLawless: mission . Wed May 31 18:24:26 2023 +0200 - Dylan: move favicon to root due to not displ on sub pages . Wed May 31 18:17:18 2023 +0200 - Dylan: favocon color for pinned tab mask . Wed May 31 18:08:41 2023 +0200 - Dylan: favicon on safari pinned error . Wed May 31 17:59:02 2023 +0200 - Dylan: favicon . Wed May 31 17:33:48 2023 +0200 - Dylan: design docs . Wed May 31 15:23:16 2023 +0200 - Dylan: method tests for pdf . Wed May 31 11:03:26 2023 +0200 - Dylan: index page . Wed May 31 09:40:34 2023 +0200 - Dylan: realtive links for githubpages . Wed May 31 09:38:16 2023 +0200 - Dylan: realtive links for githubpages . Wed May 31 09:19:08 2023 +0200 - Dylan: annotation table . Wed May 31 07:47:37 2023 +0200 - Dylan: test table . Mon May 29 14:46:07 2023 +0200 - Dylan: sidebar . Mon May 29 14:43:12 2023 +0200 - Dylan: sidebar . Mon May 29 14:36:54 2023 +0200 - Dylan: config and nav logo . Mon May 29 14:30:53 2023 +0200 - Dylan: logo test . Mon May 29 14:27:47 2023 +0200 - Dylan: variables note test . Mon May 29 14:25:46 2023 +0200 - Dylan: Non-mimums features from just the docs repo . Mon May 29 14:14:22 2023 +0200 - Dylan: config . Mon May 29 14:08:29 2023 +0200 - Dylan: config . Mon May 29 13:55:10 2023 +0200 - Dylan: nav oder test . Mon May 29 13:52:24 2023 +0200 - Dylan: present . Mon May 29 13:31:54 2023 +0200 - Dylan: 404 . Mon May 29 13:30:33 2023 +0200 - Dylan: page meta . Mon May 29 13:27:30 2023 +0200 - Dylan: docs dir . Mon May 29 13:25:16 2023 +0200 - Dylan: readme . Mon May 29 13:23:49 2023 +0200 - Dylan: mv . Mon May 29 13:23:06 2023 +0200 - Dylan: README . Mon May 29 13:02:16 2023 +0200 - DylanLawless: Update index.md . Mon May 29 12:53:49 2023 +0200 - DylanLawless: Update _config.yml . Mon May 29 12:52:04 2023 +0200 - DylanLawless: Update README.md . Mon May 29 12:51:37 2023 +0200 - DylanLawless: Update index.md . Fri May 12 17:21:19 2023 +0000 - DylanLawless: Initial commit . Git log for 2023 contains 49 entries and is saved to gitlog_2023.txt . ",
    "url": "/pages/documentation_log.html#2023",
    
    "relUrl": "/pages/documentation_log.html#2023"
  },"191": {
    "doc": "Documentation log",
    "title": "Documentation log",
    "content": " ",
    "url": "/pages/documentation_log.html",
    
    "relUrl": "/pages/documentation_log.html"
  },"192": {
    "doc": "Exomiser",
    "title": "Exomiser Scoring and Ranking",
    "content": ". | Exomiser Scoring and Ranking . | 1. Overview | 2. Algorithm . | 2.1 Notation | 2.2 Filtering (no equations) | 2.3 Variant score | 2.4 Phenotype score | 2.5 Gene-level aggregation | 2.6 Final Exomiser score | 2.7 Ranking | . | 3. Summary | 4. References | Appendix. YAML to equation mapping | . | . See exomiser phenodigm for the phenotype similarity algorithm in context. ",
    "url": "/pages/exomiser.html#exomiser-scoring-and-ranking",
    
    "relUrl": "/pages/exomiser.html#exomiser-scoring-and-ranking"
  },"193": {
    "doc": "Exomiser",
    "title": "1. Overview",
    "content": "Exomiser assigns each surviving variant a variant score. Those that passes pre-scoring filters for genotype–mode of inheritance compatibility, quality, population frequency, and consequence. Each gene containing one or more surviving variants also receives a phenotype score. The highest variant score per gene and its phenotype score are then combined using a logistic regression model, pre-trained by the authors on 10 000 pathogenic and 10 000 benign variants with 10-fold cross-validation. The model coefficients are fixed in Exomiser and are applied directly to the user’s data to produce the final Exomiser score in the range \\([0,1]\\), which determines the ranking of candidate variants. ",
    "url": "/pages/exomiser.html#1-overview",
    
    "relUrl": "/pages/exomiser.html#1-overview"
  },"194": {
    "doc": "Exomiser",
    "title": "2. Algorithm",
    "content": "2.1 Notation . Let: . | \\(f\\) = maximum MAF across selected frequency datasets (gnomAD, TOPMed, UK10K, 1000 Genomes, ExAC, ESP, plus any in-house set) | \\(V\\) = variant-level score | \\(P\\) = phenotype score for a gene | \\(S\\) = final Exomiser score for a gene | . All scores lie in \\([0,1]\\). 2.2 Filtering (no equations) . A variant is “surviving” if it passes: . | MOI compatibility for at least one selected inheritance mode (AD, AR, XD, XR, MT) | Quality thresholds | Population allele frequency limits | Allowed predicted consequence types | . 2.3 Variant score . Frequency score (reported in Cipriani et al.): . \\[\\text{freqScore}(f) = \\begin{cases} 1, &amp; f=0 \\\\[4pt] 1.13533 - 0.13533\\, e^{100f}, &amp; 0 &lt; f \\le 0.02 \\\\[4pt] 0, &amp; f &gt; 0.02 \\end{cases}\\] The constants \\(1.13533\\) and \\(0.13533\\) in the frequency score equation are fixed values calibrated once by the Exomiser authors using their reference dataset. They define the shape of the exponential decay from rare to common variants and are not recalculated for user runs. The only run-specific input is \\(f\\), the maximum MAF across the frequency datasets selected for that analysis, which is plugged into the fixed equation to yield the frequency score. Pathogenicity score (reported): Let \\(\\text{normPred}_k \\in [0,1]\\) be the normalised score from predictor \\(k\\) (PolyPhen-2, SIFT, MutationTaster, CADD, REVEL, M-CAP, MPC, MVP, PrimateAI, etc.). High-impact consequences (frameshift, nonsense, canonical splice acceptor/donor, start-loss, stop-loss) are fixed at \\(1.0\\), splice-region at \\(0.8\\). If no predictor data exist, use a preset by consequence. \\[\\text{pathScore} = \\max_k \\,\\text{normPred}_k\\] Variant score (reported): . \\[V = \\text{freqScore}(f) \\cdot \\text{pathScore}\\] Compound heterozygote averaging for AR (reported): If \\(V_1, V_2\\) are the two alleles: . \\[V_{\\text{AR}} = \\frac{V_1 + V_2}{2}\\] The frequency score equation is applied identically to homozygous and heterozygous variants. In autosomal recessive mode, Exomiser averages the two variant scores for a compound heterozygote. If trio data are available, it uses inheritance patterns to confirm the variants are in trans; otherwise, it assumes potential compound heterozygosity when a gene contains two or more rare, surviving variants, which can produce false positives in unphased singleton data. 2.4 Phenotype score . Computed per gene using PhenoDigm semantic similarity (reported). The patient’s HPO terms are compared against annotations from: . | Human diseases (OMIM, Orphanet) | Mouse models (MGI, IMPC) | Zebrafish models (ZFIN) | Protein–protein interaction neighbours (STRING) | . Because PhenoDigm uses ontology expansion, each HPO term is expanded to include all parent terms. Let \\(P \\in [0,1]\\) be the best phenotype match for the gene from any source. MOI consistency penalty (reported): . \\[P' = \\begin{cases} \\tfrac{1}{2}P, &amp; \\text{OMIM MOI conflict and omimPrioritiser enabled} \\\\ P, &amp; \\text{otherwise} \\end{cases}\\] . 2.5 Gene-level aggregation . For gene \\(g\\) with surviving variants \\(\\{v\\}\\) and their scores \\(V_v\\): . \\(V_g^\\ast = \\max_{v \\in g} V_v\\) (reported) . 2.6 Final Exomiser score . Logistic combination: form inferred, training setup reported. Model trained once on reference data; coefficients \\(\\beta_0,\\beta_1,\\beta_2\\) are fixed per Exomiser release. \\[S_g = \\sigma\\!\\big( \\beta_0 + \\beta_1 V_g^\\ast + \\beta_2 P'_g \\big) \\qquad\\text{with}\\qquad \\sigma(x) = \\frac{1}{1 + e^{-x}}\\] This step combines the variant score \\(V_g^\\ast\\) and phenotype score \\(P'_g\\) into a single probability-like value using a logistic regression model. The logistic function \\(\\sigma(x)\\) maps any real value to the range \\(0 \\le S_g \\le 1\\). The coefficients \\(\\beta_0, \\beta_1, \\beta_2\\) set the relative weight of the two scores. \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the weight for the variant score \\(V_g^\\ast\\), and \\(\\beta_2\\) is the weight for the phenotype score \\(P'_g\\). The logistic regression uses these three parameters together to balance baseline probability with the contributions of variant and phenotype evidence. The model was trained once on 10 000 pathogenic and 10 000 benign variants with 10-fold cross-validation. The resulting coefficients are fixed in each release and applied directly to user data. Variants not in the training set are scored in the same way as those that were, so novel variants are fully supported. 2.7 Ranking . Genes are ranked in descending \\(S_g\\) (reported). ",
    "url": "/pages/exomiser.html#2-algorithm",
    
    "relUrl": "/pages/exomiser.html#2-algorithm"
  },"195": {
    "doc": "Exomiser",
    "title": "3. Summary",
    "content": "Exomiser integrates variant rarity, predicted functional impact, ontology-aware phenotype–gene similarity across multiple species and sources, and MOI compatibility into a single ranking metric. Even minimal phenotype input (e.g. one HPO term) can influence ranking due to ontology expansion, though richer descriptions yield stronger matches. ",
    "url": "/pages/exomiser.html#3-summary",
    
    "relUrl": "/pages/exomiser.html#3-summary"
  },"196": {
    "doc": "Exomiser",
    "title": "4. References",
    "content": "An Improved Phenotype-Driven Tool for Rare Mendelian Variant Prioritization: Benchmarking Exomiser on Real Patient Whole-Exome Data. Cipriani V, Pontikos N, Arno G, Sergouniotis PI, Lenassi E, Thawong P, Danis D, Michaelides M, Webster AR, Moore AT, Robinson PN, Jacobsen JOB, Smedley D. Genes (Basel). 2020 Apr 23;11(4):460 PMID:32340307 DOI:https://doi.org/10.3390/genes11040460 . 100,000 Genomes Pilot on Rare-Disease Diagnosis in Health Care – Preliminary Report. 100,000 Genomes Project Pilot Investigators; Smedley D, … Caulfield M. N Engl J Med. 2021 Nov 11;385(20):1868-1880. PMID:34758253 DOI:https://doi.org/10.1056/NEJMoa2035790 . See exomiser phenodigm for the phenotype similarity algorithm in context. Damian Smedley, Anika Oellrich, Sebastian Köhler, Barbara Ruef, Sanger Mouse Genetics Project, Monte Westerfield, Peter Robinson, Suzanna Lewis, Christopher Mungall, PhenoDigm: analyzing curated annotations to associate animal models with human diseases, Database, Volume 2013, 2013, bat025, https://doi.org/10.1093/database/bat025 . ",
    "url": "/pages/exomiser.html#4-references",
    
    "relUrl": "/pages/exomiser.html#4-references"
  },"197": {
    "doc": "Exomiser",
    "title": "Appendix. YAML to equation mapping",
    "content": "Example config: . inheritanceModes: [AUTOSOMAL_DOMINANT, AUTOSOMAL_RECESSIVE, X_LINKED_RECESSIVE, X_LINKED_DOMINANT, MITOCHONDRIAL] frequencySources: [GNOMAD, TOPMED, UK10K, THOUSAND_GENOMES, EXAC, ESP, LOCAL] frequencyFilter: maxFreq: AD: 0.001 AR: 0.01 XR: 0.01 XD: 0.001 MT: 0.01 pathogenicitySources: [CADD, REVEL, SIFT, POLYPHEN, MUTATION_TASTER, MVP, PRIMATE_AI, M_CAP, MPC] hiPhivePrioritiser: {} omimPrioritiser: true qualityFilter: 20 variantEffectFilter: keep: - frameshift_variant - stop_gained - splice_acceptor_variant - splice_donor_variant - start_lost - stop_lost - missense_variant - splice_region_variant whitelist: clinvar_path_lpath.tsv.gz . | YAML key | Symbol in text | How it is used | . | frequencySources | \\(f\\) | \\(f = \\max_{\\text{sources}} \\mathrm{MAF}\\) for the variant in the selected datasets. Feeds \\(\\text{freqScore}(f)\\). | . | frequencyFilter.maxFreq.* | survival gate on \\(f\\) | Pre-filter by MOI. If variant fails, it is not scored. Also aligns with piecewise caps in \\(\\text{freqScore}(f)\\). | . | pathogenicitySources | \\(\\text{normPred}_k\\) | Predictors whose normalised scores enter \\(\\text{pathScore} = \\max_k \\text{normPred}_k\\). | . | variantEffectFilter.keep | consequence policy | Determines which consequences survive filtering. High-impact consequences map to fixed scores inside \\(\\text{pathScore}\\): frameshift, nonsense, canonical splice, start-loss, stop-loss set to \\(1.0\\); splice-region set to \\(0.8\\). | . | qualityFilter | survival gate | Variants must meet quality to be considered surviving before any scoring. | . | inheritanceModes | MOI filter and AR combine | Controls survival and whether compound heterozygote averaging \\(V_{\\mathrm{AR}} = \\frac{V_1 + V_2}{2}\\) applies. | . | hiPhivePrioritiser | \\(P\\) | Enables PhenoDigm to compute the phenotype score \\(P \\in [0,1]\\) for each gene. | . | omimPrioritiser | \\(P'\\) | When true and OMIM MOI conflicts, apply \\(P' = \\frac{1}{2} P\\), else $$P’ = P$. | . | whitelist | survival bypass | Listed variants bypass frequency and consequence filters and thus can contribute to $$V_g^\\ast$. | . Referenced equations: . \\[\\text{freqScore}(f) = \\begin{cases} 1, &amp; f=0\\\\ 1.13533 - 0.13533\\, e^{100f}, &amp; 0 &lt; f \\le 0.02\\\\ 0, &amp; f&gt;0.02 \\end{cases}\\] \\[\\text{pathScore}=\\max_k \\text{normPred}_k \\qquad V=\\text{freqScore}(f)\\cdot \\text{pathScore}\\] \\[V_{\\text{AR}}=\\frac{V_1+V_2}{2} \\qquad V_g^\\ast=\\max_{v\\in g} V_v \\qquad P'=\\begin{cases} \\tfrac{1}{2}P,&amp; \\text{OMIM MOI conflict}\\\\ P,&amp; \\text{otherwise} \\end{cases}\\] \\[S_g=\\sigma\\!\\big(\\beta_0+\\beta_1 V_g^\\ast+\\beta_2 P'_g\\big) \\qquad \\sigma(x)=\\frac{1}{1+e^{-x}}\\] Provenance: frequency formula, high-impact fixed scores, AR averaging, MOI penalty, use of a fixed pre-trained logistic model are reported. The explicit logistic form with symbolic coefficients is inferred. ",
    "url": "/pages/exomiser.html#appendix-yaml-to-equation-mapping",
    
    "relUrl": "/pages/exomiser.html#appendix-yaml-to-equation-mapping"
  },"198": {
    "doc": "Exomiser",
    "title": "Exomiser",
    "content": "Last update: 20250809 . ",
    "url": "/pages/exomiser.html",
    
    "relUrl": "/pages/exomiser.html"
  },"199": {
    "doc": "Exomiser phenodigm",
    "title": "PhenoDigm algorithm",
    "content": ". | PhenoDigm algorithm . | Overview | Pairwise concept similarity | Set to set similarity | Scaling to a percentage of a hypothetical perfect match | Combined percentage score | Relation to Exomiser’s phenotype score | References | . | . See exomiser for the use of this algorithm in context. ",
    "url": "/pages/exomiser_phenodigm.html#phenodigm-algorithm",
    
    "relUrl": "/pages/exomiser_phenodigm.html#phenodigm-algorithm"
  },"200": {
    "doc": "Exomiser phenodigm",
    "title": "Overview",
    "content": "PhenoDigm provides a cross-species phenotype similarity measure used by Exomiser to score gene–phenotype relevance. It integrates HPO, MP, and ZP annotations via OWLSim, plus bridging ontologies such as UBERON and PATO to align terms across species. Each disease or model is represented as a set of ontology concepts with inferred attributes up the ontology graph, so matches consider ancestor terms automatically. This section focuses on the equations and scoring steps. Where the paper gives an explicit formulation this is marked reported. Where a precise formula is not printed in the paper but follows directly from its description this is marked inferred. ",
    "url": "/pages/exomiser_phenodigm.html#overview",
    
    "relUrl": "/pages/exomiser_phenodigm.html#overview"
  },"201": {
    "doc": "Exomiser phenodigm",
    "title": "Pairwise concept similarity",
    "content": "Two ontology concepts \\(p\\) and \\(q\\) are compared using OWLSim. The paper evaluates Jaccard, information content, and their combination, and selects the geometric mean for the final concept-level score. | Jaccard similarity (reported): | . \\[\\text{simJ}(p,q)=\\frac{\\lvert A(p)\\cap A(q)\\rvert}{\\lvert A(p)\\cup A(q)\\rvert}\\] where \\(A(\\cdot)\\) is the set of inferred attributes for a phenotype concept, typically including ancestors in the ontology. | Information content of the least common subsumer (reported): | . \\[\\text{IC}(c)=-\\log\\frac{N(c)}{N_{\\mathrm{all}}}\\] where \\(N(c)\\) is the number of annotations to concept \\(c\\) and \\(N_{\\mathrm{all}}\\) is the total number of annotations in the corpus. For a concept pair, OWLSim uses \\(c=\\text{LCS}(p,q)\\). | Concept-pair score as geometric mean (reported choice, inferred normalisation): | . \\[\\tilde{\\text{IC}}(p,q)=\\frac{\\text{IC}(\\text{LCS}(p,q))}{\\text{IC}_{\\max}}\\] \\[s(p,q)=\\sqrt{\\text{simJ}(p,q)\\cdot \\tilde{\\text{IC}}(p,q)}\\] The paper states that the geometric mean of IC and simJ is used and performs best, but does not print the exact rescaling step; the \\(\\text{IC}_{\\max}\\) normalisation above is a minimal assumption to place IC on \\([0,1]\\) before taking a geometric mean. ",
    "url": "/pages/exomiser_phenodigm.html#pairwise-concept-similarity",
    
    "relUrl": "/pages/exomiser_phenodigm.html#pairwise-concept-similarity"
  },"202": {
    "doc": "Exomiser phenodigm",
    "title": "Set to set similarity",
    "content": "Let disease \\(a\\) have concepts \\(P=\\{p_i\\}_{i=1}^{m}\\) and model \\(b\\) have concepts \\(Q=\\{q_j\\}_{j=1}^{n}\\). Define the best match for each disease term as \\(bsm_i=\\max_j s(p_i,q_j)\\). Two overall raw scores are then computed: . | Maximum match (reported): | . \\[\\text{maxScore}(a,b)=\\max_{1\\le i\\le m}\\ bsm_i\\] . | Mean best match (reported): | . \\[\\text{avgScore}(a,b)=\\frac{1}{m}\\sum_{i=1}^{m} bsm_i\\] ",
    "url": "/pages/exomiser_phenodigm.html#set-to-set-similarity",
    
    "relUrl": "/pages/exomiser_phenodigm.html#set-to-set-similarity"
  },"203": {
    "doc": "Exomiser phenodigm",
    "title": "Scaling to a percentage of a hypothetical perfect match",
    "content": "Raw scores are not on an absolute scale. PhenoDigm therefore expresses them relative to a best possible score for the disease by choosing, for each \\(p_i\\), the model concept that maximises the pairwise score. Let \\(\\mathcal{O}\\) be the ontology’s concept space. Define disease-specific upper bounds (inferred from description): . \\[\\text{maxScore}_{\\max}(a)=\\max_{1\\le i\\le m}\\ \\max_{q\\in\\mathcal{O}} s(p_i,q)\\] \\[\\text{avgScore}_{\\max}(a)=\\frac{1}{m}\\sum_{i=1}^{m}\\ \\max_{q\\in\\mathcal{O}} s(p_i,q)\\] Scale to percentages (**reported as step, formulas inferred): . \\[\\text{maxPct}(a,b)=100\\cdot\\frac{\\text{maxScore}(a,b)}{\\text{maxScore}_{\\max}(a)}\\] \\[\\text{avgPct}(a,b)=100\\cdot\\frac{\\text{avgScore}(a,b)}{\\text{avgScore}_{\\max}(a)}\\] ",
    "url": "/pages/exomiser_phenodigm.html#scaling-to-a-percentage-of-a-hypothetical-perfect-match",
    
    "relUrl": "/pages/exomiser_phenodigm.html#scaling-to-a-percentage-of-a-hypothetical-perfect-match"
  },"204": {
    "doc": "Exomiser phenodigm",
    "title": "Combined percentage score",
    "content": "PhenoDigm uses the average of the two percentage measures as the final score presented to users (reported choice, formula inferred): . \\[\\text{combinedPct}(a,b)=\\frac{\\text{maxPct}(a,b)+\\text{avgPct}(a,b)}{2}\\] This score is used to rank animal models for a disease and, when projected to genes, to rank genes by phenotype similarity. ",
    "url": "/pages/exomiser_phenodigm.html#combined-percentage-score",
    
    "relUrl": "/pages/exomiser_phenodigm.html#combined-percentage-score"
  },"205": {
    "doc": "Exomiser phenodigm",
    "title": "Relation to Exomiser’s phenotype score",
    "content": "Exomiser queries human disease annotations, mouse and zebrafish models, and protein interaction neighbours, then takes the best available semantic similarity for each candidate gene as its phenotype score. In practice, PhenoDigm’s concept- and set-level similarities are the basis of this value. HPO terms provided for a proband propagate up the ontology, so even a single term can yield non-exact but ontologically related matches. Mode of inheritance checks in Exomiser do not alter PhenoDigm’s similarity itself but can down-weight Exomiser’s gene phenotype score when an OMIM MOI conflict is detected. ",
    "url": "/pages/exomiser_phenodigm.html#relation-to-exomisers-phenotype-score",
    
    "relUrl": "/pages/exomiser_phenodigm.html#relation-to-exomisers-phenotype-score"
  },"206": {
    "doc": "Exomiser phenodigm",
    "title": "References",
    "content": "See exomiser for the use of this algorithm in context. Damian Smedley, Anika Oellrich, Sebastian Köhler, Barbara Ruef, Sanger Mouse Genetics Project, Monte Westerfield, Peter Robinson, Suzanna Lewis, Christopher Mungall, PhenoDigm: analyzing curated annotations to associate animal models with human diseases, Database, Volume 2013, 2013, bat025, https://doi.org/10.1093/database/bat025 . ",
    "url": "/pages/exomiser_phenodigm.html#references",
    
    "relUrl": "/pages/exomiser_phenodigm.html#references"
  },"207": {
    "doc": "Exomiser phenodigm",
    "title": "Exomiser phenodigm",
    "content": "Last update: 20250809 . ",
    "url": "/pages/exomiser_phenodigm.html",
    
    "relUrl": "/pages/exomiser_phenodigm.html"
  },"208": {
    "doc": "FASTP",
    "title": "FASTP",
    "content": "A tool designed to provide fast all-in-one preprocessing for FastQ files. It performs QC, checks adapters, trimming, filtering, splitting/merging, etc. This tool is developed in C++ with multithreading supported to afford high performance. | fastp.sh runs on every file in the raw data directory. | Outputs the same directory structure with processed .fq.gz data. | Checks for existing output before starting and therefore can run incrementally. | Prints qulity reports to .json and .html. | . ",
    "url": "/pages/fastp.html#fastp",
    
    "relUrl": "/pages/fastp.html#fastp"
  },"209": {
    "doc": "FASTP",
    "title": "Links",
    "content": ". | https://github.com/OpenGene/fastp | . ",
    "url": "/pages/fastp.html#links",
    
    "relUrl": "/pages/fastp.html#links"
  },"210": {
    "doc": "FASTP",
    "title": "FASTP",
    "content": "Last update: 20230727 . | FASTP . | Links | . | . ",
    "url": "/pages/fastp.html",
    
    "relUrl": "/pages/fastp.html"
  },"211": {
    "doc": "FASTQ format data",
    "title": "FASTQ format data",
    "content": "Summary . | Analysis pipelines must account for the run directory name since it is possible that &gt;1 file has the same filename and thus output may be overwritten. | WGS data from SMOC is produced currently with Novaseq6000. | h2030gc fastq file names: . | &lt;SAMPLE_ID&gt;_&lt;NGS_ID&gt;_&lt;POOL_ID&gt;_&lt;S#&gt;_&lt;LANE&gt;_&lt;R1|R2&gt;.fastq.gz | . | Illumina fastq header: . | @&lt;instrument&gt;:&lt;run number&gt;:&lt;flowcell ID&gt;:&lt;lane&gt;:&lt;tile&gt;:&lt;x-pos&gt;:&lt;y-pos&gt; &lt;read&gt;:&lt;is filtered&gt;:&lt;control number&gt;:&lt;sample number&gt; | For the Undetermined FASTQ files only, the sequence observed in the index read is written to the FASTQ header in place of the sample number. This information can be useful for troubleshooting demultiplexing. | . | . | Element | Requirements | Description |   |   |   | . | @ | @ | Each sequence identifier line starts with @ |   |   |   | . | | Characters allowed: a–z, A–Z, 0–9 and underscore | Instrument ID |   |   |   | . | | Numerical | Run number on instrument |   |   |   | . | | Characters allowed: a–z, A–Z, 0–9 |   |   |   |   | . | Flowcell ID | | Numerical | Lane number |   |   | . | | Numerical | Tile number | | Numerical | X coordinate of cluster | . | | Numerical | Y coordinate of cluster |   |   |   | . | | Numerical | Read number. 1 can be single read or Read 2 of paired-end |   |   |   | . | | Y or N | Y if the read is filtered (did not pass), N otherwise |   |   |   | . | | Numerical | 0 when none of the control bits are on, otherwise it is an even number. On HiSeq X systems, control specification is not performed and this number is always 0. |   |   |   | . | | Numerical | Sample number from sample sheet |   |   |   | . Details . WGS data from SMOC is produced currently with Novaseq6000. Files are returned in one directory based on the order and several run directories containing the fastq files. |--- order |--- run1 |- s1_ABC_123_S1_L001_R1.fastq.gz |- s1_ABC_123_S1_L001_R2.fastq.gz |--- run2 |--- run3 . File names are structured as follows: . &lt;SAMPLE_ID&gt;_&lt;NGS_ID&gt;_&lt;POOL_ID&gt;_&lt;S#&gt;_&lt;LANE&gt;_&lt;R1|R2&gt;.fastq.gz . where . | &lt;SAMPLE_ID&gt;: is the sample ID given in the original sample sheet. | &lt;NGS_ID&gt;: the identifier of the library preparation. Usually does not change unless a new sequencing library needs to be prepared. | &lt;POOL_ID&gt;: the identifier of the pool. Your samples have NA here, as they are not pooled. | * &lt;S#&gt;: ‘S’ followed by a number given by the sequencer. | &lt;LANE&gt;: flow cell lane | *&lt;R1|R2&gt;: reads R1 and R2 (for paired-end sequencing). | . In this way, a library sequenced several times to achieve coverage can have the same name if S# is the same (decided by the sequencer). The FASTQ files are in directories representing individual runs, for example 221031_A00485_0334_AHNFF5DSX3 is run 334, performed on 31/10/2022 on the Novaseq6000 (A00485) and flow cell AHNFF5DSX3. ",
    "url": "/pages/fastq.html#fastq-format-data",
    
    "relUrl": "/pages/fastq.html#fastq-format-data"
  },"212": {
    "doc": "FASTQ format data",
    "title": "Links",
    "content": ". | https://en.wikipedia.org/wiki/FASTQ_format | https://knowledge.illumina.com/software/general/software-general-reference_material-list/000002211 | https://help.basespace.illumina.com/files-used-by-basespace/fastq-files | . ",
    "url": "/pages/fastq.html#links",
    
    "relUrl": "/pages/fastq.html#links"
  },"213": {
    "doc": "FASTQ format data",
    "title": "FASTQ format data",
    "content": "Last update: 20230727 . | FASTQ format data . | Summary | Details | . | Links | . ",
    "url": "/pages/fastq.html",
    
    "relUrl": "/pages/fastq.html"
  },"214": {
    "doc": "Filter VCF with bcftools",
    "title": "Filtering VCF files using bcftools for gnomAD_AF INFO column",
    "content": "Last update: 20240826 . ",
    "url": "/pages/filter_vcf_bcftools.html#filtering-vcf-files-using-bcftools-for-gnomad_af-info-column",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#filtering-vcf-files-using-bcftools-for-gnomad_af-info-column"
  },"215": {
    "doc": "Filter VCF with bcftools",
    "title": "Background",
    "content": "After annotating VCF files with variant effect prediction tools like VEP (Variant Effect Predictor) or SnpEff, and databases like dbnsfp, additional information such as global allele frequencies from databases like gnomAD becomes available. One such key annotation is the “gnomAD_AF” field, representing the allele frequency across diverse populations. Accurate filtering on this field is crucial for various genetic studies, especially when prioritising variants with low population frequency for rare disease investigations. ",
    "url": "/pages/filter_vcf_bcftools.html#background",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#background"
  },"216": {
    "doc": "Filter VCF with bcftools",
    "title": "Check your tool version",
    "content": "The sciCORE HPC provides bcftools v1.11 with enable-modules. This version (pre-v1.12) contains a bug which will cause an error and fail to run filtering. | Current module limitation: Currently, the latest version of bcftools available with “enable-modules” on sciCORE is bcftools version 1.11, as of September 22, 2020. This version information is documented here. | Bug present before version 1.12: There is a notable bug in bcftools pre-v1.12 that prevents import for filtering. This bug can cause format fields to drop unexpectedly, leading to broken VCF records where the number of columns does not match the number of samples, as described in this example error: [E::bcf_write] Broken VCF record, the number of columns at chr1:817186 does not match the number of samples (0 vs 1) . | Bug Fix in Version 1.12: This bug was addressed and corrected in the subsequent release, bcftools v1.12, on November 20, 2020. For detailed information on the bug and its resolution, refer to the issue tracker on GitHub here. | . ",
    "url": "/pages/filter_vcf_bcftools.html#check-your-tool-version",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#check-your-tool-version"
  },"217": {
    "doc": "Filter VCF with bcftools",
    "title": "Recommended action: updating bcftools",
    "content": "To address the limitations of the outdated bcftools version in some environments and ensure that you have the correct version for accurate VCF file manipulation, follow these steps to set up a Conda environment specifically for bcftools with a version that includes the necessary bug fixes. Create and configure a new conda environment . Here’s a script that sets up a new Conda environment, installs bcftools, and verifies the installation. Save the script as bcftools_conda.sh and execute it in your terminal: . #!/bin/bash # Create a new Conda environment named bcftools mamba create -n bcftools -y conda init mamba activate bcftools mamba install -c bioconda bcftools -y conda install -c conda-forge gsl -y bcftools --version # mamba deactivate . ",
    "url": "/pages/filter_vcf_bcftools.html#recommended-action-updating-bcftools",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#recommended-action-updating-bcftools"
  },"218": {
    "doc": "Filter VCF with bcftools",
    "title": "Example of filtering",
    "content": "Filtering based on the “gnomAD_AF” involves extracting this subfield from the VEP-annotated VCF and applying a numeric threshold. For instance, to filter out all variants with a gnomAD allele frequency greater than 0.001 (to focus on rare variants), you can use the following command pattern with bcftools: . bcftools +split-vep -c gnomAD_AF:Float ${INPUT_DIR}/input_file.vcf.gz \\ -i \"gnomAD_AF&lt;0.001\" \\ | bgzip -c &gt; ${OUTPUT_DIR}/filtered_output.vcf.gz . ",
    "url": "/pages/filter_vcf_bcftools.html#example-of-filtering",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#example-of-filtering"
  },"219": {
    "doc": "Filter VCF with bcftools",
    "title": "Conclusion",
    "content": "When performing genetic analyses where population frequency data impacts the study outcome, ensuring the accuracy of the tools and their versions is as crucial as the biological data. Users must ensure that their computational environment is properly configured to handle these nuances, especially with frequently updated tools like bcftools. For more detailed instructions and updates on bcftools, refer to the official GitHub repository of bcftools. ",
    "url": "/pages/filter_vcf_bcftools.html#conclusion",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#conclusion"
  },"220": {
    "doc": "Filter VCF with bcftools",
    "title": "References",
    "content": ". | bcftools GitHub Releases | bcftools Issue Tracker for Bug Reports | dbnsfp Database | VEP - Variant Effect Predictor | SnpEff | . ",
    "url": "/pages/filter_vcf_bcftools.html#references",
    
    "relUrl": "/pages/filter_vcf_bcftools.html#references"
  },"221": {
    "doc": "Filter VCF with bcftools",
    "title": "Filter VCF with bcftools",
    "content": " ",
    "url": "/pages/filter_vcf_bcftools.html",
    
    "relUrl": "/pages/filter_vcf_bcftools.html"
  },"222": {
    "doc": "Financial management",
    "title": "Financial management system overview",
    "content": ". | Financial management system overview . | 🔁 Currency conversion to CHF | 📁 Ledger structure and data | 🧾 Core logic | 🧾 Account structure and treatment | 📊 Key financial indicators | 📌 Year-end reporting principles | . | . We maintain full transparency and auditability of our financials by storing structured raw data and programmatically generating our accounting records. Our financial system is built in R using a modern, developer-friendly workflow grounded in double-entry principles, asset classification, and reproducible reporting. This approach ensures clarity, precision, and trust in how company funds are spent and assets are tracked. 🔁 Currency conversion to CHF . All transactions are recorded in the original invoice currency and automatically converted to CHF during import. We first apply daily spot rates from the Oanda API, which provide high-resolution exchange rates for USD/CHF, EUR/CHF, and GBP/CHF, but are limited to the most recent 180 days. For any dates outside this window or where daily data is unavailable (e.g. weekends or holidays), we fallback to the Swiss National Bank’s published monthly average rates, joined by currency and transaction month. This layered approach ensures each entry receives a consistent, auditable CHF value derived from reliable and official data sources, without requiring any manual adjustments. 📁 Ledger structure and data . We store all financial data as TSV files, with clear, human-readable entries and linked PDF receipts. This forms the single source of truth for our internal and external audits. | File path | Description | . | ~/so/docs/finance/2024_finance.tsv | Raw transaction data for 2024 | . | ~/so/docs/finance/2025_finance.tsv | Raw transaction data for 2025 | . | receipt_file column | Path to receipt PDF | . | Example: openai_chatgpt/20240313_Receipt-2423-6370.pdf | March 2024 ChatGPT invoice | . Each row records: date, description, debit_account, credit_account, amount, currency, receipt_file, notes. All amounts are automatically converted to CHF using Oanda and SNB rates. 🧾 Core logic . We use a double-entry ledger model to track every transaction. Each entry records both a debit (what was received or incurred) and a credit (how it was paid), ensuring the signed sum across all accounts is always zero. Debit-side accounts represent the category of spend (e.g. equipment, services), while credit-side accounts record the payment method (e.g. debit card, equity). Ledger entries are categorised upfront, allowing direct reporting of asset acquisitions, operating costs, and capital expenditure—independent of how a purchase was paid. 🧾 Account structure and treatment . | Account | Type | Treatment | Example | . | office_equipment | Asset | Capitalised and depreciated monthly (3y) | MacBook Pro | . | software_services | Expense | Operating cost (P\\&amp;L) | OpenAI, GitHub | . | web_domain | Expense | P\\&amp;L item unless capitalised for multi-year use | GoDaddy renewal | . | debit_card | Liability | Cash outflow channel only | Company payments | . Depreciation is applied automatically each month using journal entries from office_equipment to accumulated_depreciation_office_equipment and depreciation_expense. 📊 Key financial indicators . Our system programmatically calculates and tracks core financial indicators from structured raw data. These metrics are regenerated whenever ledger files are updated, and exported as reproducible PDF outputs. Key metrics include: . | Account balances: Summed by account type (asset, liability, expense) with directional signs for clarity. | Cumulative cash outflow: Tracks total spend over time via company payment channels (e.g. debit card). | Depreciation schedule: Monthly amortisation of tangible assets over a fixed 3-year period. | Net book value: Tracks declining value of assets after depreciation. | Capital vs recurring spend: Distinguishes between long-term investments and operational outflows. | Burn rate: Monitors monthly spend rate across core cost centres. | Monthly P\\&amp;L: Aggregates revenue (if present) and expenses to compute net income. | Balance sheet summary: Current financial position summarised by asset, liability, and equity totals. | . All metrics are based on ledger-side logic and can be verified against raw TSV records and source receipts. Outputs are saved in images/finance_*.pdf and reflect the current state of the ledger without manual edits or reclassification. 📌 Year-end reporting principles . We report based on what was acquired or spent, not the method of payment. For example, if CHF 1507 was paid via debit card, it is not reported as “CHF 1507 spent on debit card” but rather attributed to its actual categories: equipment, software, domain, etc. Assets are tracked on the balance sheet and depreciated; expenses flow to the profit and loss statement. Payment methods are treated purely as outflow channels and are not used for categorisation. ",
    "url": "/pages/financial_management.html#financial-management-system-overview",
    
    "relUrl": "/pages/financial_management.html#financial-management-system-overview"
  },"223": {
    "doc": "Financial management",
    "title": "Financial management",
    "content": "Last update: 20250727 . ",
    "url": "/pages/financial_management.html",
    
    "relUrl": "/pages/financial_management.html"
  },"224": {
    "doc": "GATK BQSR",
    "title": "Base Quality Score Recalibration (BQSR)",
    "content": "Overview . The Base Quality Score Recalibration (BQSR) step in our pipeline is designed to adjust the base quality scores in BAM files based on known sites of variation and sequencing artifacts. This recalibration helps to mitigate biases that might have been introduced during sequencing. Implementation . The 06_bqsr.sh script manages the BQSR process using the Genome Analysis Toolkit (GATK). It operates within a high-throughput computational framework facilitated by the SLURM job scheduler, specifically configured for efficient handling of genomic data. Script Description . The script employs specific SLURM settings to optimize the use of computational resources: . | Nodes: 1 | Memory: 6G | CPUs per Task: 4 | Time Limit: 32:00:00 | Job Name: bqsr | Job Array: Processes batches of 1-8 samples simultaneously. | . The script initializes by setting up the necessary environment, loading computational modules, and defining input and output directories based on predefined variables. Tools Used . | GATK (v4.4.0.0): Used for its BaseRecalibrator and ApplyBQSR tools to recalibrate base quality scores based on multiple known sites databases. | . Process Flow . | File Handling: . | Locates BAM files previously processed for duplicate marking and merging. | Assigns each file to a specific SLURM array job based on its task ID. | . | Recalibration Execution: . | The BaseRecalibrator tool generates a recalibration table based on the input BAM, a reference genome, and known variant sites. | The ApplyBQSR tool then adjusts the base quality scores using the generated recalibration table, producing a recalibrated BAM file. | . | Known Sites: . | Multiple databases are used as references for known sites, including dbSNP and the Mills and 1000 Genomes gold standard indels, to ensure comprehensive coverage and accuracy in recalibration. | . | Output: . | Each sample results in a recalibrated BAM file and an accompanying BQSR table, stored in the designated output directory. | . | . Quality Assurance . This step includes detailed logging of each process, capturing standard output and errors to facilitate troubleshooting and ensure reproducibility. The script also features robust error handling mechanisms to address potential failures during the recalibration process. Conclusion . BQSR is a critical component of our variant discovery pipeline, enhancing the reliability of variant calls by refining the accuracy of base quality scores in sequenced data. By leveraging advanced tools and high-performance computing resources, this step ensures that subsequent analyses are based on the highest quality data. ",
    "url": "/pages/gatk_bsqr.html#base-quality-score-recalibration-bqsr",
    
    "relUrl": "/pages/gatk_bsqr.html#base-quality-score-recalibration-bqsr"
  },"225": {
    "doc": "GATK BQSR",
    "title": "GATK BQSR",
    "content": " ",
    "url": "/pages/gatk_bsqr.html",
    
    "relUrl": "/pages/gatk_bsqr.html"
  },"226": {
    "doc": "GATK Genomic db import",
    "title": "Genomics Database Import",
    "content": "Overview . The Genomics Database Import step is crucial for compiling variant data from multiple samples into a unified database, enhancing the efficiency of downstream variant analysis processes such as joint genotyping. Implementation . The script 08_genomics_db_import.sh is designed to handle the consolidation of genomic variant call files (gVCFs) into a GenomicsDB workspace using GATK’s GenomicsDBImport tool. This script is configured to process the data using SLURM job arrays, enabling parallel processing of genomic data. | Number of jobs: This array is chromosome based and merges all samples into a joint cohort. This array should not be more than 25 (chromosome, not subjects) | Time: For 180 WGS this is taking ~12 hours for all chromosomes in parallel. | . Script Description . The script is optimized for high-throughput computational requirements: . | Nodes: 1 | Memory: 30G | CPUs per Task: 2 | Time Limit: 72:00:00 | Job Name: genomics_db_import | Job Array: Supports handling 25 chromosome sets in a batch processing manner. | . The script begins by establishing the environment, sourcing necessary variables, and setting up directories for input and output data. Caveats . | IMPORTANT: The -Xmx value the tool is run with should be less than the total amount of physical memory available by at least a few GB, as the native TileDB library requires additional memory on top of the Java memory. Failure to leave enough memory for the native code can result in confusing error messages! | At least one interval must be provided | Input GVCFs cannot contain multiple entries for a single genomic position | The –genomicsdb-workspace-path must point to a non-existent or empty directory. | GenomicsDBImport uses temporary disk storage during import. The amount of temporary disk storage required can exceed the space available, especially when specifying a large number of intervals. The command line argument --tmp-dir can be used to specify an alternate temporary storage location with sufficient space.. | . Tools Used . | GATK (v4.4.0.0): Used for its GenomicsDBImport tool, which allows for the efficient aggregation of gVCF files into a single database that can be queried and analyzed more efficiently. | . Process Flow . | Input Preparation: . | Identifies all gVCF files from the Haplotype Calling step. | Constructs a command to include all these files in the GenomicsDB workspace. | . | Database Creation: . | For each chromosome or chromosome set specified by the job array, a separate GenomicsDB workspace is created. | The process is customized to include optimizations for shared POSIX filesystems, which improves performance on distributed computing systems. | . | Execution Details: . | The script uses dynamic allocation of memory and CPU resources to handle the intense demands of processing large genomic datasets. | Outputs include a GenomicsDB workspace for each chromosome, facilitating rapid access and manipulation in subsequent analytical steps. | . | . Quality Assurance . Robust logging, detailed output management, and stringent error handling are implemented to ensure the reliability and reproducibility of the Genomics Database Import process. Conclusion . The consolidation of gVCF files into a GenomicsDB workspace is a pivotal step in our pipeline. It not only optimizes the storage and querying of genomic data but also sets the stage for efficient joint genotyping and variant analysis across multiple samples. This process leverages advanced computational tools and techniques to handle the complexities of large-scale genomic datasets effectively. ",
    "url": "/pages/gatk_dbimport.html#genomics-database-import",
    
    "relUrl": "/pages/gatk_dbimport.html#genomics-database-import"
  },"227": {
    "doc": "GATK Genomic db import",
    "title": "GATK Genomic db import",
    "content": " ",
    "url": "/pages/gatk_dbimport.html",
    
    "relUrl": "/pages/gatk_dbimport.html"
  },"228": {
    "doc": "GATK Duplicates",
    "title": "Duplicate Marking and Merging",
    "content": "Overview . The DNA Germline Short Variant Discovery pipeline includes a crucial step of marking duplicates and merging BAM files. This process is essential for ensuring the accuracy of variant calling by eliminating potential biases introduced by duplicate reads resulting from sequencing artifacts. Implementation . The rmdup_merge.sh script orchestrates this process using high-throughput computational resources managed by the SLURM job scheduler. The script is tailored for operation on a distributed computing environment and is designed to handle large-scale datasets efficiently. Script Description . The script is executed with the following SLURM settings: . | Nodes: 1 | Memory: 22G | Tasks: 16 | CPUs per Task: 1 | Time Limit: 32:00:00 | Job Name: rmdup_merge | Array Jobs: Supports batch processing for 1-140 samples, allowing parallel processing of multiple samples. | . The script starts by setting environment variables and loading necessary modules such as Java and GATK. It then reads from a predefined list of BAM files and processes each according to its SLURM array task ID. Tools Used . | GATK (v4.4.0.0): Utilized for its MarkDuplicatesSpark tool which identifies and marks duplicate reads in BAM files. This tool is chosen for its ability to handle large datasets and its integration with Apache Spark for improved performance. | . Process Flow . | Input and Output Directories: Specified through environment variables. | File Preparation: Reads from a list containing paths to BAM files and maps them to their respective subject IDs. | Execution: . | For each subject (or sample), the script merges and marks duplicates across multiple BAM files using MarkDuplicatesSpark. | It utilizes Spark to perform operations locally but is configured to work under the computational limits specified by SLURM job parameters. | Outputs are directed to specified output directories, and each merged BAM file is named after the subject ID. | . | Post-Processing: . | Generation of metrics files for each processed BAM to evaluate the quality of the merging and marking steps. | Cleanup operations include removing temporary directories created during the job execution to conserve storage space. | . | . Quality Assurance . This script incorporates error handling and robust logging mechanisms to ensure that each step of the process is recorded for audit and troubleshooting purposes. Output and error logs are saved to specified directories, allowing easy access to detailed run-time logs and error messages. Conclusion . The duplicate marking and merging step is pivotal for preparing sequenced data for subsequent analysis phases like variant calling. By integrating robust tools and leveraging high-performance computing resources, our pipeline ensures that data processed through this stage is of the highest fidelity, setting a strong foundation for accurate and reliable variant discovery. ",
    "url": "/pages/gatk_duplicates.html#duplicate-marking-and-merging",
    
    "relUrl": "/pages/gatk_duplicates.html#duplicate-marking-and-merging"
  },"229": {
    "doc": "GATK Duplicates",
    "title": "GATK Duplicates",
    "content": " ",
    "url": "/pages/gatk_duplicates.html",
    
    "relUrl": "/pages/gatk_duplicates.html"
  },"230": {
    "doc": "GATK Genotyping gVCFs",
    "title": "Genotyping gVCFs",
    "content": "Overview . Following the import of genomic variant call format (gVCF) files into a GenomicsDB workspace, the next stage in our pipeline involves genotyping these consolidated gVCFs. This step is crucial for calling variants across multiple samples simultaneously, which enhances the discovery and accuracy of genetic variants. Implementation . The 09_genotype_gvcf.sh script manages the genotyping of variants from the GenomicsDB workspaces. This process uses the GATK’s GenotypeGVCFs tool, specifically tailored to handle large genomic datasets with high computational efficiency. Script Description . Configured for intensive computational tasks: . | Dependency: Waits for previous jobs to complete, ensuring that all necessary data is available for genotyping. | Nodes: 1 | Memory: 30G | CPUs per Task: 2 | Time Limit: 96:00:00 | Job Name: genotype_gvcf | Job Array: Capable of processing 25 chromosomal segments in one batch. | . The script starts by setting up the required environment, sourcing variables, and preparing input and output directories. Tools Used . | GATK (v4.4.0.0): Utilized for its GenotypeGVCFs tool, which is designed to perform the final genotyping step on the aggregated gVCF data stored in a GenomicsDB workspace. | . Process Flow . | Input and Output Setup: . | Directories are established based on predefined paths, where GenomicsDB workspaces are the input and the output is specified as genomic VCF files. | . | Execution of Genotyping: . | For each job in the array, corresponding to a specific chromosome or chromosomal segment, the script accesses the appropriate GenomicsDB workspace. | The GenotypeGVCFs command is executed to produce a gVCF file for each chromosome, containing the genotyped variants. | . | Optimization and Resource Management: . | The Java options are configured to optimize memory usage and parallel processing capabilities to manage the large data volumes typically involved in genomic analysis. | . | . Quality Assurance . This stage includes comprehensive logging and error tracking to ensure the process is executed correctly and efficiently. Each step’s outputs are systematically verified to maintain high data integrity and reproducibility. Conclusion . Genotyping of gVCFs is an essential process in our DNA Germline Short Variant Discovery pipeline, enabling the detailed analysis of genetic variations across multiple samples. By leveraging high-performance computing resources and sophisticated bioinformatics tools, this step ensures that our pipeline produces accurate and reliable variant calls. ",
    "url": "/pages/gatk_genotypegvcf.html#genotyping-gvcfs",
    
    "relUrl": "/pages/gatk_genotypegvcf.html#genotyping-gvcfs"
  },"231": {
    "doc": "GATK Genotyping gVCFs",
    "title": "GATK Genotyping gVCFs",
    "content": " ",
    "url": "/pages/gatk_genotypegvcf.html",
    
    "relUrl": "/pages/gatk_genotypegvcf.html"
  },"232": {
    "doc": "GATK Genotype refine",
    "title": "Genotype Refinement",
    "content": "Overview . Genotype refinement is a critical process in our pipeline aimed at enhancing the reliability of genotype calls by utilizing additional population data. This step applies statistical methods to refine genotype probabilities and filter variants based on genotype quality (GQ) scores. Implementation . The 11_genotype_refinement.sh script uses tools from the Genome Analysis Toolkit (GATK) to refine genotypes based on a model that incorporates population-wide allele frequencies. The script is set up to process data efficiently across multiple chromosomes. Script Description . Configured for substantial computational tasks: . | Nodes: 1 | Memory: 30G | CPUs per Task: 4 | Time Limit: 96:00:00 | Job Name: genotype_refine | Job Array: Designed to handle multiple chromosomal segments in one pass. | . The script initiates by setting up the required computational environment, loading modules, and preparing directories for input and output. Tools Used . | GATK (v4.4.0.0): Utilizes CalculateGenotypePosteriors for refining genotype probabilities and VariantFiltration to apply quality filters to genotypes based on predefined thresholds. | . Process Flow . | Input Preparation: . | The script retrieves recalibrated VCF files from the VQSR step and checks their existence before proceeding. | . | Refinement Execution: . | CalculateGenotypePosteriors: This tool is used to adjust genotype likelihoods based on allele frequency data from large reference populations (e.g., gnomAD). | VariantFiltration: Applies a filter to flag genotypes with a GQ score less than 20, helping to ensure that only high-confidence genotypes are used in subsequent analyses. | . | Outputs: . | Generates two sets of VCF files: . | A refined VCF file with updated genotype likelihoods. | A filtered VCF file that excludes genotypes below a quality threshold, ensuring the dataset’s integrity for further analysis. | . | . | . Quality Assurance . The process includes comprehensive logging and condition checks to ensure accuracy and efficiency. Error handling mechanisms are in place to address any issues during file handling or processing, enhancing the robustness of the pipeline. Conclusion . Genotype refinement is an essential step to ensure the high quality and reliability of variant calls in genomic studies. By integrating additional genomic data and applying rigorous quality control measures, this step helps in producing highly accurate genetic analyses. ",
    "url": "/pages/gatk_genotyperefine.html#genotype-refinement",
    
    "relUrl": "/pages/gatk_genotyperefine.html#genotype-refinement"
  },"233": {
    "doc": "GATK Genotype refine",
    "title": "GATK Genotype refine",
    "content": " ",
    "url": "/pages/gatk_genotyperefine.html",
    
    "relUrl": "/pages/gatk_genotyperefine.html"
  },"234": {
    "doc": "GATK Haplotype caller",
    "title": "Haplotype Calling",
    "content": "Overview . Haplotype calling is a critical step in our pipeline, involving the identification of variants from sequenced DNA by constructing haplotypes. This step uses the Genome Analysis Toolkit’s (GATK) HaplotypeCaller, which allows for calling high-confidence variants. Implementation . The script 07_haplotype_caller.sh is designed to manage this intensive computational task effectively, utilizing SLURM for job scheduling to handle potentially large genomic datasets. Script Description . Configured to maximize efficiency given computational constraints: . | Nodes: 1 | Memory: 4G | CPUs per Task: 2 | Time Limit: 48:00:00 | Job Name: hc | Job Array: Capable of handling 1-68 samples concurrently. | . The script begins by setting up the necessary computing environment, sourcing variables, and preparing input and output directories. Tools Used . | GATK (v4.4.0.0): Utilized for the HaplotypeCaller tool, which is executed under specific java options to manage memory usage and parallel processing capabilities. | . Process Flow . | File Preparation: . | Finds recalibrated BAM files from the previous BQSR step. | Each BAM file is assigned to a SLURM job based on its array task ID. | . | Variant Calling Execution: . | HaplotypeCaller runs with parameters to produce a genomic VCF (gVCF) for each sample, which includes variant calls along with confidence scores. | Outputs are genomic VCF files named after each sample, stored in the designated output directory. | . | Optimization and Debugging: . | Detailed logging of the script’s execution, including start and end times, input and output details, and memory settings, helps in troubleshooting and ensuring reproducibility. | . | . Quality Assurance . This stage of the pipeline includes detailed logging and error checking to ensure that the haplotype calling process is robust against computational failures and produces reliable results. Conclusion . The Haplotype Calling step is pivotal for identifying variants accurately, setting the stage for subsequent processes such as variant annotation and interpretation. The use of high-performance computing resources ensures that our pipeline can handle large datasets efficiently and reliably. ",
    "url": "/pages/gatk_hc.html#haplotype-calling",
    
    "relUrl": "/pages/gatk_hc.html#haplotype-calling"
  },"235": {
    "doc": "GATK Haplotype caller",
    "title": "GATK Haplotype caller",
    "content": " ",
    "url": "/pages/gatk_hc.html",
    
    "relUrl": "/pages/gatk_hc.html"
  },"236": {
    "doc": "GATK VQSR",
    "title": "Variant Quality Score Recalibration (VQSR)",
    "content": "Overview . Variant Quality Score Recalibration (VQSR) is an advanced technique used in our pipeline to assess and recalibrate the confidence scores of identified variants. By using known, reliable sources as benchmarks, VQSR improves the accuracy of variant calls, distinguishing true biological variants from technical artifacts. VQSR in GATK is a sophisticated approach used to assess variant quality and filter out sequencing and data processing artifacts. This two-step procedure begins with building a recalibration model that evaluates the relationship between variant annotations, such as Quality by Depth (QD), Mapping Quality (MQ), and Read Position Rank Sum Test (ReadPosRankSum), against the likelihood that a variant is genuine. Key resources like HapMap and Omni 2.5M SNP chip array data are leveraged to train this model adaptively. By employing an adaptive error model and a Gaussian mixture model, VQSR provides a score known as VQSLOD (Variant Quality Score Log Odds), which indicates the likelihood of each variant being true. The VQSLOD scores are incorporated into the variant call file (VCF), allowing for highly precise and flexible filtering. The overall aim of VQSR is to allocate a calibrated probability to each variant, surpassing traditional methods that rely strictly on fixed annotation value thresholds. This technique is especially crucial in contexts where accuracy in variant filtering significantly impacts subsequent analyses and conclusions, particularly in medical genomics research. Implementation . The 10_vqsr.sh script executes the VQSR process using GATK’s VariantRecalibrator and ApplyVQSR tools. This script is configured for high-demand computational tasks to handle variant data across all chromosome segments. Script Description . Optimized for high-throughput: . | Nodes: 1 | Memory: 30G | CPUs per Task: 4 | Time Limit: 96:00:00 | Job Name: vqsr | Job Array: Capable of processing 25 chromosome segments in a single run. | . The script begins by preparing the environment, loading necessary modules, and setting up directories. Tools Used . | GATK (v4.4.0.0): Employed for both the VariantRecalibrator and ApplyVQSR tools, which together adjust the quality scores of variants based on their statistical likelihood of being true genetic variants. | . Process Flow . | Input Setup: . | Retrieves gVCF files for each chromosome from the previous genotyping step. | . | Execution of VQSR: . | For SNPs and INDELs, separate recalibration and application steps are performed: . | VariantRecalibrator: Generates recalibration models based on a set of user-defined annotations and known variant sites. | ApplyVQSR: Applies the recalibration model to the variant calls, filtering or adjusting their quality scores based on the recalibration. | . | . | Known Sites and Resources: . | Utilizes multiple databases such as HapMap, Omni, 1000 Genomes, and Mills for recalibration, which help in training the recalibration model to distinguish between high-confidence and low-confidence variants. | . | Output Generation: . | Produces recalibrated VCF files for SNPs and INDELs, which are stored in specified output directories. | . | . Quality Assurance . Includes extensive logging and monitoring of the recalibration process to ensure accuracy and efficiency. Error handling and checkpointing ensure that the process can resume from intermediate stages in case of failures. Conclusion . VQSR is essential for our DNA Germline Short Variant Discovery pipeline as it refines variant calls, enhancing the reliability and accuracy of downstream genetic analyses. This process leverages comprehensive known variant datasets and robust computational resources to maintain high standards of variant calling. ",
    "url": "/pages/gatk_vqsr.html#variant-quality-score-recalibration-vqsr",
    
    "relUrl": "/pages/gatk_vqsr.html#variant-quality-score-recalibration-vqsr"
  },"237": {
    "doc": "GATK VQSR",
    "title": "GATK VQSR",
    "content": " ",
    "url": "/pages/gatk_vqsr.html",
    
    "relUrl": "/pages/gatk_vqsr.html"
  },"238": {
    "doc": "Guru variant interpretation",
    "title": "Guru variant interpretation",
    "content": "Guru is an R package designed to facilitate the interpretation of genetic determinants of disease in genomic data. It employs extensive annotation and filtering based on the ACMG (American College of Medical Genetics and Genomics) and AMP (Association for Molecular Pathology) guideline standards. ",
    "url": "/pages/guru.html#guru-variant-interpretation",
    
    "relUrl": "/pages/guru.html#guru-variant-interpretation"
  },"239": {
    "doc": "Guru variant interpretation",
    "title": "Features",
    "content": ". | Automatic Annotation and Filtering: Applies extensive annotation and filtering based on ACMG/AMP guideline standards. | Customisation Options: Provides various settings for filtering and annotation criteria to suit different study requirements. | Comprehensive Visualization Tools: Generates a range of plots and visual aids to help interpret the filtering and annotation results effectively. | . ",
    "url": "/pages/guru.html#features",
    
    "relUrl": "/pages/guru.html#features"
  },"240": {
    "doc": "Guru variant interpretation",
    "title": "Installation",
    "content": "# To install the latest development version from GitHub: devtools::install_github(\"DylanLawless/Guru\") . ",
    "url": "/pages/guru.html#installation",
    
    "relUrl": "/pages/guru.html#installation"
  },"241": {
    "doc": "Guru variant interpretation",
    "title": "Usage",
    "content": "Here is a basic guide on how to use Guru to process genomic data: . library(Guru) # Load your genomic data genomic_data &lt;- guru_read(\"path_to_data/cohort.vcf.gz\") # Apply Guru annotation and filtering result &lt;- Guru::acmg_filter(genomic_data) # Visualize results Guru::plot_result(result) . ",
    "url": "/pages/guru.html#usage",
    
    "relUrl": "/pages/guru.html#usage"
  },"242": {
    "doc": "Guru variant interpretation",
    "title": "Detailed Description",
    "content": "Variant Interpretation . Guru Variant Classification . | Gene Annotation: Utilizes custom and default VEP plugins for comprehensive gene annotation. | Scoring and Interpretation: Implements scoring based on ACMG/AMP guidelines to evaluate annotation evidence. | Visualization: Includes plots showing the distribution of variants within genes and pathways, and other metrics. | . Guru Gene-Illustrate . | Data Integration: Leverages UniProt data to provide detailed insights into gene and protein structures. | Visual Representation: Generates vertical bars at amino acid positions to illustrate significant variant impacts visually. | . Guru uniprotR . | Seamless Data Retrieval: Automatically fetches data from UniProt using the UniprotR package to enrich the annotation process. | . Guru Get-Discussion . | Reporting: Produces a CSV or TSV format table summarizing the genetic analysis, providing context around technical descriptions. | . AutoDestructR . | Structure Visualisation: Deconstructs sets of PDB structures for detailed structural analysis and visualisation. | . ",
    "url": "/pages/guru.html#detailed-description",
    
    "relUrl": "/pages/guru.html#detailed-description"
  },"243": {
    "doc": "Guru variant interpretation",
    "title": "Documentation",
    "content": "For more detailed information on using Guru, please refer to https://github.com/DylanLawless/ACMGuru or the reference manual. ",
    "url": "/pages/guru.html#documentation",
    
    "relUrl": "/pages/guru.html#documentation"
  },"244": {
    "doc": "Guru variant interpretation",
    "title": "Contributing",
    "content": "Contributions are welcome! Please see the Contributing Guide for more details. ",
    "url": "/pages/guru.html#contributing",
    
    "relUrl": "/pages/guru.html#contributing"
  },"245": {
    "doc": "Guru variant interpretation",
    "title": "Guru variant interpretation",
    "content": "Last update: 20240827 . | Guru variant interpretation . | Features | Installation | Usage | Detailed Description . | Variant Interpretation . | Guru Variant Classification | Guru Gene-Illustrate | Guru uniprotR | Guru Get-Discussion | AutoDestructR | . | . | Documentation | Contributing | . | . ",
    "url": "/pages/guru.html",
    
    "relUrl": "/pages/guru.html"
  },"246": {
    "doc": "GWAS analysis",
    "title": "GWAS analysis",
    "content": "Last update: 20240929 . | GWAS analysis . | Overview | Data Preparation . | VCF to PLINK Conversion (plink_1_vcf_to_plink.sh) | Covariate and Phenotype Preparation (plink_2_covar.sh) | . | Statistical Analysis . | Association Testing (plink_3_assoc.sh) | . | Visualisation of Results . | Plotting Results (plink_4_plot.sh and plink_4_plot.R) | . | Interpreting GWAS results (plink --assoc output columns) | Significant association interpretation | Conclusion | . | . ",
    "url": "/pages/gwas.html",
    
    "relUrl": "/pages/gwas.html"
  },"247": {
    "doc": "GWAS analysis",
    "title": "Overview",
    "content": "Genome-Wide Association Studies (GWAS) are research approaches used to associate specific genetic variations with particular diseases. By analysing genetic variants across multiple samples, GWAS aim to identify genetic markers linked to disease traits. This process involves several stages, from data preparation to statistical analysis and visual representation of results. ",
    "url": "/pages/gwas.html#overview",
    
    "relUrl": "/pages/gwas.html#overview"
  },"248": {
    "doc": "GWAS analysis",
    "title": "Data Preparation",
    "content": "VCF to PLINK Conversion (plink_1_vcf_to_plink.sh) . This initial script converts joint genotyped cohort VCF files, which are split per chromosome, into a more manageable format using PLINK. The VCF files are decompressed, and the data is converted into binary format (BED, BIM, FAM files), which is suitable for fast processing in subsequent analysis steps. Covariate and Phenotype Preparation (plink_2_covar.sh) . This script prepares phenotype information and covariates, which are crucial for adjusting the GWAS analysis to prevent confounding results. It involves generating phenotype files from structured data sources and integrating them with genetic data files (FAM files). Additionally, Principal Component Analysis (PCA) is run on the genotype data to correct for population stratification, which is a critical step to ensure that genetic associations are not due to population structure differences. PCA: The population structure of genetic data can be controlled in GWAS by applying PCA. To simulate synthetic data or to understand the poplutation in cohort data we could compare it to population of classified ancestries such as the 1000 genomes project. In our exploration stages we produce the PCA bi-plot for 1000 Genomes Phase III - Version 2 (https://www.biostars.org/p/335605/) merged with our study data. Figure 1. PCA from 1000 genomes project data by Kevin Blighe via biostars.org. ",
    "url": "/pages/gwas.html#data-preparation",
    
    "relUrl": "/pages/gwas.html#data-preparation"
  },"249": {
    "doc": "GWAS analysis",
    "title": "Statistical Analysis",
    "content": "Association Testing (plink_3_assoc.sh) . Once data preparation is complete, this step involves cleaning the data further, including filtering by genotype frequency. The association testing is then carried out using PLINK, which tests for correlations between each genetic variant and the trait of interest while adjusting for covariates and significant principal components from the PCA. The results are consolidated into association files (.assoc) for each chromosome. ",
    "url": "/pages/gwas.html#statistical-analysis",
    
    "relUrl": "/pages/gwas.html#statistical-analysis"
  },"250": {
    "doc": "GWAS analysis",
    "title": "Visualisation of Results",
    "content": "Plotting Results (plink_4_plot.sh and plink_4_plot.R) . The final step involves visualising the results of the GWAS. The plink_4_plot.sh script calls an R script to generate a Manhattan plot and a QQ plot. These plots are essential for interpreting GWAS results: . | Manhattan Plot: This plot visualises the -log10(p-values) of the association tests across all chromosomes, highlighting genomic regions that surpass the genome-wide significance threshold. | QQ Plot: This plot helps assess whether the p-values conform to the expected distribution under the null hypothesis of no association, which helps in identifying potential issues like population stratification, cryptic relatedness, or differential genotyping quality. | . The output from the plink --assoc command in PLINK, when used for a case/control analysis incorporating covariates like PCA and disease outcomes, results in a file typically containing several columns. Each column header represents a specific data type or statistical measure relevant to genetic association testing. ",
    "url": "/pages/gwas.html#visualisation-of-results",
    
    "relUrl": "/pages/gwas.html#visualisation-of-results"
  },"251": {
    "doc": "GWAS analysis",
    "title": "Interpreting GWAS results (plink --assoc output columns)",
    "content": "Example results of a GWAS looks like this: output_assoc_autosomalgenome.assoc . | CHR | SNP | BP | A1 | F_A | F_U | A2 | CHISQ | P | OR | . | 1 | . | 17385 | A | 0.2838 | 0.3077 | G | 0.1182 | 0.731 | 0.8915 | . | 1 | . | 17406 | T | 0.01351 | 0 | C | 1.44 | 0.2301 | NA | . | 1 | . | 17407 | A | 0.08333 | 0.07843 | G | 0.01371 | 0.9068 | 1.068 | . | 1 | . | 17408 | G | 0 | 0.03846 | C | 2.912 | 0.08795 | 0 | . | 1 | . | 17452 | T | 0.01471 | 0.009615 | C | 0.09271 | 0.7608 | 1.537 | . | CHR: Chromosome number. This indicates the chromosome on which the single nucleotide polymorphism (SNP) is located. For instance, all entries shown in the example are located on chromosome 1. | SNP: SNP identifier. In our data, we tend to avoid use of the SNP ID since they are often redundant and introduce errors and therefore appears as a period (.), indicating that no specific SNP ID is assigned or available in the dataset. | BP: Base pair position. This numeric value represents the position of the SNP on the chromosome. | A1: The minor allele. In genetic association studies, this is the allele tested to determine if it has a significant association with the trait or disease. It is typically the allele of interest and less frequent in the population. | F_A: Frequency of the minor allele in affected individuals (cases). This percentage shows how common the minor allele is among participants with the disease. | F_U: Frequency of the minor allele in unaffected individuals (controls). This shows the prevalence of the minor allele in the control group. | A2: The major allele. This is the other allele for the SNP, generally more common than the minor allele. | CHISQ: Chi-square statistic. This value results from the chi-square test, which assesses whether there is a significant association between the genetic variant and the trait. It compares the observed counts of alleles between cases and controls to expected counts under no association. | P: P-value. This value indicates the probability of observing the test statistic as extreme as, or more extreme than, the value obtained if the null hypothesis (of no association) is true. Lower p-values suggest stronger evidence against the null hypothesis, indicating a potential association between the SNP and the trait. | OR: Odds ratio. This statistic represents the odds of the trait occurring (in this case, the disease) with the minor allele (A1) relative to the odds of the disease occurring with the major allele (A2). An OR less than 1 suggests a protective effect of the minor allele, whereas an OR greater than 1 suggests a risk effect. A value of NA or 0 can occur when the allele does not appear in one of the groups, making calculation of the odds ratio impossible or indefinite. | . Figure 2. Gif showing the qqman package for illustrating GWAS p-value results. Further invstigation of the association regions require others tools which are specific to your dataset. Commonly used tools including http://locuszoom.org and the Genotype-Tissue Expression (GTEx) Portal is a comprehensive public resource for researchers studying tissue and cell-specific gene expression and regulation (https://www.gtexportal.org/home/). ",
    "url": "/pages/gwas.html#interpreting-gwas-results-plink---assoc-output-columns",
    
    "relUrl": "/pages/gwas.html#interpreting-gwas-results-plink---assoc-output-columns"
  },"252": {
    "doc": "GWAS analysis",
    "title": "Significant association interpretation",
    "content": ". | For the SNP at BP 17385 on chromosome 1, the minor allele A has a frequency of 0.2838 in cases and 0.3077 in controls. The chi-square statistic is 0.1182 with a p-value of 0.731, suggesting no significant association. The odds ratio of 0.8915 indicates a slight protective effect, though not statistically significant. | Bonferroni Correction: To address the issue of multiple comparisons in GWAS, where thousands or millions of SNPs are tested for association with a trait, the Bonferroni correction is commonly applied. This method adjusts the p-value threshold to reduce the likelihood of false positives. It sets a more stringent significance level by dividing the conventional p-value threshold (usually 0.05) by the number of independent tests conducted. For instance, if 1 million SNPs are tested, the Bonferroni-corrected p-value threshold would be (0.05 / 1,000,000 = 0.00000005). This stringent criterion ensures that only associations with very strong statistical evidence are considered significant, thus controlling the family-wise error rate in large-scale testing scenarios. | Example threshold If the number of SNPs tested is 13’200’100, then our significant threshold can be set as psig &lt;- 0.05/13200100 = 3.79e-09. | Causal Variants and GWAS Using WGS Data: In a classic GWAS, significant p-values often identify SNPs that are not necessarily causal but are in linkage disequilibrium (LD) with the causal variant, as genotyping typically determines haplotype blocks rather than individual variants. This means the actual causal variant could lie anywhere within or even outside these blocks due to factors like quantitative trait loci (QTL) influencing the trait indirectly. However, in GWAS utilising whole genome sequencing (WGS) data, every variant in the genome is sequenced, ensuring the presence of the causal variant within the dataset. Despite this, LD can still result in a nearby benign and common variant showing a stronger association due to its higher frequency, potentially overshadowing the true causal variant. This complexity underscores the importance of comprehensive analysis and interpretation in the context of genetic association studies. | . ",
    "url": "/pages/gwas.html#significant-association-interpretation",
    
    "relUrl": "/pages/gwas.html#significant-association-interpretation"
  },"253": {
    "doc": "GWAS analysis",
    "title": "Conclusion",
    "content": "Through these stages, the GWAS pipeline efficiently processes genetic data to identify potential associations with traits. This pipeline utilises high-throughput computational tools and statistical methods to handle and analyse large-scale genetic data, ensuring robust and reliable results in genetic research. This documentation provides a clear understanding of each component’s purpose and function within the broader GWAS context, guiding users through the necessary steps and procedures. ",
    "url": "/pages/gwas.html#conclusion",
    
    "relUrl": "/pages/gwas.html#conclusion"
  },"254": {
    "doc": "HPC infrastructure",
    "title": "Hardware",
    "content": "Example comparisons . sciCORE https://scicore.ch/using-scicore/hpc-resources/ . Cluster BioMedIT . | Total nodes 15 | Total cores 976 total, 200 user | Total RAM 6 TB | Total GPUs 8 | Inter-connect Eth 100G | Total Disk 900 TB | . Cluster sciCORE . | Total nodes 215 | Total cores 13632 | Total RAM 73 TB | Total GPUs 80 | Inter-connect Eth 100G, Infiniband | Total Disk 11 PB | . EPFL https://www.epfl.ch/research/facilities/scitas/jed/ . | Peak performance Rpeak: 2’322 TFLOPs | Total RAM: 233,5 TB | Storage: 350 TB | The cluster is composed of 419 compute nodes, each with . | 2 Intel(R) Xeon(R) Platinum 8360Y processors running at 2.4 GHz, with 36 cores each (72 cores per machine), | 3 TB of SSD disk | . | for a total of 30’240 cores (including the frontend node) | 375 nodes have 512 GB of RAM, | 42 nodes have 1 TB of RAM, | 2 nodes have 2 TB of RAM . | approx 250 TB total | . | . ",
    "url": "/pages/hpc.html#hardware",
    
    "relUrl": "/pages/hpc.html#hardware"
  },"255": {
    "doc": "HPC infrastructure",
    "title": "HPC documentation",
    "content": "sciCORE uses SLURM workload manager https://slurm.schedmd.com/overview.html . Examples of documentation on simimlar infrastructure. | NIH Biowulf https://hpc.nih.gov | For the sciCORE cluster: https://wiki.biozentrum.unibas.ch/display/scicore/sciCORE+user+guide | For using SLURM: https://wiki.biozentrum.unibas.ch/display/scicore/SLURM+user+guide | EPFL SCITAS: https://www.epfl.ch/research/facilities/scitas/documentation/ | . ",
    "url": "/pages/hpc.html#hpc-documentation",
    
    "relUrl": "/pages/hpc.html#hpc-documentation"
  },"256": {
    "doc": "HPC infrastructure",
    "title": "Acknowledgements",
    "content": ". | sciCORE: “Calculations were performed at sciCORE (http://scicore.unibas.ch/) scientific computing core facility at University of Basel.” | sciCORE/SIB: “Calculations were performed at sciCORE (http://scicore.unibas.ch/) scientific computing core facility at University of Basel, with support by the SIB Swiss Institute of Bioinformatics.” | . ",
    "url": "/pages/hpc.html#acknowledgements",
    
    "relUrl": "/pages/hpc.html#acknowledgements"
  },"257": {
    "doc": "HPC infrastructure",
    "title": "Data stream",
    "content": "Read about where the data is generated, how it comes fro BioMedIT and how the responsibility of management is controlled here: Data stream . ",
    "url": "/pages/hpc.html#data-stream",
    
    "relUrl": "/pages/hpc.html#data-stream"
  },"258": {
    "doc": "HPC infrastructure",
    "title": "HPC infrastructure",
    "content": "Last update: 20230616 . ",
    "url": "/pages/hpc.html",
    
    "relUrl": "/pages/hpc.html"
  },"259": {
    "doc": "Home",
    "title": "\n  \n  Dev docs\n",
    "content": ". ",
    "url": "/",
    
    "relUrl": "/"
  },"260": {
    "doc": "Home",
    "title": "Welcome",
    "content": "This is the documentation index for bioinformatic pipeline development. You can visit the project home page at https://switzerlandomics.ch. ",
    "url": "/#welcome",
    
    "relUrl": "/#welcome"
  },"261": {
    "doc": "Home",
    "title": "Quick start",
    "content": "Most likely, you are looking for one of our design documents. Check the side bar or go here: Index page for design documents. These pages describe the processes for our pipelines, start to finish, with links to supporting information. ",
    "url": "/#quick-start",
    
    "relUrl": "/#quick-start"
  },"262": {
    "doc": "Home",
    "title": "Latest updates",
    "content": "See the platform release updates for the most recent features. ",
    "url": "/#latest-updates",
    
    "relUrl": "/#latest-updates"
  },"263": {
    "doc": "Home",
    "title": "Full site navigation",
    "content": ". | 404 | Sepsis score: Phoenix | ACAT | ACMG criteria | Aggregate multiplexed data | Annotation table | BWA | Bayes 1 probability in placenta previa | Bayesian 2 probability in placenta previa | Bayesian discrete probability example in genetics | Bayes MCMC samplers | Bayes multiparameter bioassay demo | Bayes multiparameter models | Benchmarking pipeline output | BeviMed | BioMedIT | Bookmarks | BWA | Causal inference stats | Causal inference mosquito nets | Data concepts | Data stream | Design PCA SNV INDEL v1 | Design DNA SNV INDEL v1 | Design release DNA SNV INDEL v1 | Design release DNA SNV INDEL v2 | Design documents | Design QV SNV INDEL v1 | Design statistical genomics v1 | DNA annotation | DNA interpretation | DNA QC | Docker with singularity | Documentation log | Exomiser | Exomiser phenodigm | FASTP | FASTQ format data | Filter VCF with bcftools | Financial management | GATK BQSR | GATK Genomic db import | GATK Duplicates | GATK Genotyping gVCFs | GATK Genotype refine | GATK Haplotype caller | GATK VQSR | Guru variant interpretation | GWAS analysis | HPC infrastructure | Home | Inference of causal metabolite networks | Layout | MathJax config | MBDF models | MBDF supervised | MBDF unsupervised | WGS metadata | WGS metadata users | Metrics Bcftools counts | Metrics Bcftools stats | Metrics CollectWgsMetrics | Multiblock data fusion | QV - My voice | Panels disease gene | PCA biplot 1000genomes | PCA features | Platform updates | Pre-annotation MAF | Pre-annotation processing | Precision Medicine Unit (PMU) | Presentations | Read group | Reference genome | RL finite MDP | SLURM monitoring | SLURM sbatch headers | Stats Analysis of methods | Stats CI from P | Stats Correlation, regression and repeated data | Stats Odds ratios, SE &amp; CI | Stats Receiver operating characteristic plots | Stats Sensitivity and specificity | Storage architecture plan | Storage estimates | Storage, usage, and Git practices | Style page guide | Style writing guide | Structural variation detection | Synthetic data | Variables | Variant to RDF concept | VCF - Variant Call Format | VCF and gVCF | Virtual gene panels | VSAT with SKAT | SetID for VSAT | . ",
    "url": "/#full-site-navigation",
    
    "relUrl": "/#full-site-navigation"
  },"264": {
    "doc": "Home",
    "title": "About",
    "content": "General . Please contact us if you find any documentation that requires updates. We aim to include information about tools, reference datasets, databases, and any other component used in our pipelines besides sensitive and protected personal information. We will not include README information about pipeline source code here - this information should be found in the individual project repositories. However, should any stable pipelines become routinely implemented, we are likely add a user guide here. Content . This documentation is made to track the public progress of pipeline development. It includes information about the pipeline branches, for example DNA single variant detection, DNA CNV detection, RNA quantitative expression, etc. Many features are common to different branches (e.g. reference databases, reference datasets) are therefore listed separately form branch-specific content. We rely on accurate reference data sources and therefore each component is tracked with identification information such as source location (URL, citation, etc), date, checksum, file size, contributor ID, etc. We maintain a database to ensure accurate tracking to reduce duplication or mislabelling. This database is to be used for automated variable generation, reporting, and cross-project consistency. We aim to maintain this database in our public git repository to promote data integrity and open science. Data protection . No research data or personal information will be included in public documentation. Sensitive variables will only be stored within private offline project repositories. We use a public development repository to ensure a clear separation of infrastructure development and private patient data. ",
    "url": "/#about",
    
    "relUrl": "/#about"
  },"265": {
    "doc": "Home",
    "title": "Contributors",
    "content": ". | The switzerlandomics organisation on GitHub: github.com/switzerlandomics. | This documentation repository is hosted on our GitHub organisation under docs: github.com/docs-switzerlandomics.github.io. | . | You can request to become a member of the organisation via GitHub. | You can make pull requests to the docs repository. | Alternatively, you can email us with comments directly. | . ",
    "url": "/#contributors",
    
    "relUrl": "/#contributors"
  },"266": {
    "doc": "Home",
    "title": "Documentation style",
    "content": "This site is built using the bare-minimum template from the “Just the Docs” theme. It uses Jekyll to build the static site which is then hosted on GitHub pages (or hosted from any other server). If Jekyll is installed on your computer, you can also build and preview the created site locally. This lets you test changes before committing them, and avoids waiting for GitHub Pages.1 And you will be able to deploy your local build to a different platform than GitHub Pages. We currently allow GitHub pages to rebuild the site using Jekyll. We have also tested the method to push the pre-built _site, should additional custom plugins be required. However, we aim to rely on minimum complexity. More specifically, the created site: . | uses a gem-based approach, i.e. uses a Gemfile and loads the just-the-docs gem | uses the GitHub Pages / Actions workflow to build and publish the site on GitHub Pages | . Browse the theme documentation to learn more about how to use this theme. You can read about how to maintain docs in the docs directory of an existing project repo, see Hosting your docs from an existing project repo in the template README. However, this docs repo is a standalone repository within the SwitzerlandOmics github orgnaisation. We can use the special styles for {: .note } and {: .warning } by adding this code before a paragraph. ",
    "url": "/#documentation-style",
    
    "relUrl": "/#documentation-style"
  },"267": {
    "doc": "Home",
    "title": "Change log",
    "content": "The change log and (intermittently) the page count is tracked in the documentation log page. | Git log for 2024 contains 90 entries and is saved to gitlog_2024.txt | Git log for 2023 contains 49 entries and is saved to gitlog_2023.txt | . Growth . . | It can take up to 10 minutes for changes to your site to publish after you push the changes to GitHub. &#8617; . | . ",
    "url": "/#change-log",
    
    "relUrl": "/#change-log"
  },"268": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"269": {
    "doc": "Inference of causal metabolite networks",
    "title": "Inference of causal metabolite networks",
    "content": "A summary of: Chen, S., Lin, Z., Shen, X., Li, L., &amp; Pan, W. (2023). Inference of causal metabolite networks in the presence of invalid instrumental variables with GWAS summary data. Genetic Epidemiology, 1–15, https://doi.org/10.1002/gepi.22535 1. Briefly, this paper is about using instrumental variables (SNPs) in causal inference with applications to genome-wide association studies (GWAS). | Exposure: . | Metabolites: They serve as intermediate phenotypes connecting genetic variants to clinical outcomes and play a crucial role in biological processes. | . | Instrumental Variable (IV): . | SNPs (Single Nucleotide Polymorphisms): These are used as instrumental variables to isolate the variability in metabolites that is independent of confounders. | . | Outcome: . | Clinical outcomes: These are affected by metabolites, which in turn are influenced by SNPs. The causal effect of metabolites on clinical outcomes is what the study aims to determine. | . | . For a more gradual background jump down to the “In context” section. ",
    "url": "/pages/inf_causal_metab_sem.html#inference-of-causal-metabolite-networks",
    
    "relUrl": "/pages/inf_causal_metab_sem.html#inference-of-causal-metabolite-networks"
  },"270": {
    "doc": "Inference of causal metabolite networks",
    "title": "Abstract summary",
    "content": "Structural equation models (SEMs) for inferring causal networks in metabolites and other complex traits. The method: . | Performs causal analysis to discover relationships among multiple traits. | Accounts for potential invalid IVs. | Allows for data analysis using only GWAS summary statistics. | Considers bidirectional relationships between traits. | . The approach uses a stepwise selection to identify invalid IVs, and demonstrates its superior performance using both real and simulated GWAS data. ",
    "url": "/pages/inf_causal_metab_sem.html#abstract-summary",
    
    "relUrl": "/pages/inf_causal_metab_sem.html#abstract-summary"
  },"271": {
    "doc": "Inference of causal metabolite networks",
    "title": "Key methods summary",
    "content": "2.1 One-sample data . For one-sample GWAS individual-level data: . | For each of \\(n\\) individuals: . | We have \\(p\\) SNPs as IVs and \\(M\\) traits. | Traits are denoted by an \\(n x M\\) matrix \\(Y = [y_1, y_2, ... , y_M]\\). | Where \\(y_i = (y_1i, y_2i, ..., y_ni)^T\\) is the vector of \\(n\\) observations for trait \\(i = 1, 2, ..., M\\). | . | IVs are denoted as \\(Z = [z_1, ... , z_p]\\). | Where \\(z_j = (z_1j, ..., z_nj)^T\\) is the vector of the \\(n\\) observations for IV \\(j = 1, 2, ..., p\\). | . | . | . 2.2 SEM with individual-level data . | SEM is used as a tool for multivariate causal inference in this framework. | The analysis is based on one-sample GWAS individual-level data. | The system considers linear equations for the ‘n’ individuals in the sample. | Notations follow Wang et al. 2016. | The model has random errors denoted by a ‘n x M’ matrix. | The matrix ‘E’ consists of vectors for the ‘n’ random errors for each trait from 1 to M. | The assumption is made that the expected value of error ‘e_i’ is 0. | For each trait, the error follows a normal distribution with mean 0 and variance ‘σ_i^2’. | The matrix ‘M x M’ denotes the coefficients for the traits, represented by ‘Γ’. | The coefficients for each trait ‘i’ range from 1 to M. | The linear model for the ‘i-th’ trait is represented by an equation where traits are connected linearly and the sum is 0. | . 2.3 SEM with some invalid IVs . | Discusses the scenario when some IVs may be invalid. | Violation of one or more of the three valid IV Assumptions (A1)–(A3). | Proposes a method to account for invalid IVs in SEM. | For trait i, the first \\({p}_{0i}\\) IVs are considered invalid IVs. | Represents the matrix of invalid IVs with \\({Z}_{ {\\mathscr{A}}_{i}}=[{z}_{1},{\\rm{\\ldots }},{z}_{p_{0i}}]\\). | \\({z}_{j}={({z}_{1j},{\\rm{\\ldots }},{z}_{nj})}^{T}\\) is the vector for the n observations of IV \\(j=1,2,{\\rm{\\ldots }},{p}_{0i}\\). | \\(M\\) vectors are present for the coefficients of invalid IVs: \\({B}_{1},{\\rm{\\ldots }},{B}_{M}\\). | \\({B}_{i}={({\\beta }_{1i},{\\beta }_{2i},{\\rm{\\ldots }},{\\beta }_{p_{0i}i})}^{T}\\) represents the direct or (horizontal) pleiotropic effects of the invalid IVs on trait \\(i=1,2,{\\rm{\\ldots }},M\\). | The linear SEM for trait i is represented as: \\({y}_{1}{\\gamma }_{1i}+\\cdots +{y}_{i}{\\gamma }_{ii}+\\cdots +{y}_{M}{\\gamma }_{Mi}+{z}_{1}{\\beta }_{1i}+\\cdots +{z}_{p_{0}i}{\\beta }_{p_{0i}i}+{e}_{i}=0,\\). | \\(\\gamma\\)’s and \\(\\beta\\)’s are unknown parameters in the model. | . ",
    "url": "/pages/inf_causal_metab_sem.html#key-methods-summary",
    
    "relUrl": "/pages/inf_causal_metab_sem.html#key-methods-summary"
  },"272": {
    "doc": "Inference of causal metabolite networks",
    "title": "Code",
    "content": "onesample mvstepIV . The original source code is here: https://github.com/chen-siyi7/one-sample-stepwise-IV-selection/blob/main/one-sample%20stepwise%20IV%20code.R . The function onesample_mvstepIV conducts one-sample stepwise IV. Input Parameters: . | p: Total number of predictors. | R: Correlation matrix. | betaZX: Regression coefficients for predictors. | betaZY: Regression coefficients for outcomes. | se_betaZY: Standard error of betaZY. | n: Sample size. | gamma_hat: Gamma hat values (prior information). | . Main Computations: . | Initialize ZTZ using R as: \\(ZTZ = R\\) | Compute ZTY as the element-wise product of ZTZ diagonal and betaZY: \\(ZTY = \\text{diag}(ZTZ) \\times \\beta{ZY}\\) | Calculate the median of YTY for each predictor SNP as: \\(YTY[SNP] = (n-1) \\times ZTZ[SNP,SNP] \\times (se_\\beta{ZY}^2)[SNP] + ZTY[SNP] \\times \\beta{ZY}[SNP]\\), excluding NA values. | Compute Bayesian Information Criterion (BIC) for each predictor. For each predictor i: . | a. Create a matrix test11 with diagonal element i set to 1. | b. Create matrix W1 by combining test11 and gamma_hat. | c. Solve for W1 using: \\(\\text{solve.W1} = W1^T \\times ZTZ \\times W1\\) | d. Compute beta estimates beta1 as: \\(\\beta1 = (solve.W1^{-1} \\times W1^T \\times ZTY)\\) | e. Calculate BIC as: \\(testbic[i] = n \\times \\log(YTY - \\beta1^T \\times W1^T \\times ZTY) + \\log(n) \\times \\sum_{i} \\text{diag}(test11)\\) | . | Determine the optimal instrument variables (IVs) based on BIC: . | a. For each iteration j, select the predictor i with the smallest BIC. | b. Repeat the process by adding one predictor at a time. | c. Stop if the current and previous IV are the same. | . | Compute the final beta estimates using the invalid IVs: . | a. Extract the invalid IVs from whichIV and set their diagonal elements in test11 to 1. | b. Compute beta estimates beta1 as: \\(\\beta1 = (solve.W1^{-1} \\times W1^T \\times ZTY)\\) | c. Calculate variance of beta Varbeta as: \\(\\text{Var\\beta} = \\text{diag}(solve.W1 \\times n) \\times \\sigma_u2\\) where \\(\\sigma_u2 = YTY - \\beta1^T \\times W1^T \\times ZTY\\). | . | . Output: . | invalidIV: Indices of invalid IVs. | beta_est: Estimated beta values. | beta_se: Standard error of beta estimates. | K: Number of invalid IVs. | . onesample mvstepIV ind . The original source code is here: https://github.com/chen-siyi7/one-sample-stepwise-IV-selection/blob/main/onesample_mvstepIV_ind%20code.R . The function onesample_mvstepIV_ind performs one-sample stepwise IV for independent SNPs. Input Parameters: . | Y: Response variable. | Z: Predictor matrix. | n: Sample size. | gamma_hat: Gamma hat values (prior information). | . Main Computations: . | Initialize testbic for Bayesian Information Criterion. | For each predictor i: . | a. Initialize a zero vector l with length dim(Z)[2] and set the ith element to 1. | b. Modify matrix Z22 such that for each row j, Z22[j,] is Z[j,]*l. | c. Perform a linear regression (lm_stage2) of Y on Z22 and Z*gamma_hat. | d. Calculate BIC for this predictor using: \\(testbic[i] = n \\times \\log\\left(\\frac{\\sum(lm\\_stage2\\text{residuals}^2)}{n}\\right) + \\log(n) \\times \\sum(l)\\) | . | Determine the optimal instrument variables (IVs) based on BIC: . | a. For each iteration j, select the predictor i with the smallest BIC. | b. Modify matrix Z22 for the selected predictors and add one predictor at a time. | c. Repeat the linear regression and calculate BIC as in step 2. | d. Stop if the current and previous IV are the same. | . | Extract the invalid IVs, which.invalid, from whichIV and sort them to obtain K. | Compute the final beta estimates using the invalid IVs: . | a. Extract columns K from Z to form Z22. | b. Perform linear regression (lm_stage2) of Y on Z22 and Z*gamma_hat. | c. Calculate beta estimates betaest as: \\(\\beta{est} = \\text{summary}(lm\\_stage2)\\text{coef[,1]}\\) | d. Calculate variance of residuals sigma_u2 as: \\(\\sigma_u2 = \\frac{\\sum(lm\\_stage2\\text{residuals}^2)}{n}\\) | e. Calculate variance of beta Varbeta using: \\(\\text{Varbeta} = \\text{diag}(ginv(X^TX)) \\times \\sigma_u2\\) where \\(X = \\text{cbind}(Z22, Dhat)\\) and \\(Dhat = Z*gamma\\_hat\\). | f. Compute the standard error betase as: \\(\\beta{se} = \\sqrt{\\text{Varbeta}}\\) | . | . Output: . | beta_est: Estimated beta values. | beta_se: Standard error of beta estimates. | invalid IVs: Indices of invalid IVs. | no. of invalid IV: Number of invalid IVs. | . ",
    "url": "/pages/inf_causal_metab_sem.html#code",
    
    "relUrl": "/pages/inf_causal_metab_sem.html#code"
  },"273": {
    "doc": "Inference of causal metabolite networks",
    "title": "In context",
    "content": "Recap: . | Exposure: . | Metabolites: They serve as intermediate phenotypes connecting genetic variants to clinical outcomes and play a crucial role in biological processes. | . | Instrumental Variable (IV): . | SNPs (Single Nucleotide Polymorphisms): These are used as instrumental variables to isolate the variability in metabolites that is independent of confounders. | . | Outcome: . | Clinical outcomes: These are affected by metabolites, which in turn are influenced by SNPs. The causal effect of metabolites on clinical outcomes is what the study aims to determine. | . | . Background . In GWAS, associations are generally sought between single nucleotide polymorphisms (SNPs) and a single trait. But GWAS data can also be used to analyze multiple related traits, leading to improved power and new biological insights. Specifically, network analysis of multiple traits is gaining interest, especially when it comes to causal network analysis. This is pivotal for elucidating relationships among multiple traits, such as in gene network and protein network analyses. Metabolite network analysis, the focal point of this research, posits that metabolites are integral parts of many biological processes, often interacting with each other in regulatory networks. By inferring these networks, we can gain insight into relationships among metabolites in biological processes. In causal networks, traits, including metabolites, proteins, and genes, serve as the nodes. Their causal relationships are represented by directed edges connecting them. SNPs are utilized as instrumental variables (IVs). To model these intricate biological networks, structural equation models (SEMs) have been adopted. What is an Instrumental Variable (IV)? . An instrumental variable is associated with the exposure but does not have a direct association with the outcome, except through its relationship with the exposure. Its role is to isolate the variability in the exposure that is independent of the confounders. Key Assumptions . | Relevance: The IV is correlated with the exposure. | Exclusion: The IV only affects the outcome through its effect on the exposure. | Exchangeability: The IV is not associated with unobserved confounders. | . How Does It Work? . IV analysis uses the variation in the exposure explained by the instrument to estimate the causal effect of the exposure on the outcome. Why is it Necessary for Causal Inference? . | Control for Unmeasured Confounding: IVs can provide unbiased estimates of causal effects when unmeasured or unobserved confounding is present. | Endogeneity: IVs can solve the problem of endogeneity. | Natural Experiments: IVs can be employed in “natural experiments” where random assignment of treatments is not feasible. | . Usage . In the context of GWAS and metabolite network analysis, IV methods are crucial. They help determine causal relationships in complex biological processes, especially when metabolites, which do not function in isolation, interact within metabolite regulatory networks. Limitations . | Weak Instruments: Weak correlation between the IV and exposure can lead to biased IV estimates. | Violations of Assumptions: IV estimates can be biased if any core assumptions are violated. | Interpretability: The causal effect estimated through IV is often specific to a particular population, reducing generalizability. | . Summary . Instrumental Variables are a pivotal tool in causal inference, especially in genome-wide association studies (GWAS). When utilized properly, they can provide valuable insights into causal relationships in settings laden with confounding and endogeneity. However, they come with their own assumptions and potential limitations. ",
    "url": "/pages/inf_causal_metab_sem.html#in-context",
    
    "relUrl": "/pages/inf_causal_metab_sem.html#in-context"
  },"274": {
    "doc": "Inference of causal metabolite networks",
    "title": "References",
    "content": ". | Chen, S., Lin, Z., Shen, X., Li, L., &amp; Pan, W. (2023). Inference of causal metabolite networks in the presence of invalid instrumental variables with GWAS summary data. Genetic Epidemiology, 1–15, https://doi.org/10.1002/gepi.22535. &#8617; . | . ",
    "url": "/pages/inf_causal_metab_sem.html#references",
    
    "relUrl": "/pages/inf_causal_metab_sem.html#references"
  },"275": {
    "doc": "Inference of causal metabolite networks",
    "title": "Inference of causal metabolite networks",
    "content": "Last update: 20230813 . | Inference of causal metabolite networks . | Abstract summary | Key methods summary . | 2.1 One-sample data | 2.2 SEM with individual-level data | 2.3 SEM with some invalid IVs | . | Code . | onesample mvstepIV | onesample mvstepIV ind | . | In context . | Background | What is an Instrumental Variable (IV)? | Key Assumptions | How Does It Work? | Why is it Necessary for Causal Inference? | Usage | Limitations | Summary | . | References | . | . ",
    "url": "/pages/inf_causal_metab_sem.html",
    
    "relUrl": "/pages/inf_causal_metab_sem.html"
  },"276": {
    "doc": "Layout",
    "title": "Data generation and control",
    "content": "We rely on strict data control processes. Once study participation is established according to the legal and ethical framework, samples are sent for data gernation. Omic data is generated by SMOC and data is entered and controlled within SPHN DCC BioMedIT sciCORE on the MOMIC tenant. ",
    "url": "/pages/layout.html#data-generation-and-control",
    
    "relUrl": "/pages/layout.html#data-generation-and-control"
  },"277": {
    "doc": "Layout",
    "title": "Data generation",
    "content": "SMOC has 3 branches: . | CGAC - Genomics. | based in Health2030 genome center Geneva. | . | CPAC - Proteotyping. | based in ETHZ Metabolomics &amp; Biophysics | . | CMAC - Metabolomics &amp; Lipidomics. | based in ETHZProteomics | . | . ",
    "url": "/pages/layout.html#data-generation",
    
    "relUrl": "/pages/layout.html#data-generation"
  },"278": {
    "doc": "Layout",
    "title": "Data control",
    "content": "Organisation: . | Swiss Personalised Health Network (SPHN) . | Data Coordination Center (DCC) | DCC is also a part of SIB . | data on BioMedIT network . | our tenant is called MOMIC | on the sciCORE infrastructure | . | . | . | omic data from SMOC http://smoc.ethz.ch/ | database structure requires Resource Description Framework (RDF) Schema | training material available here, but is a DCC responsibility Useful image, our aim it to show #3 “Use cases” | . ",
    "url": "/pages/layout.html#data-control",
    
    "relUrl": "/pages/layout.html#data-control"
  },"279": {
    "doc": "Layout",
    "title": "Simplified example",
    "content": ". | For our genetic analysis dataset we might have 1000 patient_ID | For each patient_ID we have several sequence files and a sequence metadata file | For each patient_ID we require all available clinical data | . Genetic_sequence_data: . file_name: SPH_001_R1.fq.gz file_content: FASTQ sequence raw data storage_location: ./project/data/ . Genetic_sequence_meta_data: . sample_facility: SMOC sequence_method: exome_capture sequence_preparation: twist_humancorePlusRefSeq_hg38 sequence_technology: NovaSeq6000 demultiplexing_version: 1.1.13 order_ID: 12345 sample_reception: 20210102 sample_completion: 20210109 patient_ID: SPH_001 md5sun: 7d776c010149208c782ed7253ce70159 file_name: SPH_001_R1.fq.gz . Clinical data: . patient_ID: SPH_001 hospital_ID: bern_01 age_days: 100 sex: 0 admission_date: 20210101 discharge_data: 20210201 picu.adm: NA death.date: NA comorbities: 0 clin.focus: UTI pathogen_found: p.aeruginosa severity_score: 5 . ",
    "url": "/pages/layout.html#simplified-example",
    
    "relUrl": "/pages/layout.html#simplified-example"
  },"280": {
    "doc": "Layout",
    "title": "Workflow order",
    "content": "First stage order: . | Clinical data is used to select the patient_ID | The patient_ID is used to select the Genetic_sequence_data and Genetic_sequence_meta_data. | Genetic_sequence_meta_data is used to confirm that Genetic_sequence_data is usable. | Genetic_sequence_data is used as input for primary analysis pipeline. | Primary analysis pipeline consists of ~20 individual tasks. | Primary analysis pipeline tasks include multiple new data formats which are later deleted. | Primary analysis pipeline outputs final format for storage (e.g. VCF format). | Primary analysis pipeline outputs additional formats if required. Examples: . | clinical genetics report | eigenval/eigenvec | annotation | cohort level statistics | . | . Second stage: . | Output from primary stage is merged with all clinical data for statistical analysis | Analysis include that of the genomics lab, machine learning lab. | Example 1: genetic association studies: | . | outcome: genotype | predictors: clinical consequence | covariates: clinical/demographic features * Example 2: machine learning study: | outcome: clinical consequence | predictors: clinical/demographic features | covariates: genetics | . | . ",
    "url": "/pages/layout.html#workflow-order",
    
    "relUrl": "/pages/layout.html#workflow-order"
  },"281": {
    "doc": "Layout",
    "title": "Examples of data re-use:",
    "content": ". | All original raw sequence data will be re-run incrementally as the first stage receives major pipeline updates. | Secondary stage will require re-using the same output from first stage many times for different projects. | Pertinent findings from both first stage and second stage will require follow ups. Data from both stages will be re-used. | Output from first stage and second stage will include summary statistics that will can be reused for QC and follow-up statistics. | . ",
    "url": "/pages/layout.html#examples-of-data-re-use",
    
    "relUrl": "/pages/layout.html#examples-of-data-re-use"
  },"282": {
    "doc": "Layout",
    "title": "Other notes:",
    "content": ". | Here is an example of an existing concept for GeneticVariation | Note that genetic variant and location are rarely sufficient for final storage. We rely on accurate nomenclature, reference genome version, variant call quality, variant call confidence, etc. | Note that the final output from first stage is typically VCF format (but will adapt to needs) | We also typically annotate a VCF with &gt;150 annotation databases (i.e. like one “column” per annotation per variant row) | We also typically store as a cohort-level VCF (e.g. 1000 samples) rather than one VCF per sample | . ",
    "url": "/pages/layout.html#other-notes",
    
    "relUrl": "/pages/layout.html#other-notes"
  },"283": {
    "doc": "Layout",
    "title": "Layout",
    "content": "Last update: 20230531 . ",
    "url": "/pages/layout.html",
    
    "relUrl": "/pages/layout.html"
  },"284": {
    "doc": "MathJax config",
    "title": "MathJax Configuration",
    "content": ". | Create or extend _includes/head_custom.html with: . {% assign math = page.math | default: layout.math | default: site.math %} {% case math %} {% when \"mathjax\" %} {% include mathjax.html %} {% endcase %} . | Copy the following files to your website source repo: . | . | _includes/mathjax.html | _layouts/mathjax.html | assets/js/mathjax-script-type.js | . For the source file _includes/mathjax.html from just-the-docs, we have updated the incorrectly set path from src=\"/just-the-docs/assets/js/mathjax-script-type.js\" to the repositry root src=\"/assets/js/mathjax-script-type.js\". | To make MathJax available on all your web pages, add to _config.yml: . math: mathjax . To restrict MathJax to pages that use it, add to the front matter either: . math: mathjax . or: . layout: mathjax . You can add a preamble of MathJax definitions of new commands and environments in _layouts/mathjax.html. It extends the default layout. | . ",
    "url": "/pages/mathjax.html#mathjax-configuration",
    
    "relUrl": "/pages/mathjax.html#mathjax-configuration"
  },"285": {
    "doc": "MathJax config",
    "title": "MathJax options",
    "content": "You can customise MathJax by adding further options in _includes/mathjax.html. You can customise Just the Docs sites to support MathJax, as explained in the configuration suggestions. Pages then render \\(\\mathrm{\\LaTeX}\\) code in kramdown math blocks using MathJax. For example: . \\[\\begin{aligned} &amp; \\phi(x,y) = \\phi \\left(\\sum_{i=1}^n x_ie_i, \\sum_{j=1}^n y_je_j \\right) = \\sum_{i=1}^n \\sum_{j=1}^n x_i y_j \\phi(e_i, e_j) = \\\\ &amp; (x_1, \\ldots, x_n) \\left( \\begin{array}{ccc} \\phi(e_1, e_1) &amp; \\cdots &amp; \\phi(e_1, e_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\phi(e_n, e_1) &amp; \\cdots &amp; \\phi(e_n, e_n) \\end{array} \\right) \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right) \\end{aligned}\\] To see the \\(\\mathrm{\\LaTeX}\\) source of the formula, right-click anywhere on it. ",
    "url": "/pages/mathjax.html#mathjax-options",
    
    "relUrl": "/pages/mathjax.html#mathjax-options"
  },"286": {
    "doc": "MathJax config",
    "title": "MathJax config",
    "content": " ",
    "url": "/pages/mathjax.html",
    
    "relUrl": "/pages/mathjax.html"
  },"287": {
    "doc": "MBDF models",
    "title": "Multiblock Data Fusion Options",
    "content": "This document outlines three levels of data fusion methods: . ",
    "url": "/pages/mbdf_models.html#multiblock-data-fusion-options",
    
    "relUrl": "/pages/mbdf_models.html#multiblock-data-fusion-options"
  },"288": {
    "doc": "MBDF models",
    "title": "High-level data fusion",
    "content": ". | Approach: integration of outcomes from individual models. | Key features: . | treat each data block separately | combine summary statistics or predictions | utilise ensemble learning or voting schemes | . | Use case: when joint interpretation of biomarker patterns or decision fusion is desired | . ",
    "url": "/pages/mbdf_models.html#high-level-data-fusion",
    
    "relUrl": "/pages/mbdf_models.html#high-level-data-fusion"
  },"289": {
    "doc": "MBDF models",
    "title": "Mid-level data fusion",
    "content": ". | Approach: extract and integrate features from each data block. | Key features: . | dimensionality reduction methods (e.g. PCA, PLS) to obtain scores | two-step procedure: . | middle-up: reduce dimensions then concatenate scores | middle-down: select key variables then concatenate subsets | . | . | Use case: when intermediate patterns or characteristics are needed for further analysis | . ",
    "url": "/pages/mbdf_models.html#mid-level-data-fusion",
    
    "relUrl": "/pages/mbdf_models.html#mid-level-data-fusion"
  },"290": {
    "doc": "MBDF models",
    "title": "Low-level data fusion",
    "content": ". | Approach: direct integration of raw signals or data. | Key features: . | analyse relationships across data blocks | factor analysis or multiblock modelling to create components | optimise for both within-block representation and between-block correlation | . | Use case: when it is necessary to capture detailed inter-variable and inter-block relationships | . ",
    "url": "/pages/mbdf_models.html#low-level-data-fusion",
    
    "relUrl": "/pages/mbdf_models.html#low-level-data-fusion"
  },"291": {
    "doc": "MBDF models",
    "title": "Final notes",
    "content": " ",
    "url": "/pages/mbdf_models.html#final-notes",
    
    "relUrl": "/pages/mbdf_models.html#final-notes"
  },"292": {
    "doc": "MBDF models",
    "title": "Matrix factorisation",
    "content": "Below is an example of how individual contributions from each data block can combine into a global factor for three factors (F_1), (F_2), and (F_3): . \\[F_1 = X_1^1 w_1^1 + X_2^1 w_2^1 + \\dots + X_n^1 w_n^1\\] \\[F_2 = X_1^2 w_1^2 + X_2^2 w_2^2 + \\dots + X_n^2 w_n^2\\] \\[F_3 = X_1^3 w_1^3 + X_2^3 w_2^3 + \\dots + X_n^3 w_n^3\\] The objective is often to maximise the overall covariance across blocks: . \\[\\max \\sum_{j,k} \\operatorname{cov}\\bigl(X_j^m w_j^m,\\; X_k^m w_k^m\\bigr)\\] ",
    "url": "/pages/mbdf_models.html#matrix-factorisation",
    
    "relUrl": "/pages/mbdf_models.html#matrix-factorisation"
  },"293": {
    "doc": "MBDF models",
    "title": "Correlation, variance and covariance",
    "content": "Correlation quantifies the relationship between two variables. This principle is extended to assess links between entire data blocks: . \\[\\operatorname{corr}(x,y) = \\frac{\\operatorname{cov}(x,y)}{\\sqrt{\\operatorname{var}(x)\\,\\operatorname{var}(y)}}\\] ",
    "url": "/pages/mbdf_models.html#correlation-variance-and-covariance",
    
    "relUrl": "/pages/mbdf_models.html#correlation-variance-and-covariance"
  },"294": {
    "doc": "MBDF models",
    "title": "Extracting structures from data",
    "content": "Data components are linear combinations of original variables. Their relationships can be compared using: . \\[\\operatorname{cov}^2\\bigl(X_j w_j,\\; X_k w_k\\bigr) = \\operatorname{var}(X_j w_j)\\,\\operatorname{corr}^2\\bigl(X_j w_j,\\; X_k w_k\\bigr)\\,\\operatorname{var}(X_k w_k)\\] The RV coefficient is an alternative that compares sample configurations across matrices, even when variable counts differ. Methods like partial least squares (PLS) determine the first latent variables (t_1) and (u_1) for two data blocks by maximising their covariance. ",
    "url": "/pages/mbdf_models.html#extracting-structures-from-data",
    
    "relUrl": "/pages/mbdf_models.html#extracting-structures-from-data"
  },"295": {
    "doc": "MBDF models",
    "title": "Low-level horizontal multiblock analysis",
    "content": "This approach integrates multiple data blocks sharing the same observations. It can be considered at two levels: . | block level (local): \\(w'X_1,\\quad w'X_2,\\quad w'X_3\\) . | super level (global): global scores combine the individual block contributions, for example: \\(tX_s,\\quad w^T,\\quad qX,\\quad uY,\\quad Y\\) . | . The super level is constructed based on the chosen model and aims to capture overall patterns. ",
    "url": "/pages/mbdf_models.html#low-level-horizontal-multiblock-analysis",
    
    "relUrl": "/pages/mbdf_models.html#low-level-horizontal-multiblock-analysis"
  },"296": {
    "doc": "MBDF models",
    "title": "Methods and criteria to be optimised",
    "content": "Many methods require choosing which variation to prioritise. In practice, covariance is used as a criterion to balance within-block detail and between-block connections. ",
    "url": "/pages/mbdf_models.html#methods-and-criteria-to-be-optimised",
    
    "relUrl": "/pages/mbdf_models.html#methods-and-criteria-to-be-optimised"
  },"297": {
    "doc": "MBDF models",
    "title": "Potential model outputs",
    "content": ". | new common subspace: shared latent components revealing global patterns | common/distinct components: factors separating shared structure from block-specific variation | pattern recognition: improved detection of meaningful relationships across variables | global/local observation scores: individual sample scores facilitating both overall and block-level interpretations | variables loadings: insights into which variables contribute most strongly to each component | block weights: balancing each block’s influence on the global model | more complete interpretation: linking variables within each block and across multiple blocks | . Slide from SIB training github. ",
    "url": "/pages/mbdf_models.html#potential-model-outputs",
    
    "relUrl": "/pages/mbdf_models.html#potential-model-outputs"
  },"298": {
    "doc": "MBDF models",
    "title": "MBDF models",
    "content": "Last update: 20250307 . | Multiblock Data Fusion Options . | High-level data fusion | Mid-level data fusion | Low-level data fusion | . | Final notes . | Matrix factorisation | Correlation, variance and covariance | Extracting structures from data | Low-level horizontal multiblock analysis | Methods and criteria to be optimised | Potential model outputs | . | . ",
    "url": "/pages/mbdf_models.html",
    
    "relUrl": "/pages/mbdf_models.html"
  },"299": {
    "doc": "MBDF supervised",
    "title": "MBDF supervised analysis",
    "content": "Last update: 20250307 . This document presents a supervised multiblock analysis to discriminate between two genotypes (wild-type and PPAR) by integrating gene expression and lipid data. In this analysis, a block PLS-DA model is constructed using the function block.plsda(), where the predictor matrices (genes and lipids) are combined into a multiblock structure and the response is a categorical variable. The model builds latent variables, often denoted by $t$, which are computed as linear combinations of the predictors, i.e. \\(t = Xw,\\) where $X$ is the data matrix and $w$ is a weight vector. These latent variables are used to explain the variance in the response and to achieve discrimination. Once the model is built, the next step is to select the optimal number of latent components. This is done using the perf() function, which evaluates performance metrics such as the classification error rate, $R^2$, and the average variance explained ($AVE_X$) through cross-validation. The idea is to balance model complexity with predictive accuracy, ensuring that additional components contribute meaningfully to the discrimination. The model’s significance is then assessed via permutation tests, implemented with DIABLO.test(). Here, the null hypothesis posits that the observed discrimination is no better than what could be expected by chance. By comparing the classification error rate (CER) and other metrics from the original model to those from models built on permuted data, one can derive a p-value indicating statistical significance. An important aspect of the analysis is the examination of variance explained by each block and the overall model. For each block, the metric $AVE_X$ quantifies how much of the original variance is captured by the latent variables, while a global measure, $AVE_{outer}$, summarises the performance across blocks. This allows one to evaluate both the contribution of individual data types and the integrated model. Visualisation plays a key role in interpreting the results. Scores plots are used to display the position of each sample in the latent variable space, facilitating the assessment of group separation. Similarly, loadings plots reveal which genes and lipids contribute most strongly to the discrimination, with variables showing high absolute loading values being of particular interest. An alternative approach is taken with Consensus OPLS-DA, where the data blocks are scaled and integrated in a consensus framework. The model produces a predictive component, $p_1$, linked to the outcome, and an orthogonal component, $o_1$, which captures variability unrelated to the outcome. This method further refines the identification of discriminative features and validates the model performance through cross-validation and permutation-based statistics. Key questions addressed in this analysis include: . | Can the integrated gene and lipid data successfully discriminate between wild-type and PPAR samples? | What is the optimal number of latent variables required to balance complexity and performance? | Is the observed model statistically significant compared to random chance? | Which variables, among genes and lipids, are most important in driving the discrimination? | . By systematically building the model, selecting the optimal number of latent variables, testing significance through permutations, and visualising both sample distributions and variable contributions, this approach provides a comprehensive framework for discriminant analysis using multiblock data. ",
    "url": "/pages/mbdf_supervised.html#mbdf-supervised-analysis",
    
    "relUrl": "/pages/mbdf_supervised.html#mbdf-supervised-analysis"
  },"300": {
    "doc": "MBDF supervised",
    "title": "MBDF supervised",
    "content": " ",
    "url": "/pages/mbdf_supervised.html",
    
    "relUrl": "/pages/mbdf_supervised.html"
  },"301": {
    "doc": "MBDF unsupervised",
    "title": "MBDF unsupervised analysis",
    "content": "Last update: 20250307 . This document presents an unsupervised multiblock analysis using ComDim, which extracts common dimensions from multiple data blocks. In this case, gene expression and lipid concentration data from the nutrimouse study are combined, creating a single data matrix with known block sizes. The analysis centres on computing global components, $T_g$, by performing a multiblock weighted principal components analysis. In effect, each global component is computed as a linear combination of the original variables via weights, i.e. \\(T_g = XW_g,\\) where $X$ is the concatenated data matrix and $W_g$ represents the global weights. The output from the ComDim analysis includes several key elements. The matrix of saliences contains block-specific weights that reveal each block’s contribution to the global components. Global scores ($Scor.g$) project the original samples into the common space, while global loadings ($Load.g$) indicate the contribution of each variable to these components. Additionally, the explained inertia is provided both per block and cumulatively, allowing one to assess the proportion of total variance captured by the components. By visualising the scores on different dimensions (for example, plotting Dim.1 vs Dim.2 or Dim.3 vs Dim.4), the analysis identifies clusters of samples and highlights the main sources of variation in the data. Loadings plots further reveal which genes and lipids drive these variations. The document also shows how the analysis can be repeated on specific subsets (e.g. wild-type or PPAR samples only) to explore group-specific data structures. This unsupervised multiblock analysis with ComDim offers a comprehensive way to integrate and explore the shared structure of complex datasets by extracting common dimensions and examining the contributions of each data block. ",
    "url": "/pages/mbdf_unsupervised.html#mbdf-unsupervised-analysis",
    
    "relUrl": "/pages/mbdf_unsupervised.html#mbdf-unsupervised-analysis"
  },"302": {
    "doc": "MBDF unsupervised",
    "title": "MBDF unsupervised",
    "content": " ",
    "url": "/pages/mbdf_unsupervised.html",
    
    "relUrl": "/pages/mbdf_unsupervised.html"
  },"303": {
    "doc": "WGS metadata",
    "title": "WGS metadata",
    "content": "Meta data from sequencing assays are produced in the following format for WGS. ",
    "url": "/pages/metadata.html#wgs-metadata",
    
    "relUrl": "/pages/metadata.html#wgs-metadata"
  },"304": {
    "doc": "WGS metadata",
    "title": "Project summary",
    "content": "| Field | Details | . | Project name | WGS_NDS_Project_date | . | Customer name and contact details | Email: n.a. | . | First sample reception | Jan 01 2028 | . | Last sample reception | Jan 01 2028 | . | Analysis ID | hg38 | . | Sample type | DNA | . | Library protocol | WGS_TruSeq DNA PCR-free (Whole genome sequencing library preparation using the Illumina TruSeq DNA PCR-free reagents) | . | Analysis type | WGS | . | Reads | Read1: Read / 150; Read2: Index / 8; Read3: Index / 8; Read4: Read / 150 | . | Mapping reference | GCA_000001405.15_GRCh38_no_alt_analysis_set.fna | . | Sequencer model | NovaSeq6000 | . | Demultiplexing pipeline version | 01.01.13 | . ",
    "url": "/pages/metadata.html#project-summary",
    
    "relUrl": "/pages/metadata.html#project-summary"
  },"305": {
    "doc": "WGS metadata",
    "title": "Deliverable summary",
    "content": "| Field | Details | . | Customer order ID | WNGS00000001 | . | Number of samples submitted by customer | 100 | . | Number of samples requested by customer for processing | 99 | . | Number of samples sequenced | 99 | . | Contact person(s) | Names and emails | . | Files delivered to | Data manager name and email | . | Delivered file types | FastQ | . | Requested average MEAN_COVERAGE across samples | 30x | . | Average MEAN_COVERAGE across samples | 34.9 (lowest coverage is 10.0x for sample ABC001) | . | QC analysis checksum | 78fds6fds56fds567567fds678fds | . | Timestamp | Jan 01 2028 01:01:01 GMT / v0.9.0 | . | Report approved by | Provider manager name | . | Comments for customer | None | . | Note | Sample ‘ABC002’ was not listed on order request | . ",
    "url": "/pages/metadata.html#deliverable-summary",
    
    "relUrl": "/pages/metadata.html#deliverable-summary"
  },"306": {
    "doc": "WGS metadata",
    "title": "Samples",
    "content": "| SAMPLE | BARCODE | LIBRARY ID | PF CLUSTERS | %PF DUPLICATES | %PF BASES ALIGNED | Q30 | REQUESTED COVERAGE | MEAN COVERAGE | %NEW VARIANTS | PRIMARY SAMPLE TYPE | . | ABC001 | A0123456 | NGS000011336 | 390,500,001 | 10.01 | 96.01 | 91.41 | 30x | 30.5x | 0.66 | Fibroblasts | . | ABC003 | A0123457 | NGS000011337 | 320,001,002 | 9.99 | 98.09 | 91.21 | 30x | 30.5x | n.a. | Fibroblasts | . ",
    "url": "/pages/metadata.html#samples",
    
    "relUrl": "/pages/metadata.html#samples"
  },"307": {
    "doc": "WGS metadata",
    "title": "Run ID",
    "content": "Sequencing assay may not always reach the intended sequencing depth (e.g. 30x) from the library for a sample. Therefore, the same library might be run again. The run ID will then be new but the data from both runs will be later merged to meet the intended sequencing depth. Each of the FASTQ files will be output to unique directories. The directory name will contain the run ID (e.g. ABC8182DHCBS901). ",
    "url": "/pages/metadata.html#run-id",
    
    "relUrl": "/pages/metadata.html#run-id"
  },"308": {
    "doc": "WGS metadata",
    "title": "WGS metadata",
    "content": "Last update: 20240406 . ",
    "url": "/pages/metadata.html",
    
    "relUrl": "/pages/metadata.html"
  },"309": {
    "doc": "WGS metadata users",
    "title": "What is your genome metadata?",
    "content": "Genome metadata is the information stored alongside your genome files. It includes file names, sizes, checksums, sample identifiers, and quality metrics. Not every record will have the same fields. Raw data may contain only FASTQ files, while processed genomes often include VCF files, sample information, and quality control summaries. As your genome moves through analysis, more metadata may be added. ",
    "url": "/pages/metadata_users.html#what-is-your-genome-metadata",
    
    "relUrl": "/pages/metadata_users.html#what-is-your-genome-metadata"
  },"310": {
    "doc": "WGS metadata users",
    "title": "Overview",
    "content": ". | Genome ID: GV-TEST1234 | Status: stored | Created: 03.08.2025, 22:07 | . ",
    "url": "/pages/metadata_users.html#overview",
    
    "relUrl": "/pages/metadata_users.html#overview"
  },"311": {
    "doc": "WGS metadata users",
    "title": "Files",
    "content": "Your genome may be stored in different file formats, depending on the analysis stage. Each file has a size and a checksum (MD5) that helps verify integrity. | FASTQ: vault/genomes/gv-test1234.fastq.gz (10.2 GB) . | MD5: abc123def456... | . | VCF: vault/genomes/gv-test1234.vcf.gz (160.5 MB) . | MD5: 789ghi012jkl... | . | . Other formats such as BAM or CRAM may also be available for efficient storage and access. ",
    "url": "/pages/metadata_users.html#files",
    
    "relUrl": "/pages/metadata_users.html#files"
  },"312": {
    "doc": "WGS metadata users",
    "title": "Shares",
    "content": "When you request that your genome (or part of it) be shared, each event is recorded. This log shows what was shared, when, and with whom. | 2025-08-07 — Type: QV_ACMG — Recipient: Dr. Jones &lt;dr.jones@hospital.ch&gt; | 2025-08-07 — Type: FULL_GENOME — Recipient: Dr. request &lt;dr.request@hospital.ch&gt; . | FTP: ftp://example/path | . | . ",
    "url": "/pages/metadata_users.html#shares",
    
    "relUrl": "/pages/metadata_users.html#shares"
  },"313": {
    "doc": "WGS metadata users",
    "title": "Sample details",
    "content": "After sequencing, technical metrics are available. These values help confirm the quality of the data. For example, mean coverage and Q30 indicate how complete and reliable your genome is. A requested coverage of 30x or higher is considered excellent in most analyses. | Sample ID: ABC001 | Library ID: NGS000012135 | Barcode: A0123456 | Primary sample type: Fibroblasts | Requested coverage: 30x | Mean coverage: 30.5x | Q30: 91.41% | PF clusters: 390,500,001 | PF duplicates: 10.01% | PF bases aligned: 96.01% | New variants: 0.66 | . ",
    "url": "/pages/metadata_users.html#sample-details",
    
    "relUrl": "/pages/metadata_users.html#sample-details"
  },"314": {
    "doc": "WGS metadata users",
    "title": "Quality control summary",
    "content": "Quality control (QC) summaries may be added after expert review. These provide additional checks on completeness and accuracy. | Timestamp: 2028-01-01T01:01:01Z | Approved by: Dr. Smith | QC checksum: 76fds4fds56fds567567fds678fds | Requested coverage: 30x | Mean coverage: 34.9x | Lowest coverage: 10.0x | . ",
    "url": "/pages/metadata_users.html#quality-control-summary",
    
    "relUrl": "/pages/metadata_users.html#quality-control-summary"
  },"315": {
    "doc": "WGS metadata users",
    "title": "Delivered file types",
    "content": "The types of files you may see include: . | FASTQ — raw sequencing reads | VCF — list of genetic variants | BAM / CRAM — processed formats for storage and quick access | . Different providers may deliver different formats, depending on the service requested. ",
    "url": "/pages/metadata_users.html#delivered-file-types",
    
    "relUrl": "/pages/metadata_users.html#delivered-file-types"
  },"316": {
    "doc": "WGS metadata users",
    "title": "Notes",
    "content": ". | Raw data may only include FASTQ files and checksums. | Processed genomes usually include VCFs, QC metrics, and sample details. | All fields are optional; availability depends on your analysis stage. | . ",
    "url": "/pages/metadata_users.html#notes",
    
    "relUrl": "/pages/metadata_users.html#notes"
  },"317": {
    "doc": "WGS metadata users",
    "title": "WGS metadata users",
    "content": "Last update: 2025-08-24 . ",
    "url": "/pages/metadata_users.html",
    
    "relUrl": "/pages/metadata_users.html"
  },"318": {
    "doc": "Metrics Bcftools counts",
    "title": "Metrics Bcftools counts",
    "content": "This is the main source of basic variant counts when understanding the effects of a qualifying variant (QV) design. The script shown below runs on a set of gVCF directories (INPUT_DIRS) where incremental filtering has reduces the dataset in each sequential step. A pyramid filtering plot is likely to use such count data. Example output: . | Number of samples: 180 | Number of SNPs: 97296 | Number of INDELs: 17945 | Number of MNPs: 0 | Number of others: 0 | Number of sites: 112132 | . Usage Example: . #!/bin/bash #SBATCH --array=0-24 ..... set -e echo \"START AT $(date)\" variables=\"/path/variables.sh\" source ${variables} QV_MODEL=\"Design_QV_SNV_INDEL_v1\" # Tools setup module load StdEnv/2023 gcc/12.3 bcftools/1.19 # Input directories setup declare -a INPUT_DIRS=( \"${VQSR_DIR}\" \"${GENOTYPE_REFINEMENT_DIR}\" \"${PRE_ANNOTATION_DIR}\" \"${PRE_ANNOTATION_MAF_DIR}\" \"${ANNOTATION_DIR}\" \"${ANNOTATION_GNOMAD_DIR}\" ) declare -a NUMBER for j in {1..22} X Y M; do NUMBER+=($j); done INDEX=${NUMBER[$SLURM_ARRAY_TASK_ID]} # Processing each directory for INPUT_DIR in \"${INPUT_DIRS[@]}\"; do echo \" \" echo \"Working on directory: ${INPUT_DIR}\" # Create an array of VCF files for the current chromosome mapfile -t vcf_files &lt; &lt;(ls ${INPUT_DIR}/chr${INDEX}_*.vcf.gz) # Process each VCF file in the directory for vcf in \"${vcf_files[@]}\"; do # Output file path for count results count_out=\"${QC_SUMMARY_STATS}/all_gvcf/${QV_MODEL}_$(basename \"${vcf}\" .vcf.gz)_qc.log\" # Get basic variant counts echo \"Input: ${vcf}\" echo \"Output: ${count_out}\" bcftools +counts \"${vcf}\" &gt; \"${count_out}\" done done echo \"END AT $(date)\" . ",
    "url": "/pages/metrics_bcftoolscounts.html#metrics-bcftools-counts",
    
    "relUrl": "/pages/metrics_bcftoolscounts.html#metrics-bcftools-counts"
  },"319": {
    "doc": "Metrics Bcftools counts",
    "title": "Metrics Bcftools counts",
    "content": "Last update: 20241219 . ",
    "url": "/pages/metrics_bcftoolscounts.html",
    
    "relUrl": "/pages/metrics_bcftoolscounts.html"
  },"320": {
    "doc": "Metrics Bcftools stats",
    "title": "Metrics Bcftools stats",
    "content": "Usage Example: . # Run bcftools stats echo \"--Generating stats for file: ${vcf}\" bcftools stats \"${vcf}\" &gt; \"${stats_out}\" # Run plot-vcfstats echo \"--Generating plots for stats of file: ${vcf}\" plot-vcfstats -p \"${plot_out}\" \"${stats_out}\" . We use this in our study book for: . | bcftools stats and plot-vcfstats: 07c_qc_summary_stats.sh -&gt; study_book/qc_summary_stats gVCF summary after HC | . ",
    "url": "/pages/metrics_bcftoolsstats.html#metrics-bcftools-stats",
    
    "relUrl": "/pages/metrics_bcftoolsstats.html#metrics-bcftools-stats"
  },"321": {
    "doc": "Metrics Bcftools stats",
    "title": "Metrics Bcftools stats",
    "content": "Last update: 20241217 . ",
    "url": "/pages/metrics_bcftoolsstats.html",
    
    "relUrl": "/pages/metrics_bcftoolsstats.html"
  },"322": {
    "doc": "Metrics CollectWgsMetrics",
    "title": "CollectWgsMetrics (Picard)",
    "content": "Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments. This tool collects metrics about the fractions of reads that pass base- and mapping-quality filters as well as coverage (read-depth) levels for WGS analyses. Both minimum base- and mapping-quality values as well as the maximum read depths (coverage cap) are user defined. Note: Metrics labeled as percentages are actually expressed as fractions! https://gatk.broadinstitute.org/hc/en-us/articles/360037269351-CollectWgsMetrics-Picard . Usage Example: . java -jar picard.jar CollectWgsMetrics \\ I=input.bam \\ O=collect_wgs_metrics.txt \\ R=reference_sequence.fasta . We use this in our study book for: . | CollectWgsMetrics: 03b_collectwgsmetrics.sh -&gt; study_book/qc_summary_stats mapping, depth, and more. | . | Metric | Summary | . | GENOME_TERRITORY | The number of non-N bases in the genome reference over which coverage will be evaluated. | . | MEAN_COVERAGE | The mean coverage in bases of the genome territory, after all filters are applied. | . | SD_COVERAGE | The standard deviation of coverage of the genome after all filters are applied. | . | MEDIAN_COVERAGE | The median coverage in bases of the genome territory, after all filters are applied. | . | MAD_COVERAGE | The median absolute deviation of coverage of the genome after all filters are applied. | . | PCT_EXC_ADAPTER | The fraction of aligned bases that were filtered out because they were in reads with mapping quality 0 and looked like adapter reads. | . | PCT_EXC_MAPQ | The fraction of aligned bases that were filtered out because they were in reads with low mapping quality (lower than MIN_MAPPING_QUALITY). | . | PCT_EXC_DUPE | The fraction of aligned bases that were filtered out because they were in reads marked as duplicates. | . | PCT_EXC_UNPAIRED | The fraction of aligned bases that were filtered out because they were in reads without a mapped mate pair. | . | PCT_EXC_BASEQ | The fraction of aligned bases that were filtered out because they were of low base quality (lower than MIN_BASE_QUALITY). | . | PCT_EXC_OVERLAP | The fraction of aligned bases that were filtered out because they were the second observation from an insert with overlapping reads. | . | PCT_EXC_CAPPED | The fraction of aligned bases that were filtered out because they would have raised coverage above COVERAGE_CAP. | . | PCT_EXC_TOTAL | The total fraction of aligned bases excluded due to all filters. | . | PCT_1X | The fraction of bases that attained at least 1X sequence coverage in post-filtering bases. | . | PCT_5X | The fraction of bases that attained at least 5X sequence coverage in post-filtering bases. | . | PCT_10X | The fraction of bases that attained at least 10X sequence coverage in post-filtering bases. | . | PCT_15X | The fraction of bases that attained at least 15X sequence coverage in post-filtering bases. | . | PCT_20X | The fraction of bases that attained at least 20X sequence coverage in post-filtering bases. | . | PCT_25X | The fraction of bases that attained at least 25X sequence coverage in post-filtering bases. | . | PCT_30X | The fraction of bases that attained at least 30X sequence coverage in post-filtering bases. | . | PCT_40X | The fraction of bases that attained at least 40X sequence coverage in post-filtering bases. | . | PCT_50X | The fraction of bases that attained at least 50X sequence coverage in post-filtering bases. | . | PCT_60X | The fraction of bases that attained at least 60X sequence coverage in post-filtering bases. | . | PCT_70X | The fraction of bases that attained at least 70X sequence coverage in post-filtering bases. | . | PCT_80X | The fraction of bases that attained at least 80X sequence coverage in post-filtering bases. | . | PCT_90X | The fraction of bases that attained at least 90X sequence coverage in post-filtering bases. | . | PCT_100X | The fraction of bases that attained at least 100X sequence coverage in post-filtering bases. | . | FOLD_80_BASE_PENALTY | The fold over-coverage necessary to raise 80% of bases to the mean coverage level. | . | FOLD_90_BASE_PENALTY | The fold over-coverage necessary to raise 90% of bases to the mean coverage level. | . | FOLD_95_BASE_PENALTY | The fold over-coverage necessary to raise 95% of bases to the mean coverage level. | . | HET_SNP_SENSITIVITY | The theoretical HET SNP sensitivity. | . | HET_SNP_Q | The Phred Scaled Q Score of the theoretical HET SNP sensitivity. | . ",
    "url": "/pages/metrics_collectwgsmetrics.html#collectwgsmetrics-picard",
    
    "relUrl": "/pages/metrics_collectwgsmetrics.html#collectwgsmetrics-picard"
  },"323": {
    "doc": "Metrics CollectWgsMetrics",
    "title": "Metrics CollectWgsMetrics",
    "content": "Last update: 20241217 . ",
    "url": "/pages/metrics_collectwgsmetrics.html",
    
    "relUrl": "/pages/metrics_collectwgsmetrics.html"
  },"324": {
    "doc": "Multiblock data fusion",
    "title": "Multiblock Data Fusion in Statistics and Machine Learning",
    "content": "Last update: 20250108 . | Multiblock Data Fusion in Statistics and Machine Learning . | Unsupervised Methods in Selected Methods for Unsupervised and Supervised Topologies (Chapter 5) . | Shared Variable Mode | Shared Sample Mode | Advanced Topics | Framework and Methodological Considerations | Concluding Thoughts | . | Alternative Unsupervised Methods (chapter 9) . | 9.i General Introduction | 9.ii Relationship to the General Framework | 9.1 Shared Variable Mode . | Generalised SVD | . | . | 9.2 Shared Sample Mode . | 9.2.1 Only Common Variation . | 9.2.1.1 DIABLO | 9.2.1.2 Generalised Coupled Tensor Factorisation | 9.2.1.3 Representation Matrices | 9.2.1.4 Extended PCA | . | 9.2.2 Common, Local, and Distinct Variation . | 9.2.2.1 Generalised SVD | 9.2.2.2 Structural Learning and Integrative Decomposition | 9.2.2.3 Bayesian Inter-battery Factor Analysis | 9.2.2.4 Group Factor Analysis | 9.2.2.5 OnPLS | 9.2.2.6 Generalised Association Study | 9.2.2.7 Multi-Omics Factor Analysis | . | 9.3 Two Shared Modes and Only Common Variation . | 9.3.1 Generalised Procrustes Analysis (GPA) | 9.3.2 Three-way Methods | . | 9.4 Conclusions and Recommendations | 9.4.1 Open Issues | . | Decision trees | Method list | . | . This text is largely summarised directly from the following source. Please see the original for details and credit. This is a review of selected topics from the textbook: “Multiblock Data Fusion in Statistics and Machine Learning” 2022. Age K. Smilde, Tormod Næs, Kristian Hovde Liland. https://onlinelibrary.wiley.com/doi/book/10.1002/9781119600978 . ",
    "url": "/pages/multiblock_data_fusion.html#multiblock-data-fusion-in-statistics-and-machine-learning",
    
    "relUrl": "/pages/multiblock_data_fusion.html#multiblock-data-fusion-in-statistics-and-machine-learning"
  },"325": {
    "doc": "Multiblock data fusion",
    "title": "Unsupervised Methods in Selected Methods for Unsupervised and Supervised Topologies (Chapter 5)",
    "content": "Chapter 5 delves into various unsupervised methods used in multiblock data analysis, focusing primarily on shared variable and shared sample modes while also exploring different variations such as only common variation and common, local, and distinct variations. Shared Variable Mode . The chapter begins by examining methods in the shared variable mode, with a specific focus on isolating only common variations such as Simultaneous Component Analysis (SCA), which integrates data from multiple sources to find common patterns. It also addresses more complex structures that include both common and distinct variations, employing methods like Distinct and Common Components Analysis and Multivariate Curve Resolution to dissect the unique and shared signals within the data. Shared Sample Mode . In the shared sample mode, the discussion shifts towards methods designed to handle data structures where samples are shared across different datasets but the variables may differ. Techniques like SUM-PCA and Multiple Factor Analysis provide tools for extracting common information across these shared samples. The chapter further explores the integration of these methods with statistical techniques like Generalised Canonical Analysis and Regularised Generalised Canonical Correlation Analysis, enhancing their ability to deal with various data complexities including high-dimensionality and under-determined systems. Exponential Family SCA and Optimal-scaling are discussed for their ability to adapt the component analysis to different scales and data distributions. Advanced Topics . The latter sections of the chapter address more advanced topics, such as Joint and Individual Variation Explained (JIVE) for parsing out distinct and shared information across multiple datasets, and Advanced Coupled Matrix and Tensor Factorisation, which extend these concepts into higher-dimensional data. Penalised-ESCA and further Multivariate Curve Resolution approaches are introduced for dealing with extremely heterogeneous data. Framework and Methodological Considerations . A generic framework for simultaneous unsupervised methods is outlined, providing a structured approach to applying these diverse methodologies across different types of multiblock data scenarios. This framework helps in identifying the appropriate methodological adjustments needed for specific data characteristics and analysis goals. Concluding Thoughts . The chapter concludes with a synthesis of the discussed methods, offering recommendations based on the complexities and specific requirements of various data analysis scenarios. Open issues in the field are highlighted, pointing to areas where further research and development are needed to advance the capabilities of unsupervised multiblock data analysis. ",
    "url": "/pages/multiblock_data_fusion.html#unsupervised-methods-in-selected-methods-for-unsupervised-and-supervised-topologies-chapter-5",
    
    "relUrl": "/pages/multiblock_data_fusion.html#unsupervised-methods-in-selected-methods-for-unsupervised-and-supervised-topologies-chapter-5"
  },"326": {
    "doc": "Multiblock data fusion",
    "title": "Alternative Unsupervised Methods (chapter 9)",
    "content": "9.i General Introduction . This chapter explores some unsupervised methods that are less commonly used than those highlighted in Chapter 5. This does not necessarily indicate that they are less useful, but rather that some are relatively new and their full potential is still being evaluated. We will cover methods that focus on identifying common variation as well as those that can differentiate between common, local, and distinct variations. The methods discussed will apply to data situations with a shared variable mode, a shared sample mode, and scenarios where both modes are shared. We will examine approaches suitable for both homogeneous and heterogeneous data fusion. Throughout this chapter, we will assume that data blocks are column-centred unless stated otherwise, and the default norm used for vectors and matrices will be the Frobenius norm. 9.ii Relationship to the General Framework . This section provides a brief overview of the unsupervised methods discussed in this chapter as outlined in Table 10.1. The table illustrates a diverse range of methods based on different foundational principles. Most methods operate simultaneously and are model-based, including those based on factor analysis models that incorporate penalties (such as BIBFA, GFA, MOFA, and GAS) and are capable of handling heterogeneous data. The representation method (RM) provides a unique approach through three-way models of specially constructed data representations. Other methods discussed include generalizations of SVD (GSVD) and methods based on copulas (XPCA). This diversity showcases the rich variety of data science approaches available to address similar problems across different fields. 9.1 Shared Variable Mode . In scenarios involving a shared variable mode, we explore methods that can distinguish between common and distinct components. One such method is the generalized singular value decomposition (GSVD), also known as quotient SVD (QSVD), which has been developed over the years (Van Loan, 1976; Paige and Saunders, 1981; De Moor and Zha, 1991). GSVD is an eigenvalue-based technique for separating common from distinct components in shared variable contexts. Generalised SVD . For two data blocks, the GSVD model is formulated as follows: \\(X_1 = X̃_1 + E_1 = T_1 D_1 P^T + E_1\\) \\(X_2 = X̃_2 + E_2 = T_2 D_2 P^T + E_2\\) Here, (X̃_m) represents the data filtered through an SCA model applied to the concatenated matrix ([X = [X_1^T|X_2^T]^T]). The matrices (T_m) are orthonormal (i.e., (T_m^T T_m = I)), and (P) is a full-rank matrix of common loadings, which are not necessarily orthonormal. The matrix (D_m) is diagonal, and the constraint (D_1 + D_2 = I) allows the generalised singular values to be categorized into three groups based on their significance in each block, which aids in distinguishing between common and distinct components. The model structure is such that: \\(X_1 = T_{11} D_{11} P_{t1} + T_{12} D_{12} P_{t2} + T_{13} D_{13} P_{t3} + E_1\\) \\(X_2 = T_{21} D_{21} P_{t1} + T_{22} D_{22} P_{t2} + T_{23} D_{23} P_{t3} + E_2\\) The matrices (D_{11}) and (D_{22}) represent distinct components for (X_1) and (X_2), respectively, while (D_{13}) and (D_{23}) indicate the common components. Ideally, the matrices (D_{12}) and (D_{21}) should contain small values, indicating minimal ‘spill-over’ between the distinct and common components. This method has been applied in various fields, including gene-expression analysis, and has been extended to handle more than two data blocks, allowing for the detection of common, local, and distinct components across multiple blocks. However, determining these components in the context of multiple blocks is complex and requires careful analysis. ",
    "url": "/pages/multiblock_data_fusion.html#alternative-unsupervised-methods-chapter-9",
    
    "relUrl": "/pages/multiblock_data_fusion.html#alternative-unsupervised-methods-chapter-9"
  },"327": {
    "doc": "Multiblock data fusion",
    "title": "9.2 Shared Sample Mode",
    "content": "9.2.1 Only Common Variation . 9.2.1.1 DIABLO . A method used in bioinformatics for multiblock data analysis is called Data Integration Analysis for Biomarker discovery using a Latent component method for Omics studies, DIABLO for short. This method relies strongly on the RGCCA method (see Section 5.2.1.4) and has been implemented in a sparse version (Singh et al., 2019). The model formulation is: \\(\\max \\sum_{m,m'=1}^M c_{m,m'} \\text{corr}(X_m w_m, X_{m'} w_{m'}) \\text{ s.t. } \\|w_m\\|_2 = 1, \\|w_{m'}\\|_1 &lt; \\lambda_{m'}\\) where (c_{m,m’}) indicates connected blocks, components (t_m = X_m w_m) are calculated, (\\lambda_m &gt; 0) are user-set penalties, ensuring ( |w_{m’}|1 &lt; \\lambda{m’} ) as a lasso-type penalty. Deflation is performed by ( X_{m,\\text{new}} = X_m - t_m w_{m’}^T ). This model, similar to RGCCA, uses deflation not trivial to apply (also see Section 2.8). An example of DIABLO is given in Elaboration 9.1, demonstrating a maximization of correlations across three data blocks, generalizing canonical correlation to multiple blocks. 9.2.1.2 Generalised Coupled Tensor Factorisation . Generalised coupled tensor factorisation (GCTF) (Yılmaz et al., 2011) extends coupled matrix tensor factorisation (CMTF) to heterogeneous data using exponential dispersion models (Jørgensen, 1992). It involves minimizing: \\(\\min \\sum_{m=1}^M v_m d_m [X_m - X_m(\\theta, \\theta_m)]\\) with (d_m) as a divergence measure, (\\theta) and (\\theta_m) as common and block-specific parameters, respectively, and (v_m) as weights. This method is applicable in a Bayesian framework for learning weights (Şimşekli et al., 2013), representing a generalization of GSCA and ESCA models (see Section 5.2.1.5). 9.2.1.3 Representation Matrices . Representation matrices encode variables differently in multivariate analysis, useful for handling diverse data types and measurement scales. Originally termed quantification matrices, these matrices facilitate variable associations analysis. Representation matrices for ratio-, interval-, and ordinal-scaled variables transform variable measurements into forms suitable for generating familiar associations like Pearson and Spearman correlations using: \\(q_{jk} = \\frac{2 \\text{tr}(S^T_j S_k)}{\\text{tr}(S^T_j S_j) + \\text{tr}(S^T_k S_k)}\\) For nominal-scaled variables, square matrices based on indicator matrices are used, particularly effective for categorical data, giving rise to known correlation measures like T2 coefficient (Tschuprow, 1939). 9.2.1.4 Extended PCA . Extended PCA (XPCA) (Anderson-Bergman et al., 2018) employs copulas to handle J-dimensional distributions comprising diverse marginal distributions using a Gaussian copula for PCA. This method integrates empirical distributions estimated from heterogeneous data, offering potential applications in natural and life sciences. 9.2.2 Common, Local, and Distinct Variation . For a shared sample mode, several methods exist to separate common, local, and distinct components, applicable to both homogeneous and heterogeneous data. 9.2.2.1 Generalised SVD . Generalised SVD (GSVD) extends the approach used in shared variable modes to shared sample modes by focusing on consensus scores (T) rather than loadings. For two data blocks X1 (I × J1) and X2 (I × J2): \\(X1 = \\tilde{X}_1 + E1 = TD1P^T_1 + E1\\) \\(X2 = \\tilde{X}_2 + E2 = TD2P^T_2 + E2\\) where (\\tilde{X}_m) are SCA-filtered data, T is a full-rank matrix, (P^T_m P_m = I), and (D_1 + D_2 = I). The elements of (D_m) categorize into distinct parts for X1 and X2, and a common part reflected in both X1 and X2 through consensus scores (T1, T2, T3) and respective loadings. An extension of GSVD for more than two data blocks exists (Ponnapalli et al., 2011) but integrating local components remains complex, discussed in Section 11.5.2. 9.2.2.2 Structural Learning and Integrative Decomposition . Structural Learning and Integrative Decomposition (SLIDE) (Gaynanova and Li, 2019), similar to PESCA, imposes group penalties on loading matrices to discover the structure of common, local, and distinct components across data blocks. Starting with an SCA model: \\(X1 = TP^T_1 + E1, X2 = TP^T_2 + E2, X3 = TP^T_3 + E3\\) After fitting, loading matrices might show patterns such as: \\(P1 = [xx00xx], P2 = [xxx000], P3 = [xxxx00]\\) indicating components’ relevance across blocks: first two are common, third is local between blocks 2 and 3, and the last are distinct. SLIDE determines the structure of components through penalisation, allowing a re-fit with these identified structures. The method’s uniqueness relies on the same principles as JIVE (Section 5.2.2.1), with variations in group penalties proposed (Van Deun et al., 2011). 9.2.2.3 Bayesian Inter-battery Factor Analysis . Bayesian Inter-battery Factor Analysis (BIBFA) (Klami et al., 2013) builds on Tucker’s inter-battery factor analysis (1958) and its probabilistic counterpart (Browne, 1979). BIBFA separates the common and distinct parts of two data blocks: \\(t \\sim N(0, I), \\quad t_m \\sim N(0, I), \\quad x_m \\sim N(P_m C t + P_m D t_m, \\Sigma_m)\\) where (P_m) are loadings, (t, t_m) are scores with t representing consensus scores. BIBFA redefines this model in a Bayesian framework with structured priors to determine component numbers and structural models. Its applications include genomics and analytical chemistry (Acar et al., 2015), demonstrating its utility in decomposing complex data into meaningful components, though setting appropriate priors remains crucial for model performance. These methods collectively address the challenges of multiblock data analysis by delineating shared and unique variance components across various data types, enhancing interpretability and applicative value in diverse scientific domains. 9.2.2.4 Group Factor Analysis . Group Factor Analysis (GFA) is a machine learning method that extends Bayesian Inter-battery Factor Analysis (BIBFA). It begins with a factor analysis model: \\(X = TP^T + E\\) Assuming (X) is the concatenation of ([X_1 | \\ldots | X_M]), the GFA model is expressed as: \\([X_1 | \\ldots | X_M] = T[P_1 | \\ldots | P_M]^T + [E_1 | \\ldots | E_M]\\) This setup is similar to the SLIDE model but uses a fully stochastic model where the latent variables (T) are Gaussian distributed, an assumption significant for data with inherent structural relationships, such as experimental designs in the natural and life sciences. GFA allows for independent and normally distributed residuals with potentially differing variances across blocks. It applies Bayesian maximum-likelihood estimation with a group-wise ARD prior for identifying common, local, and distinct components. Reported applications include genomics and fMRI studies (Virtanen et al., 2012). 9.2.2.5 OnPLS . OnPLS, an extension of O2PLS, focuses on identifying common and distinct components across multiple data blocks. Initially developed for two data blocks with shared sample modes, the model for two blocks, (X_1) and (X_2), is structured as: \\(X_1 = T_1C P_{t1C} + T_1D P_{t1D} + E_1 = X_1C + X_1D + E_1\\) \\(X_2 = T_2C P_{t2C} + T_2D P_{t2D} + E_2 = X_2C + X_2D + E_1\\) Where (T_1DP_{t1D}) and (T_2DP_{t2D}) can be seen as distinct components, previously referred to as structural noise. OnPLS generalizes this structure for multiple blocks, effectively parsing data into components that are either shared, local to specific pairs of datasets, or unique to individual datasets. This model has been used in various omics and metabolomics studies, demonstrating its utility in complex data integration scenarios (Löfstedt and Trygg, 2011). 9.2.2.6 Generalised Association Study . The Generalised Association Study (GAS) integrates features from JIVE with extensions to handle heterogeneous data using exponential family distributions. The model posits: \\(\\Theta_1 = 1 \\mu_{t1} + T C P_{t1C} + T_1D P_{t1D}\\) \\(\\Theta_2 = 1 \\mu_{t2} + T C P_{t2C} + T_2D P_{t2D}\\) This model structure is similar to JIVE but includes additional constraints for identifiability and assumes pre-determined dimensionalities for matrices (T_C), (T_1D), and (T_2D). GAS estimates model parameters using a block-descent algorithm within a GLM framework, addressing the distinct challenges of handling data with different distributions (Li and Gaynanova, 2018). 9.2.2.7 Multi-Omics Factor Analysis . Multi-omics Factor Analysis (MOFA) is a Bayesian model that extends BIBFA and GFA to accommodate heterogeneous data from multiple sources, such as different omics technologies. It employs: \\(X_m = T P_{tm} + E_m; \\quad m = 1, \\ldots, M\\) Where (T) includes all latent variables (common, local, distinct). MOFA uses ARD and spike-and-slab priors to enforce different sparsity levels across the factors, suitable for datasets with varying degrees of underlying structure and noise levels. Its complex Bayesian framework requires robust statistical knowledge for effective application. MOFA’s ability to learn the structure of loadings (P) from data distinguishes it from models requiring a priori structure specifications, like DISCO models (Argelaguet et al., 2018). The example provided for MOFA and PESCA illustrates the practical applications of these models in understanding complex relationships in chronic lymphocytic leukaemia data, highlighting the flexibility and depth of insights achievable through advanced multiblock data analysis methods. 9.3 Two Shared Modes and Only Common Variation . When datasets possess two shared modes, special analytical methods are required. This typically involves the same set of samples and variables measured across different occasions, demanding techniques that can address the complexities of repeated measurements. The nature of the data and the specific research questions posed dictate the choice of method. 9.3.1 Generalised Procrustes Analysis (GPA) . In contexts such as sensory analysis where different assessors evaluate the same products using the same sensory characteristics, there is a need to achieve consensus among assessors. Generalised Procrustes Analysis (GPA) is well-suited for this purpose. GPA aims to align the configurations of samples across multiple blocks of data (e.g., different assessors) as closely as possible in terms of translation, dilation, and rotation. Column-centring each data block handles translation, while optimal scaling factors and rotation matrices ((\\lambda_m)s and (Q_m)s) are used to manage dilation and rotation, minimizing the Frobenius norm of the differences between each block and a consensus configuration (V). This consensus can then be analyzed further, typically with principal component analysis (PCA) to understand the variance and similarities across blocks. GPA’s versatility extends beyond sensory analysis to fields like genomics, indicating its broad applicability in multidisciplinary research. 9.3.2 Three-way Methods . For datasets that can be structured into three-dimensional arrays (three-way data), methods like PARAFAC (Parallel Factor Analysis) and Tucker3 models provide powerful tools for decomposition. These methods are particularly relevant for chemical data measured through techniques like excitation-emission fluorescence spectroscopy, where the goal is to estimate underlying chemical concentrations: . | PARAFAC: Simplifies the analysis by decomposing a three-way array into three matrices corresponding to each mode, linked by a Khatri-Rao product. It assumes a simple structure where each component is linked to only one factor per mode, facilitating interpretation but potentially limiting flexibility. | Tucker3: Offers a more general form of three-way decomposition, utilizing a core array that interacts with matrices corresponding to each mode. This method can adapt to more complex variations in data structures, accommodating different numbers of components per mode. | . These three-way methods extend the concept of PCA to multidimensional data, effectively capturing the inherent structure and correlations within such datasets. Their applications range from analytical chemistry to psychometrics and social sciences, reflecting their adaptability and power in extracting meaningful information from complex data structures. 9.4 Conclusions and Recommendations . This chapter presents a variety of unsupervised multiblock methods, ranging from GSVD for shared variables to GPA and three-way methods like PARAFAC and Tucker for datasets with shared samples and variables. The choice of method depends on several factors: . | Shared Modes: Whether the dataset involves shared variables, samples, or both. | Number of Blocks: The complexity and number of data blocks involved. | Data Homogeneity: Whether the data are homogeneous or heterogeneous. | Need for Differentiation: Whether there is a need to differentiate between common, local, and distinct components. | . For instance, if the dataset consists of more than two blocks of homogeneous data and requires parsing out common, local, and distinct components, methods like SLIDE, GFA, and OnPLS are suitable choices. SLIDE and GFA are more similar to each other, focusing on simultaneous analysis, whereas OnPLS adopts a sequential approach to component extraction. 9.4.1 Open Issues . Despite the strengths of the methods discussed, there are several open issues: . | Complexity of Methodology: The advanced statistical techniques, such as Bayesian estimation and the use of penalties, require significant expertise and careful tuning. | Properties of Estimates: The stability and identifiability of the estimated parameters under various conditions remain areas of ongoing research. | . These challenges underscore the need for continued development and refinement in unsupervised multiblock data analysis techniques, ensuring their applicability and effectiveness in addressing complex, real-world data analysis scenarios. ",
    "url": "/pages/multiblock_data_fusion.html#92-shared-sample-mode",
    
    "relUrl": "/pages/multiblock_data_fusion.html#92-shared-sample-mode"
  },"328": {
    "doc": "Multiblock data fusion",
    "title": "Decision trees",
    "content": "Matrix correlation methods . Unsupervised methods for the shared variable mode case and the shared sample mode case . ASCA-based methods . Alternative unsupervised method . Selecting a supervised method . ",
    "url": "/pages/multiblock_data_fusion.html#decision-trees",
    
    "relUrl": "/pages/multiblock_data_fusion.html#decision-trees"
  },"329": {
    "doc": "Multiblock data fusion",
    "title": "Method list",
    "content": "Chapter 5: Unsupervised Methods in Selected Methods for Unsupervised and Supervised Topologies . | Simultaneous Component Analysis (SCA) - No specific citation provided. | Distinct and Common Components Analysis (DCCA) - No specific citation provided. | Multivariate Curve Resolution (MCR) - No specific citation provided. | SUM-PCA - No specific citation provided. | Multiple Factor Analysis (MFA) - No specific citation provided. | Generalised Canonical Analysis (GCA) - No specific citation provided. | Regularised Generalised Canonical Correlation Analysis (RGCCA) - No specific citation provided. | Exponential Family SCA (ESCA) - No specific citation provided. | Optimal-scaling (Optimal-SCA) - No specific citation provided. | Joint and Individual Variation Explained (JIVE) - No specific citation provided. | Advanced Coupled Matrix and Tensor Factorisation (ACMTF) - No specific citation provided. | Penalised-ESCA (P-ESCA) - No specific citation provided. | . Chapter 9: Alternative Unsupervised Methods . | Generalised Singular Value Decomposition (GSVD) also known as Quotient SVD (QSVD) . | Van Loan, 1976; Paige and Saunders, 1981; De Moor and Zha, 1991. | . | Data Integration Analysis for Biomarker discovery using a Latent component method for Omics studies (DIABLO) . | Singh et al., 2019. | . | Generalised Coupled Tensor Factorisation (GCTF) . | Yılmaz et al., 2011; Şimşekli et al., 2013. | . | Extended PCA (XPCA) . | Anderson-Bergman et al., 2018. | . | Structural Learning and Integrative Decomposition (SLIDE) . | Gaynanova and Li, 2019. | . | Bayesian Inter-battery Factor Analysis (BIBFA) . | Klami et al., 2013. | . | Group Factor Analysis (GFA) . | Virtanen et al., 2012. | . | Orthogonal Projections to Latent Structures (OnPLS) . | Löfstedt and Trygg, 2011. | . | Generalised Association Study (GAS) . | Li and Gaynanova, 2018. | . | Multi-Omics Factor Analysis (MOFA) . | Argelaguet et al., 2018. | . | Generalised Procrustes Analysis (GPA) . | No specific citation provided. | . | Parallel Factor Analysis (PARAFAC) . | No specific citation provided. | . | Tucker3 Model . | No specific citation provided. | . | . ",
    "url": "/pages/multiblock_data_fusion.html#method-list",
    
    "relUrl": "/pages/multiblock_data_fusion.html#method-list"
  },"330": {
    "doc": "Multiblock data fusion",
    "title": "Multiblock data fusion",
    "content": " ",
    "url": "/pages/multiblock_data_fusion.html",
    
    "relUrl": "/pages/multiblock_data_fusion.html"
  },"331": {
    "doc": "QV - My voice",
    "title": "My voice",
    "content": "My voice is an optional note you can add when you request your genomic data. It gives you a place to record what matters to you in plain words. You might write why you are interested in your genome, what you hope to learn, details of health or family history, or simply how you would like information explained in the future. Your note may or may not change the scientific analysis; it depends on your preference. It is stored securely with your request and kept as part of your history, so it can be referred to later if needed. Each new request lets you add a fresh note or reuse what you wrote before. Recording your voice ensures that your perspective stays linked with your data. It provides a clear record of your intentions and context, whether your interest is health, ancestry, or long-term storage. ",
    "url": "/pages/my_voice.html#my-voice",
    
    "relUrl": "/pages/my_voice.html#my-voice"
  },"332": {
    "doc": "QV - My voice",
    "title": "Here are examples",
    "content": "(left blank) . “I am healthy but want to store my genome securely for future screening.” . “This request is for family planning. I want to know if I carry anything that could affect children.” . “I am mainly interested in health risks such as cancer or heart disease.” . “There is a strong family history of early heart attacks on my father’s side.” . “My mother and grandmother both had breast cancer. I would like to know if I have inherited risk.” . “Our child has epilepsy. We want to understand if there is a genetic explanation and whether it might affect siblings.” . “This is to support my participation in a research project.” . “I would like to keep a record of treatments that were tried and how effective they were.” . “I want the information explained in plain language first, but I also want a full report I can share with my doctor.” . ",
    "url": "/pages/my_voice.html#here-are-examples",
    
    "relUrl": "/pages/my_voice.html#here-are-examples"
  },"333": {
    "doc": "QV - My voice",
    "title": "QV - My voice",
    "content": "Last update: 2025-08-24 . ",
    "url": "/pages/my_voice.html",
    
    "relUrl": "/pages/my_voice.html"
  },"334": {
    "doc": "Panels disease gene",
    "title": "Panels disease gene",
    "content": "Last update: 20241205 . The main source of our panel data is from Genomics England panelapp https://panelapp.genomicsengland.co.uk. The PanelApp database includes all the gene panels that relate to genomic tests listed in the NHS National Genomic Test Directory, as well as the virtual gene panels that were used in the 100,000 Genomes Project. We explicitly aim to develop methods that avoid the bias of virtual panel analysis. However, the knowledge of known disease-gene mechanisms are a valuable scoring feature in variant classification. For example, ACMG variant classification standards include eight stages which use knowledge of genes with a known mechanism of disease. These apply when classifying evidence related to both pathogenic or benign effects. The affected ACGM criteria are: PVS1, PS1, PM1, PP1, BVS1, BS1, BM1, BP1. A subsequent example is the ACMG classification step, PVS1, which is defined as a “null variant (nonsense, frameshift, canonical +- 2 splice sites, initiation codon, single or multiexon deletion) in a gene where LOF is a known mechanism of disease”. Thus to define a gene as belonging to a known disease mechanism we must use a stable and reliable source of disease-genes. Read more here. ",
    "url": "/pages/panels_disease.html",
    
    "relUrl": "/pages/panels_disease.html"
  },"335": {
    "doc": "Panels disease gene",
    "title": "Current usage",
    "content": ". | Human inborn errors of immunity . | File: 10876_2022_1289_MOESM2_ESM_DLcleaned.tsv | Source: https://lawlessgenomics.com/topic/iuis-iei-table-page via International Union of Immunological Societies (IUIS) Inborn Errors of Immunity Committee (IEI) (https://iuis.org) | Caveat: Cleaning was implemented here | . | Likely inborn error of metabolism . | File: Likely_inborn_error_of_metabolism_targeted_testing_not_possible.tsv | Source: https://panelapp.genomicsengland.co.uk/panels/467/ | Caveat: None | . | Pending: All 451 panels of GE panel app . | File: Not shown | Source: GE panel app via the API | Caveat: Login required - this database has been locally downloaded and processed for our pipelines. This notice will be replaced when it is ready for use. | . | . ",
    "url": "/pages/panels_disease.html#current-usage",
    
    "relUrl": "/pages/panels_disease.html#current-usage"
  },"336": {
    "doc": "PCA biplot 1000genomes",
    "title": "Preparing a public data reference panel from 1000 genomes project for PCA",
    "content": "Last update: 20240807 . | Preparing a public data reference panel from 1000 genomes project for PCA . | Overview | Data Source | Tools and tutorials referenced | 1000 genomes project . | Pedigree data | Population codes | . | PCA eigenvectors | Scripts Overview | Step-by-Step Summary | Conclusion | . | . Figure 1: Principal component analysis (PCA) of 1000 genomes project, reference genome GRCh38, showing population structure. ",
    "url": "/pages/pca_biplot_1kg.html#preparing-a-public-data-reference-panel-from-1000-genomes-project-for-pca",
    
    "relUrl": "/pages/pca_biplot_1kg.html#preparing-a-public-data-reference-panel-from-1000-genomes-project-for-pca"
  },"337": {
    "doc": "PCA biplot 1000genomes",
    "title": "Overview",
    "content": "This documentation outlines the process of preparing a reference panel using the 1000 Genomes Project data, focusing on converting data to PLINK format and performing Principal Component Analysis (PCA). This enables mapping of cohort data to determine population labels and provides a reference for genetic diversity analysis. ",
    "url": "/pages/pca_biplot_1kg.html#overview",
    
    "relUrl": "/pages/pca_biplot_1kg.html#overview"
  },"338": {
    "doc": "PCA biplot 1000genomes",
    "title": "Data Source",
    "content": "The data utilized is derived from the GRCh38 release of the 1000 Genomes Project, specifically from: . | 1000 Genomes Project - GRCh38: | http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ | . ",
    "url": "/pages/pca_biplot_1kg.html#data-source",
    
    "relUrl": "/pages/pca_biplot_1kg.html#data-source"
  },"339": {
    "doc": "PCA biplot 1000genomes",
    "title": "Tools and tutorials referenced",
    "content": ". | Kevin Blighe’s Biostar Tutorial: “Produce PCA bi-plot for 1000 Genomes Phase III - Version 2” available at Biostars https://www.biostars.org/p/335605/. | 1000 genomes project: Homepage https://www.internationalgenome.org | . ",
    "url": "/pages/pca_biplot_1kg.html#tools-and-tutorials-referenced",
    
    "relUrl": "/pages/pca_biplot_1kg.html#tools-and-tutorials-referenced"
  },"340": {
    "doc": "PCA biplot 1000genomes",
    "title": "1000 genomes project",
    "content": "This is a project which provides public WGS data in VCF format and related metadata. The pedigree informaiton comes from 20130606_g1k.ped. We used the GRCh38 liftover data set http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/phase3_liftover_nygc_dir/. The phase 3 variant calls released by the 1000 Genomes project was on GRCh37 reference. To be able to compare them with the variant calls on the high coverage data they had to be lifted over to GRCh38. The liftover was performed at New York Genome Center (NYGC) using CrossMap version 0.5.4. The GRCh37 phase 3 calls used in the liftover are available here, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/. The chain file used in liftover is available at UCSC and can be downloaded from https://hgdownload.cse.ucsc.edu/goldenpath/hg19/liftOver/ . We did not attempt to liftover SVs that were in phase 3. CrossMap does not liftover any records that either had multiple hits on GRCh38 or after liftover the REF allele matches ALT allele. Additionally, we failed any record that was lifted over to a different chromosome or if the REF allele contained symbols (Y, W, Z etc.). [1] Download the files as VCF.gz (and tab-indices). #!/bin/bash prefix=\"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/phase3_liftover_nygc_dir/phase3.chr\" ; suffix=\".GRCh38.GT.crossmap.vcf.gz\" ; for chr in {1..22} X Y; do wget \"${prefix}\"\"${chr}\"\"${suffix}\" \\ \"${prefix}\"\"${chr}\"\"${suffix}\".tbi ; done wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/phase3_liftover_nygc_dir/phase3.crossmap.GRCh38.07302021.README.html wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/phase3_liftover_nygc_dir/phase3.crossmap.GRCh38.07302021.manifest.tsv mkdir 1000genomes mv phase3* 1000genomes/ . [2] Download 1000 Genomes PED file. wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/20130606_g1k.ped . [3] Download (link to the existing copy) of the reference genome. Read here: Reference genome. Pedigree data . | Family.ID | Individual.ID | Paternal.ID | Maternal.ID | Gender | Phenotype | Population | Relationship | . | HG00096 | HG00096 | 0 | 0 | 1 | 0 | GBR | unrel | . | HG00097 | HG00097 | 0 | 0 | 2 | 0 | GBR | unrel | . | HG00099 | HG00099 | 0 | 0 | 2 | 0 | GBR | unrel | . | HG00100 | HG00100 | 0 | 0 | 2 | 0 | GBR | unrel | . | HG00101 | HG00101 | 0 | 0 | 1 | 0 | GBR | unrel | . | HG00102 | HG00102 | 0 | 0 | 2 | 0 | GBR | unrel | . Population codes . | Code | Population Description | Geographic/Ethnic Details | . | CHB | Han Chinese | Han Chinese in Beijing, China | . | JPT | Japanese | Japanese in Tokyo, Japan | . | CHS | Southern Han Chinese | Han Chinese South | . | CDX | Dai Chinese | Chinese Dai in Xishuangbanna, China | . | KHV | Kinh Vietnamese | Kinh in Ho Chi Minh City, Vietnam | . | CHD | Denver Chinese | Chinese in Denver, Colorado (pilot 3 only) | . | CEU | CEPH | Utah residents (CEPH) with Northern and Western European ancestry | . | TSI | Tuscan | Toscani in Italia | . | GBR | British | British in England and Scotland | . | FIN | Finnish | Finnish in Finland | . | IBS | Spanish | Iberian populations in Spain | . | YRI | Yoruba | Yoruba in Ibadan, Nigeria | . | LWK | Luhya | Luhya in Webuye, Kenya | . | GWD | Gambian | Gambian in Western Division, The Gambia | . | MSL | Mende | Mende in Sierra Leone | . | ESN | Esan | Esan in Nigeria | . | ASW | African-American SW | African Ancestry in Southwest US | . | ACB | African-Caribbean | African Caribbean in Barbados | . | MXL | Mexican-American | Mexican Ancestry in Los Angeles, California | . | PUR | Puerto Rican | Puerto Rican in Puerto Rico | . | CLM | Colombian | Colombian in Medellin, Colombia | . | PEL | Peruvian | Peruvian in Lima, Peru | . | GIH | Gujarati | Gujarati Indian in Houston, TX | . | PJL | Punjabi | Punjabi in Lahore, Pakistan | . | BEB | Bengali | Bengali in Bangladesh | . | STU | Sri Lankan | Sri Lankan Tamil in the UK | . | ITU | Indian | Indian Telugu in the UK | . ",
    "url": "/pages/pca_biplot_1kg.html#1000-genomes-project",
    
    "relUrl": "/pages/pca_biplot_1kg.html#1000-genomes-project"
  },"341": {
    "doc": "PCA biplot 1000genomes",
    "title": "PCA eigenvectors",
    "content": "| Individual | PC1 | PC2 | PC3 | PC4 | PC5 | . | HG00096 | -0.01032 | 0.0270 | 0.0117 | 0.0192 | 0.002517 | . | HG00097 | -0.01054 | 0.0275 | 0.0104 | 0.0180 | 0.003890 | . | HG00099 | -0.01067 | 0.0275 | 0.0104 | 0.0168 | 0.001831 | . | HG00100 | -0.00968 | 0.0275 | 0.0109 | 0.0191 | -0.000839 | . | HG00101 | -0.01038 | 0.0270 | 0.0116 | 0.0184 | 0.000796 | . | HG00102 | -0.01063 | 0.0272 | 0.0110 | 0.0178 | 0.003824 | . ",
    "url": "/pages/pca_biplot_1kg.html#pca-eigenvectors",
    
    "relUrl": "/pages/pca_biplot_1kg.html#pca-eigenvectors"
  },"342": {
    "doc": "PCA biplot 1000genomes",
    "title": "Scripts Overview",
    "content": "The process is currently set up in three scripts: . | pca_biplot_1kg.sh, pca_biplot_1kg_part2.sh, pca_biplot_1kg_part3_ggplot.R | 1: Parallel processing (per chromosome) for the conversion of 1000 Genomes VCF files to BCF and subsequently to PLINK format. The script handles data normalization, ID reformatting, and variant pruning to reduce linkage disequilibrium, enhancing the quality of genetic association analyses. | 2: Continues from Script 1, merging PLINK files across chromosomes into a single dataset, performing PCA to explore population structure. | 3: Uses ggplot2 in R to visualize the PCA results, highlighting the population stratification among global populations. This script also integrates demographic data to color-code populations in the bi-plot, providing clear visual insights into genetic diversity. | . ",
    "url": "/pages/pca_biplot_1kg.html#scripts-overview",
    
    "relUrl": "/pages/pca_biplot_1kg.html#scripts-overview"
  },"343": {
    "doc": "PCA biplot 1000genomes",
    "title": "Step-by-Step Summary",
    "content": ". | Data conversion and normalisation . | Convert original VCF files to BCF using bcftools, ensuring all variants have unique IDs based on chromosome positions. | Annotate and reformat using bcftools annotate to adjust variant IDs. | Normalise and remove duplicate variants to clean the dataset for further analysis. | . | PLINK file preparation . | Convert BCF files to PLINK format, ensuring allele orders are maintained. | Prune variants using PLINK to reduce the dataset based on minor allele frequency and linkage disequilibrium. | . | Data merging and PCA analysis . | Merge all chromosome-specific PLINK files into a single dataset. | Perform PCA to identify principal components that explain the maximum variance, indicative of population stratification. | . | Visualisation and interpretation . | Use ggplot2 and patchwork in R to create bi-plots of the first few principal components. | Overlay population data to visually interpret population structure and genetic diversity. | . | . ",
    "url": "/pages/pca_biplot_1kg.html#step-by-step-summary",
    
    "relUrl": "/pages/pca_biplot_1kg.html#step-by-step-summary"
  },"344": {
    "doc": "PCA biplot 1000genomes",
    "title": "Conclusion",
    "content": "This reference panel and PCA analysis provide a framework for understanding genetic diversity and population structure within global populations, using the latest 1000 Genomes Project data. This methodological approach is essential for genomic studies requiring a comprehensive understanding of genetic backgrounds. ",
    "url": "/pages/pca_biplot_1kg.html#conclusion",
    
    "relUrl": "/pages/pca_biplot_1kg.html#conclusion"
  },"345": {
    "doc": "PCA biplot 1000genomes",
    "title": "PCA biplot 1000genomes",
    "content": " ",
    "url": "/pages/pca_biplot_1kg.html",
    
    "relUrl": "/pages/pca_biplot_1kg.html"
  },"346": {
    "doc": "PCA features",
    "title": "Understanding the role and features of PCA in genetic analysis",
    "content": "Last update: 20250106 . ",
    "url": "/pages/pca_features.html#understanding-the-role-and-features-of-pca-in-genetic-analysis",
    
    "relUrl": "/pages/pca_features.html#understanding-the-role-and-features-of-pca-in-genetic-analysis"
  },"347": {
    "doc": "PCA features",
    "title": "PCA in genomic analysis",
    "content": "For example of PCA correcting for population structure in GWAS see https://cloufield.github.io/GWASTutorial/05_PCA/. ",
    "url": "/pages/pca_features.html#pca-in-genomic-analysis",
    
    "relUrl": "/pages/pca_features.html#pca-in-genomic-analysis"
  },"348": {
    "doc": "PCA features",
    "title": "Haplotype blocks",
    "content": "Consider how DNA is inherited from parents: during recombination, each chromosome in the nucleus - whether autosomal (chr1-22), sex (X, Y), or mitochondrial (Mt) - receives one DNA strand from each parent. These strands then break and exchange segments, resulting in a mix of parental DNA in the offspring, yet maintaining two copies of each chromosome. Figure 1 and 2: Guy Drouin, Université d’Ottawa, for ACFAS.ca magazine. As illustrated in Figure 1, each generation sees a number of “crossings” between our various heritages, at different points. In this way, the intact or “non-recombining” pieces - the famous haplotypes - become smaller and smaller with each generation. For example, figure 2 shows that the second copy of chromosome 2 contains an abnormally long haplotype. It is therefore a much more recent piece of chromosome than the others. In fact, this abnormally long haplotype represents a piece of chromosome that contains a useful variant. It enabled the person with this variant to survive better, and this person passed this variant on to his or her descendants. The more favorable a variant, the faster it spreads through the population. ",
    "url": "/pages/pca_features.html#haplotype-blocks",
    
    "relUrl": "/pages/pca_features.html#haplotype-blocks"
  },"349": {
    "doc": "PCA features",
    "title": "Haplotype blocks and genetic analysis",
    "content": "This recombination process creates long stretches of DNA, known as haplotype blocks, which are identical to those found in one parent or the other. By identifying a variant in a parent, one can track the same genetic block in the child, which will include the variant. This approach underlies methods like cheap genotyping and genome-wide association studies (GWAS), which focus on these blocks rather than individual genetic variants. ",
    "url": "/pages/pca_features.html#haplotype-blocks-and-genetic-analysis",
    
    "relUrl": "/pages/pca_features.html#haplotype-blocks-and-genetic-analysis"
  },"350": {
    "doc": "PCA features",
    "title": "The Function of PCA",
    "content": "As human populations have migrated and diversified, these haplotype blocks have mixed differently across regions, reflecting historical patterns of human movement. Principal Component Analysis (PCA) captures these differences in haplotype block distributions, correlating them with ancestral geographic origins. This is crucial for adjusting genetic studies for population stratification, ensuring that associations found are due to genetics and not population bias. Figure 3, from: Wang C, Zöllner S, Rosenberg NA (2012) A Quantitative Comparison of the Similarity between Genes and Geography in Worldwide Human Populations. PLOS Genetics 8(8): e1002886. https://doi.org/10.1371/journal.pgen.1002886 . ",
    "url": "/pages/pca_features.html#the-function-of-pca",
    
    "relUrl": "/pages/pca_features.html#the-function-of-pca"
  },"351": {
    "doc": "PCA features",
    "title": "Expectations in specific genetic conditions",
    "content": "For conditions like cystic fibrosis (deficiency in cystic fibrosis transmembrane conductance regulator, CFTR), where the most common known causes of disease are due to a variant that is inherited rather than spontaneous, PCA is expected reveal clustering within certain population groups, attributed to shared ancestral lineages. By default, cases with a shared variant are likely to cluster together in population groups because their DNA recombinations will have shared lineages (including the variant of interest AND all of the other blocks of the genome). Otherwise, they would not have inherited the block of DNA containing this variant. This is opposed to spontaneous variants, which appear independently of ancestral haplotypes and are unrelated to the distribution patterns PCA might reveal. ",
    "url": "/pages/pca_features.html#expectations-in-specific-genetic-conditions",
    
    "relUrl": "/pages/pca_features.html#expectations-in-specific-genetic-conditions"
  },"352": {
    "doc": "PCA features",
    "title": "Comparison  with viral genetics",
    "content": "Human genetics is generally more complex than viral genomes which depend on asexual reproduction. However, parallels can be drawn with viral genetics, where tracking variants is often more straightforward due to less genetic diversity compared to humans. This screenshot from Nextstrain Viral Epidemiology shows the clade of viruses across their genetic tree and physical location. In this case, the complication of recombination is removed and we simply have direct clonal descent and some spontaneous variants occuring. Figure 4: Screenshot of Nexstrain - Genomic epidemiology of mpox viruses across clades Data updated 2025-01-04. Enabled by data from GenBank. Showing 549 of 549 genomes. ",
    "url": "/pages/pca_features.html#comparison--with-viral-genetics",
    
    "relUrl": "/pages/pca_features.html#comparison--with-viral-genetics"
  },"353": {
    "doc": "PCA features",
    "title": "Analysis for unknown variants",
    "content": "Generally, association testing is focused on finding some shared genetic feature among affected patients. PCA is used to remove the background noise of benign haplotype blocks which are shared by inheritance. Some unique patterns belong to different population groups which would otherwise appear as if they are related to disease. After correcting for population structure with PCA, we hope to ignore those false positives and only identify variants that are truly associated (i.e. potentially causal) with disease. Our intention is that this association will subsequently give was to causal inference with additional analysis. ",
    "url": "/pages/pca_features.html#analysis-for-unknown-variants",
    
    "relUrl": "/pages/pca_features.html#analysis-for-unknown-variants"
  },"354": {
    "doc": "PCA features",
    "title": "Sibling and related samples",
    "content": "The strategy for keeping a single sibling, or other approaches, should be carefully assessed for an analysis. Closely related individuals, such as siblings will be seen to cluster closely together due to their very closely shared ancestry. Any haplotype block that one sibling carries, is likely to be present for the second sibling also. In a statistical analysis, if they method relies on detecting enriched features, such as the haplotype block in the case of GWAS, then the signal will be fasly amplified since it doubled, by default, for siblings. Typically in GWAS and other similar analysis it is important to count only one sibiling from a family in the analysis cohort. KING software (Kinship-based INference for Gwas) is often used for estimating kinship coefficients and inferring IBD segments for all pairwise relationships. This can also use the same input Plink format data as the PCA steps with Plink. https://www.kingrelatedness.com/manual.shtml . The following explanation example from KING is pertinent - even if it is not required for an analysis - which we quote here: . KINSHIP INFERENCE –kinship estimates pair-wise kinship coefficients using the KING-Robust algorithm described in the original KING paper. If pedigrees are documented in the .fam file (see LINKAGE format examples), kinship coefficients can be estimated within families. Note if each FID is unique and no pedigrees are provided, then the within-family inference will be skipped. The output for within-family relationship checking using –kinship (saved in file king.kin) will look like this: . FID ID1 ID2 N_SNP Z0 Phi HetHet IBS0 Kinship Error 28 1 2 2359853 0.000 0.2500 0.162 0.0008 0.2459 0 28 1 3 2351257 0.000 0.2500 0.161 0.0008 0.2466 0 28 2 3 2368538 1.000 0.0000 0.120 0.0634 -0.0108 0 117 1 2 2354279 0.000 0.2500 0.163 0.0006 0.2477 0 117 1 3 2358957 0.000 0.2500 0.164 0.0006 0.2490 0 117 2 3 2348875 1.000 0.0000 0.122 0.0616 -0.0017 0 1344 1 12 2372286 0.000 0.2500 0.149 0.0003 0.2480 0 1344 1 13 2370435 0.000 0.2500 0.148 0.0003 0.2465 0 1344 12 13 2374888 1.000 0.0000 0.117 0.0582 0.0003 0 . Each row above provides information for one pair of individuals. The columns are . FID: Family ID for the pair ID1: Individual ID for the first individual of the pair ID2: Individual ID for the second individual of the pair N_SNP: The number of SNPs that do not have missing genotypes in either of the individual Z0: Pr(IBD=0) as specified by the provided pedigree data Phi: Kinship coefficient as specified by the provided pedigree data HetHet: Proportion of SNPs with double heterozygotes (e.g., AG and AG) IBS0: Porportion of SNPs with zero IBS (identical-by-state) (e.g., AA and GG) Kinship: Estimated kinship coefficient from the SNP data Error: Flag indicating differences between the estimated and specified kinship coefficients (1 for error, 0.5 for warning) . The default kinship coefficient estimation only involves the use of SNP data from this pair of individuals, and the inference is robust to population structure. A negative kinship coefficient estimation indicates an unrelated relationship. The reason that a negative kinship coefficient is not set to zero is a very negative value may indicate the population structure between the two individuals. Close relatives can be inferred fairly reliably based on the estimated kinship coefficients as shown in the following simple algorithm: an estimated kinship coefficient range &gt;0.354, [0.177, 0.354], [0.0884, 0.177] and [0.0442, 0.0884] corresponds to duplicate/MZ twin, 1st-degree, 2nd-degree, and 3rd-degree relationships respectively. Relationship inference for more distant relationships is more challenging. A plot of the estimated kinship coefficient against the proportion of zero IBS-sharing is highly recommended. In the absence of population structure, relationship inference can also be carried out using an alternative algorithm through parameter “–homog”. Manichaikul A, Mychaleckyj JC, Rich SS, Daly K, Sale M, Chen WM (2010) Robust relationship inference in genome-wide association studies. Bioinformatics 26(22):2867-2873 Abstract PDF Citations. ",
    "url": "/pages/pca_features.html#sibling-and-related-samples",
    
    "relUrl": "/pages/pca_features.html#sibling-and-related-samples"
  },"355": {
    "doc": "PCA features",
    "title": "Population structure in other omics",
    "content": "In GWAS, PCA is employed to control for population structure by including the top principal components in the analysis. The applicability of this method extends to other omics analsis, including RNAseq differential expression (DE), which quantitatively test for RNAseq abundance, protein abundance, or multi-omic data (mixed DNA, RNA, and protein), especially when the analysis resembles a simple association test similar to Plink’s –assoc. It is both necessary and effective to use PCA in RNAseq DE, for example, to manage population structure. Analogous to its use in GWAS, where PCA corrects for population structure in genetic data, RNAseq-based genetic principal components (RG-PCs) can be computed and employed as covariates in DE analyses. This strategy reduces confounding effects due to population stratification, thereby improving the accuracy and reliability of the results. For more reading see: . | Fachrul et al. (2023). Direct inference and control of genetic population structure from RNA sequencing data. https://www.nature.com/articles/s42003-023-05171-9 | Storey, J. D. et al. Gene-expression variation within and among human populations. Am. J. Hum. Genet. 80, 502–509 (2007). https://doi.org/10.1086%2F512017 | Thami, P. K. &amp; Chimusa, E. R. Population structure and implications on the genetic architecture of HIV-1 phenotypes within Southern Africa. Front. Genet. 10, 905 (2019).https://doi.org/10.3389%2Ffgene.2019.00905 | Li, J., Liu, Y., Kim, T., Min, R. &amp; Zhang, Z. Gene expression variability within and between human populations and implications toward disease susceptibility. PLoS Comput. Biol. 6, e1000910 (2010).https://doi.org/10.1371%2Fjournal.pcbi.1000910 | Jovov, B. et al. Differential gene expression between African American and European American colorectal cancer patients. PLoS ONE 7, e30168 (2012).https://doi.org/10.1371%2Fjournal.pone.0030168 | Price, A. L. et al. Principal components analysis corrects for stratification in genome-wide association studies. Nat. Genet. 38, 904–909 (2006).https://doi.org/10.1038%2Fng1847 | . Lastly, the abstract from the following says why the the answer is not always commonly reported: . | Shiquan Sun, Michelle Hood, Laura Scott, Qinke Peng, Sayan Mukherjee, Jenny Tung, Xiang Zhou, Differential expression analysis for RNAseq using Poisson mixed models, Nucleic Acids Research, Volume 45, Issue 11, 20 June 2017, Page e106, https://doi.org/10.1093/nar/gkx204 | . Identifying differentially expressed (DE) genes from RNA sequencing (RNAseq) studies is among the most common analyses in genomics. However, RNAseq DE analysis presents several statistical and computational challenges, including over-dispersed read counts and, in some settings, sample non-independence. Previous count-based methods rely on simple hierarchical Poisson models (e.g. negative binomial) to model independent over-dispersion, but do not account for sample non-independence due to relatedness, population structure and/or hidden confounders. ",
    "url": "/pages/pca_features.html#population-structure-in-other-omics",
    
    "relUrl": "/pages/pca_features.html#population-structure-in-other-omics"
  },"356": {
    "doc": "PCA features",
    "title": "PCA features",
    "content": " ",
    "url": "/pages/pca_features.html",
    
    "relUrl": "/pages/pca_features.html"
  },"357": {
    "doc": "Platform updates",
    "title": "Platform updates",
    "content": "Last update: 20250509 . | ☆ Minor features | ☆☆ Major features | ☆☆☆ Milestones | 🚀 Publication | . | Type | Version date | Release | . | 🚀 | v2.0 20250509 | Pre-print: Application of qualifying variants for genomic analysis | . | 🚀 | v4.0 20250325 | Pre-print: Quantifying prior probabilities for disease-causing variants reveal the top genetic contributors in inborn errors of immunity | . | 🚀 | v2.0 20250320 | Pre-print: PanelAppRex aggregates disease gene panels and facilitates sophisticated search | . | 🚀 | v1.0 20250317 | Pre-print: Archipelago method for variant set association test statistics | . | ☆☆☆ | v2.0 20250301 | Main dataset DNA SNV and INDEL V2 (internal) data release | . | ☆☆ | v1.0 20250123 | Dante report package live release | . | ☆☆ | v1.0 20250120 | PanelAppRex package live release | . | ☆☆☆ | v1.0 20250102 | Main dataset DNA SNV and INDEL V1 (internal) data release | . | ☆☆ | v1.0 20241220 | PCA SNV and INDEL Design V1 release | . | ☆ | v1.0 20241219 | Multiple QC metrics for WGS 3 | . | ☆☆ | v1.0 20241217 | Statistical Genomics Design V1 release | . | ☆ | v1.0 20241217 | QV SNV and INDEL Design V1 release | . | ☆☆ | v1.0 20241217 | DNA SNV and INDEL Design V1 release | . | ☆ | v1.0 20241217 | Multiple QC metrics for WGS 2 | . | ☆ | v1.0 20241217 | Multiple QC metrics for WGS 1 | . | ☆ | v1.0 20241210 | Reference genome Update | . | ☆☆ | v1.0 20241205 | Panels of all known (GE) disease associated genes implemented | . | ☆☆ | v0.9 20240929 | GWAS implemented | . | ☆ | v0.8 20240827 | Guru System Overview | . | ☆ | v0.7 20240820 | Structural variants (SV) test methods | . | ☆☆ | v0.6 20240807 | PCA biplot analysis with 1KG for ancestry | . | ☆ | v0.5 20230823 | VSAT SetID configuration | . | ☆ | v0.4 20230727 | BWA algorithm implemented for WGS | . | ☆ | v0.4 20230727 | Fastp QC added | . | ☆ | v0.4 20230727 | FASTQ processing added | . | ☆☆ | v0.3 20230531 | Main annotation sources added | . | ☆☆ | v0.2 20230425 | VSAT module to test genomics | . | ☆☆ | v0.2 20230425 | ACAT module to test mulitomics | . | ☆ | v0.1 20220701 | Design Document | . ",
    "url": "/pages/platform_updates.html",
    
    "relUrl": "/pages/platform_updates.html"
  },"358": {
    "doc": "Platform updates",
    "title": "Internal pre-prints",
    "content": ". | archipelago2025lawless https://www.overleaf.com/project/6794adeaf67d79f23962b81a | qv2025lawless https://www.overleaf.com/project/6794ac2a8429fed58bd1cba9 | sph2025lawless https://www.overleaf.com/project/667a7e6cb0ac9a21b9fe9a6b | . ",
    "url": "/pages/platform_updates.html#internal-pre-prints",
    
    "relUrl": "/pages/platform_updates.html#internal-pre-prints"
  },"359": {
    "doc": "Pre-annotation MAF",
    "title": "Pre-Annotation MAF Filtering",
    "content": "Last update: 20241101 . Overview . The Pre-Annotation MAF Filtering step selectively removes variants from VCF files that exceed a specified minor allele frequency threshold. This step is crucial for focusing analyses on rare variants, which are often of particular interest in studies of rare genetic disorders. Implementation . The 13_pre_annotation_MAF.sh script employs vcftools to apply MAF-based filtering to the VCF files prepared in the previous steps. This script is optimized to process multiple chromosomal segments, ensuring that only variants with desired allele frequency characteristics are retained for further analysis. Script Description . Configured for resource-intensive tasks: . | Nodes: 1 | Memory: 30G | CPUs per Task: 4 | Time Limit: 96:00:00 | Job Name: maf_pre_annotation | Job Array: Processes up to 25 chromosomal segments. | . The script sets up the necessary environment and directories, preparing for efficient execution of MAF filtering. Tools Used . | vcftools: This tool is used for applying filters to VCF files based on allele frequency data, effectively removing common variants according to the specified MAF threshold. | . Process Flow . | Input File Checks: . | Verifies the presence of input VCF files to ensure that all necessary data is available for processing. | . | MAF Filtering: . | Applies a filter to exclude variants with a MAF higher than the specified threshold (0.4 in this setup), focusing the dataset on rarer variants. | Recodes the VCF to include only the variants that pass the filtering criteria. | . | Output Compression and Indexing: . | Compresses the filtered VCF files using bgzip and creates indexed files with tabix to facilitate efficient data retrieval and further processing. | . | Cleanup: . | Removes intermediate files to conserve storage space and maintain a clean working environment. | . | . Quality Assurance . This step includes detailed logging of all operations and stringent checks to ensure that filtering is applied correctly and comprehensively. Error handling mechanisms safeguard against potential data processing issues. Conclusion . MAF filtering is a critical component of our variant analysis pipeline, enabling researchers to focus on variants of interest by filtering out common genetic variations. This process not only refines the dataset but also ensures that subsequent analyses, such as variant annotation and association studies, are more targeted and meaningful. ",
    "url": "/pages/pre_anno_maf.html#pre-annotation-maf-filtering",
    
    "relUrl": "/pages/pre_anno_maf.html#pre-annotation-maf-filtering"
  },"360": {
    "doc": "Pre-annotation MAF",
    "title": "Pre-annotation MAF",
    "content": " ",
    "url": "/pages/pre_anno_maf.html",
    
    "relUrl": "/pages/pre_anno_maf.html"
  },"361": {
    "doc": "Pre-annotation processing",
    "title": "Pre-Annotation Processing",
    "content": "Last update: 20241101 . Overview . The Pre-Annotation Processing step refines VCF files prior to detailed annotation. This involves filtering, normalizing, and decomposing variants to ensure that the data fed into the annotation tools is of high quality and structured correctly. Implementation . The 12_pre_annotation_processing.sh script employs a combination of bioinformatics tools including bcftools, GATK, and vt to refine VCF files. This script is designed to handle large datasets efficiently, processing data across multiple chromosomal segments. Script Description . Optimized for high-throughput computing: . | Nodes: 1 | Memory: 30G | CPUs per Task: 4 | Time Limit: 96:00:00 | Job Name: pre_annotation | Job Array: Capable of processing up to 25 chromosome segments in one pass. | . The script starts by establishing the necessary computational environment and directories for input and output data. Tools Used . | bcftools: Used for initial filtering based on quality metrics such as quality score, depth, and genotype quality. | GATK (v4.4.0.0): Utilized for selecting variants based on filtering criteria. | vt: Applied for decomposing and normalizing variants to a canonical form, simplifying subsequent annotation processes. | . Process Flow . | Initial Filtering: . | Filters variants using bcftools based on predefined quality criteria to ensure that only high-quality variants are processed further. | . | Compression and Indexing: . | Compresses the filtered VCF files using bgzip and indexes them with tabix, preparing them for further processing. | . | GATK Selection: . | Selects variants using GATK’s SelectVariants to exclude filtered and non-variant entries, refining the dataset. | . | Normalization and Decomposition: . | Uses vt to decompose multiallelic sites into simpler allelic forms and normalizes them against the reference genome. This step ensures that variants are represented in their simplest form, aiding accurate annotation. | . | Final Compression and Clean-up: . | Compresses and re-indexes the final VCF files for efficient storage and access. | Cleans up intermediate files to free storage space and maintain a tidy workspace. | . | . Quality Assurance . This step includes extensive error handling and logging to ensure that each sub-process is completed successfully. Detailed logs facilitate troubleshooting and ensure reproducibility. Conclusion . Pre-Annotation Processing is crucial for preparing VCF files for detailed annotation. By ensuring that the data is high-quality and properly formatted, this step lays the groundwork for accurate and efficient genomic annotation, which is critical for downstream genomic analyses. ",
    "url": "/pages/pre_annoprocess.html#pre-annotation-processing",
    
    "relUrl": "/pages/pre_annoprocess.html#pre-annotation-processing"
  },"362": {
    "doc": "Pre-annotation processing",
    "title": "Pre-annotation processing",
    "content": " ",
    "url": "/pages/pre_annoprocess.html",
    
    "relUrl": "/pages/pre_annoprocess.html"
  },"363": {
    "doc": "Precision Medicine Unit (PMU)",
    "title": "Overview",
    "content": "The Precision Medicine Unit takes a new style in pediatric healthcare. This whitepaper presents our systematic approach. It departs from the traditional project-based research model, adopting a continuous, structured workflow that ensures consistency, reliability, and reproducibility across all levels of operation. Figure 1: Precision medicine unit overview. The unit needs illustrate the management philosophy for minimal disruption for technical progress. ",
    "url": "/pages/precision_med.html#overview",
    
    "relUrl": "/pages/precision_med.html#overview"
  },"364": {
    "doc": "Precision Medicine Unit (PMU)",
    "title": "Why the PMU is unique",
    "content": "Unlike traditional research projects which often start from scratch, our unit operates on a model of systematic and structured workflow crucial for effective precision medicine. This method ensures that every element of our work - from data collection to clinical application - is interconnected and efficiently managed. Our approach allows us to maintain consistency across various projects, enhancing the reproducibility and reliability of our results. Each project within the unit is a continuation of an evolving system, which is documented and audited for compliance with the latest regulatory standards, particularly the In Vitro Diagnostic Medical Devices Regulation (IVDR). ",
    "url": "/pages/precision_med.html#why-the-pmu-is-unique",
    
    "relUrl": "/pages/precision_med.html#why-the-pmu-is-unique"
  },"365": {
    "doc": "Precision Medicine Unit (PMU)",
    "title": "Why This Matters",
    "content": "The structured approach of our Precision Medicine Unit ensures that all processes, from genomic data analysis to clinical implementation, are not only compliant with regulatory frameworks but also optimised for adaptation to new challenges and innovations. This model is crucial so that every element of our work remains connected so that we can run on the basis of single-source management and everything-as-code. Figure 2. The database relationship network plot visually represents the interconnectedness of various elements within the database, in this case DNAsnake_v1.0. The ability to make this plot demonstrates how different pieces of data are related and how they flow through the analysis pipeline. This plot is made from variables which retrieve values from an SQL database and illustrates how the connectedness is used to audit any element at any stage of our analysis. ",
    "url": "/pages/precision_med.html#why-this-matters",
    
    "relUrl": "/pages/precision_med.html#why-this-matters"
  },"366": {
    "doc": "Precision Medicine Unit (PMU)",
    "title": "Precision Medicine Unit (PMU)",
    "content": "Last update: 20240716 . Download the whitepaper directly here. Or visit the github repository here. ",
    "url": "/pages/precision_med.html",
    
    "relUrl": "/pages/precision_med.html"
  },"367": {
    "doc": "Presentations",
    "title": "Presentations",
    "content": ". | 2023 | . Here are the presentations that have been used previously or prepared pre-emptively for use on short notice. ",
    "url": "/pages/present/presentations.html",
    
    "relUrl": "/pages/present/presentations.html"
  },"368": {
    "doc": "Presentations",
    "title": "2023",
    "content": ". | Plan overview 20230516.pdf | . ",
    "url": "/pages/present/presentations.html#2023",
    
    "relUrl": "/pages/present/presentations.html#2023"
  },"369": {
    "doc": "Read group",
    "title": "Read group",
    "content": "Last update: 20240611 . This concept of “read groups” in sequencing data described here is copied from the GATK documentation by Derek Caetano-Anolles: https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups. You may be intersting in using this while doing aggregate multiplex. Understanding Read Groups in Sequencing Data . Definition: A ‘read group’ is a collection of reads from a single run of a sequencing instrument. In simpler setups where one library preparation from a biological sample is run on a single lane, all reads from that lane constitute a read group. In more complex cases involving multiplexing, each subset of reads from separate library preparations run on the same lane forms distinct read groups. Identification: Read groups are identified by specific tags in the SAM/BAM/CRAM file, defined in the official SAM specification. These tags include: . | ID: Read group identifier, unique per read group, referenced in the read record and often based on the flowcell and lane. | PU: Platform Unit, captures information about the flowcell barcode, lane, and sample barcode. | SM: Sample name, indicating the sample sequenced in the read group. | PL: Platform used, such as ILLUMINA or PACBIO. | LB: DNA preparation library identifier. | . Importance: Proper assignment of these tags is crucial for differentiating samples and mitigating technical artifacts during data processing steps like duplicate marking and base recalibration. Example Usage: . | To view read group information in a BAM file: samtools view -H sample.bam | grep '^@RG' . This command extracts lines starting with @RG from the BAM header, revealing the read group details. | . Example Read Group Fields: . @RG ID:H0164.2 PL:illumina PU:H0164ALXX140820.2 LB:Solexa-272222 SM:NA12878 . | ID: H0164.2 (flowcell and lane) | PU: H0164ALXX140820.2 (flowcell, lane, and sample barcode) | LB: Solexa-272222 (library identifier) | SM: NA12878 (sample name) | PL: illumina (sequencing platform) | . Multi-sample and Multiplexed Example: For a trio of samples (MOM, DAD, KID) with two libraries each (200 bp and 400 bp inserts), and each library sequenced across two lanes, the read group tags in the headers might appear as follows: . | Dad’s Data: @RG ID:FLOWCELL1.LANE1 PL:ILLUMINA LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE2 PL:ILLUMINA LB:LIB-DAD-1 SM:DAD PI:200 @RG ID:FLOWCELL1.LANE3 PL:ILLUMINA LB:LIB-DAD-2 SM:DAD PI:400 @RG ID:FLOWCELL1.LANE4 PL:ILLUMINA LB:LIB-DAD-2 SM:DAD PI:400 . | Mom’s and Kid’s Data similarly detailed. | . ",
    "url": "/pages/read_group.html",
    
    "relUrl": "/pages/read_group.html"
  },"370": {
    "doc": "Read group",
    "title": "An example",
    "content": "While doing alignment with BWA I check that the info is updated like this: . # This could go in variables.sh with more explicite names sm=$(echo ${sample_id} | awk -F '_' '{print $1}') pu=$(zcat ${FILE1} | awk 'NR==1 {split($1,a,\":\"); print a[3] \".\" a[4] \".\" \"'$sm' \"}') lb=$(echo ${sample_id} | awk -F '_' '{print $1 \"_\" $2}') pl=\"NovaSeq6000_WGS_TruSeq\" echo \"ID = ${sample_id}\" echo \"SM = ${sm}\" echo \"PL = ${pl}\" echo \"PU = ${pu}\" echo \"LB = ${lb}\" # Define your read group rg=\"@RG\\tID:${sample_id}\\tSM:${sm}\\tPL:${pl}\\tPU:${pu}\\tLB:${lb}\" echo \"RG = ${rg}\" echo \"starting bwa mem and samtools\" bwa mem \\ ${REF} \\ ${FILE1} \\ ${FILE2} \\ -R $rg \\ -v 1 -M -t 8 |\\ samtools view --threads 8 -O BAM -o ${output_file} # check read group e.g. # samtools view -H AAA073_NGS000033312_NA_S20_L004.bam | grep '^@RG' # remove fq temp files # we can also use logs to see if we have any read group collision which should b e unique . Then in GATK when files are being merged later in BAM format, MarkDuplicatesSpark handles the read group info correctly from each individual sample for a subject. Conclusion: Understanding and correctly implementing read group information is critical for high-quality genomic data processing, helping distinguish between various technical and biological factors that affect sequencing outcomes. ",
    "url": "/pages/read_group.html#an-example",
    
    "relUrl": "/pages/read_group.html#an-example"
  },"371": {
    "doc": "Reference genome",
    "title": "Reference genome",
    "content": "Last update: 20241210 . ",
    "url": "/pages/ref.html",
    
    "relUrl": "/pages/ref.html"
  },"372": {
    "doc": "Reference genome",
    "title": "Share",
    "content": "Reference genome datasets are prepared and stored at: . | /project/data/shared . | Read-only | Datamanager control | Includes: README.md | Includes: ref.sh creation | . | . ",
    "url": "/pages/ref.html#share",
    
    "relUrl": "/pages/ref.html#share"
  },"373": {
    "doc": "Reference genome",
    "title": "Genome reference consortium",
    "content": "Go here for the original source: https://www.ncbi.nlm.nih.gov/grc/human . ",
    "url": "/pages/ref.html#genome-reference-consortium",
    
    "relUrl": "/pages/ref.html#genome-reference-consortium"
  },"374": {
    "doc": "Reference genome",
    "title": "GRCh38",
    "content": "Choice . Reference genome choice is discussed succinctly in many difference places. Therefore, we link other usefull sources. | Heng Li - Which human reference genome to use? | . https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use . | Illumina review | . https://www.illumina.com/science/genomics-research/articles/dragen-demystifying-reference-genomes.html . Our reference genome was donwloaded and installed by ref.sh which does the following: . Installation . | Get local copy wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` . | Get checksum md5 GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz &gt; GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz.md5 . | Transfer to cluster sftp username@cluster cd data/ref put GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz put GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz.md5 put ref.sh . | Preparation | Once downloaded we need the index which is done by bwa index GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz . | . ",
    "url": "/pages/ref.html#grch38",
    
    "relUrl": "/pages/ref.html#grch38"
  },"375": {
    "doc": "Reference genome",
    "title": "Features of GRCh38/hg38",
    "content": "See GATK for info (https://gatk.broadinstitute.org/hc/en-us/articles/360035890951-Human-genome-reference-builds-GRCh38-or-hg38-b37-hg19). Primary assembly: . | Assembled chromosomes for hg38 are chromosomes 1–22 (chr1–chr22), X (chrX), Y (chrY) and Mitochondrial (chrM). | Unlocalized sequences (known to belong on a specific chromosome but with unknown order or orientation) are identified by the _random suffix. | Unplaced sequences (chromosome of origin unknown) are identified by the chrU_ prefix. | . Legacy assemblies: . GRCh37/b37 and Hg19 . For these builds, the primary assembly coordinates are identical for the original release but patch updates were different. In addition, the naming conventions of the references differ, e.g. the use of chr1(in hg19) versus 1 (in b37) to indicate chromosome 1, and chrM vs. MT for the mitochondrial genome. | chr1(in hg19) | 1 (in b37) | . Included decoys were also different. So it is possible to lift-over resources from one to the other, but it should be done using Picard LiftoverVcf with the appropriate chain files. Trying to convert between them just by renaming contigs is a bad idea. And in the case of BAMs, well, the bad news is that if you have a BAM aligned to one reference build but you need the other, you’ll have to re-map the data from scratch. ",
    "url": "/pages/ref.html#features-of-grch38hg38",
    
    "relUrl": "/pages/ref.html#features-of-grch38hg38"
  },"376": {
    "doc": "Reference genome",
    "title": "Other builds",
    "content": "We use GRCh38 but for some old prepared data we must use the existing version with the reference genome used at that time. The mentioned refernce is “human_g1k_v37_decoy_chr.fasta”. There are 4 common “hg19” references, and they are NOT directly interchangeable: https://gatk.broadinstitute.org/hc/en-us/articles/360035890711-GRCh37-hg19-b37-humanG1Kv37-Human-Reference-Discrepancies . ",
    "url": "/pages/ref.html#other-builds",
    
    "relUrl": "/pages/ref.html#other-builds"
  },"377": {
    "doc": "RL finite MDP",
    "title": "Finite Markov Decision Process (MDP)",
    "content": "Last update: 20250322 . In reinforcement learning, the agent interacts with an environment through a well-defined interface. The agent’s goal is to maximise cumulative rewards by selecting actions that influence the state of the environment. Key concepts include: . | Goals: The objectives the agent strives to achieve. | Rewards: Immediate feedback received after performing an action. | Returns: The total accumulated reward over an episode. | Episodes: Sequences of states, actions, and rewards that conclude when a terminal condition (goal achieved or failure) is met. | . These ideas are exemplified in the cart pole balancing problem, where the agent must keep a pole balanced on a moving cart. Here, the environment continuously provides rewards based on the pole’s stability, and an episode ends when the pole falls or the cart leaves a designated area. Figure example 3.4 Cart pole balance metrics and the final result of learning to balance. Moving on to a discrete setting, the gridworld example represents a simple Markov Decision Process (MDP) with a 5×5 grid where each cell is a state. The environment is defined by standard grid dynamics, with a discount factor (γ = 0.9) and a termination criterion based on a small change threshold (1e–6). The agent can move in one of four directions (up, right, down, left), but there are two special states (A and B) that trigger exceptional transitions: . | Special Transitions: . | When in state A (cell at (1,4)), any action leads the agent to state A’ (cell at (1,0)) with a reward of +10. | When in state B (cell at (3,4)), any action leads the agent to state B’ (cell at (3,2)) with a reward of +5. | . | . For other states, moving off the grid results in staying in the same state with a penalty of –1, while valid moves provide zero immediate reward. Two main value functions are computed: . | Uniform Policy Value Function (V): This is computed by averaging the backups (expected returns) over all actions, simulating a uniform random policy. | Optimal Value Function (V*): Here, the Bellman optimality equation is used (taking the maximum over actions) to iteratively compute the optimal state-value function. Alongside V*, the optimal policy (π*) is derived, indicating for each state the action(s) that maximise the expected return. | . The resulting figures illustrate these concepts: . | Figure 3.2 (Combined Uniform Policy and Exceptional Dynamics): . | Left Panel – Exceptional Reward Dynamics: This plot shows the grid with curved red arrows that indicate the special transitions from state A to A’ and from state B to B’. The arrows are annotated with the respective rewards (+10 and +5), highlighting the exceptional dynamics of the MDP. | Right Panel – Uniform State-Value Function: Here, the state-value function computed under a uniform random policy is visualised. Each cell displays its value (printed as text) and is colour-coded, providing a clear picture of how the environment’s dynamics (including the special transitions) propagate values throughout the grid. | . | . Figure 3.2: Grid example with random policy . | Figure 3.5 (Combined Optimal Value and Policy): This composite figure consists of three subplots: . | Exceptional Reward Dynamics: (Same as in Figure 3.2) Reiterates the exceptional transitions to contextualise the optimal calculations. | Optimal State-Value Function (V*): The computed optimal values are displayed in each cell with both numerical annotations and a colour gradient, illustrating the effect of choosing the best possible actions. | Optimal Policy (π*): This plot overlays directional arrows inside each grid cell, representing the optimal action(s) derived from V*. The arrows correspond to the directions (up, right, down, left) that yield the highest expected return. In some cells, multiple arrows appear if several actions are equally optimal. | . | . Figure 3.5: Optimal solutions to the gridworld example Together, these figures provide a comprehensive visualisation of the theory behind reinforcement learning, illustrating both the agent–environment interface (as demonstrated in the pole balancing task) and the formal MDP framework (as implemented in the gridworld example). They demonstrate how rewards and returns are generated in episodes, how value functions are computed, and how optimal decision-making is represented both in value terms and as explicit action choices within the grid. ",
    "url": "/pages/rl_finte_mdp.html#finite-markov-decision-process-mdp",
    
    "relUrl": "/pages/rl_finte_mdp.html#finite-markov-decision-process-mdp"
  },"378": {
    "doc": "RL finite MDP",
    "title": "RL finite MDP",
    "content": " ",
    "url": "/pages/rl_finte_mdp.html",
    
    "relUrl": "/pages/rl_finte_mdp.html"
  },"379": {
    "doc": "SLURM monitoring",
    "title": "Monitoring jobs and node configurations on slurm",
    "content": "Slurm schedules jobs in a fair and orderly manner, taking into account various factors to optimize the use of available resources. If all nodes are fully occupied with long-running jobs, it may not be possible to expedite the scheduling of new jobs. However, such situations are relatively rare, as Slurm’s scheduling algorithms are designed to maximize efficiency and minimize wait times. This ensures that all users get fair access to the resources in a timely manner. To effectively manage and monitor jobs on a Slurm cluster, it’s important to know a few basic commands that provide insights into job status, queue details, and node configurations. Here are two essential commands: . ",
    "url": "/pages/slurm_manager.html#monitoring-jobs-and-node-configurations-on-slurm",
    
    "relUrl": "/pages/slurm_manager.html#monitoring-jobs-and-node-configurations-on-slurm"
  },"380": {
    "doc": "SLURM monitoring",
    "title": "Commands",
    "content": ". | squeue: This command displays information about jobs queued and running. | sinfo: This command shows the status of the partitions and nodes in the cluster. | . ",
    "url": "/pages/slurm_manager.html#commands",
    
    "relUrl": "/pages/slurm_manager.html#commands"
  },"381": {
    "doc": "SLURM monitoring",
    "title": "Understanding squeue uutput",
    "content": "The following table explains the output columns of the squeue command: . | Column | Description | . | JOBID | Unique identifier for each job | . | PARTITION | The partition (queue) the job is running on | . | NAME | The name of the job | . | USER | The username of the job owner | . | ST | The state of the job (R for running, PD for pending) | . | TIME | The time the job has been running or waiting | . | NODES | The number of nodes being used | . | NODELIST(REASON) | The specific nodes being used or the reason if pending | . ",
    "url": "/pages/slurm_manager.html#understanding-squeue-uutput",
    
    "relUrl": "/pages/slurm_manager.html#understanding-squeue-uutput"
  },"382": {
    "doc": "SLURM monitoring",
    "title": "Example squeue output",
    "content": "JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 98467 all-nodes dbnsfp username1 R 7:23 1 tenant-name-slurm-compute-dynamic-13 98458 dynamic-8 sys/dash username1 R 3:06:49 1 tenant-name-slurm-compute-dynamic-01 98464 dynamic-8 sys/dash username2 PD 0:00 1 (Resources) 98460 dynamic-8 sys/dash username2 R 2:18:27 1 tenant-name-slurm-compute-dynamic-04 . ",
    "url": "/pages/slurm_manager.html#example-squeue-output",
    
    "relUrl": "/pages/slurm_manager.html#example-squeue-output"
  },"383": {
    "doc": "SLURM monitoring",
    "title": "Understanding sinfo output",
    "content": "The following table explains the output columns of the sinfo command: . | Column | Description | . | PARTITION | Name of the partition. | . | AVAIL | Availability of the partition (up for available, inact for inactive). | . | TIMELIMIT | Maximum time that jobs can run in this partition (usually infinite). | . | NODES | Number of nodes available or in use in the partition. | . | STATE | Current state of the nodes: | . |   | - alloc: All of the nodes are allocated to jobs. | . |   | - idle~: Nodes are idle with possibly some jobs pending. | . |   | - down: Nodes are down and not available for jobs. | . |   | - mix: Nodes are partially allocated, some resources are free. | . | NODELIST | Specific nodes assigned to the partition. | . ",
    "url": "/pages/slurm_manager.html#understanding-sinfo-output",
    
    "relUrl": "/pages/slurm_manager.html#understanding-sinfo-output"
  },"384": {
    "doc": "SLURM monitoring",
    "title": "Example sinfo Output",
    "content": "PARTITION AVAIL TIMELIMIT NODES STATE NODELIST static inact infinite 1 down tenant-name-slurm-compute-template dynamic-8cores-16g* up infinite 4 alloc tenant-name-slurm-compute-dynamic-[01-04] dynamic-16cores-32g up infinite 4 idle~ tenant-name-slurm-compute-dynamic-[05-08] dynamic-16cores-64g up infinite 4 idle~ tenant-name-slurm-compute-dynamic-[09-12] dynamic-16cores-128g up infinite 1 idle~ tenant-name-slurm-compute-dynamic-14 dynamic-16cores-128g up infinite 1 mix tenant-name-slurm-compute-dynamic-13 dynamic-a100gpu-128cores-900g-4gpus up infinite 1 idle~ tenant-name-slurm-compute-dynamic-gpu-01 all-nodes-cpu up infinite 9 idle~ tenant-name-slurm-compute-dynamic-[05-12,14] all-nodes-cpu up infinite 1 mix tenant-name-slurm-compute-dynamic-13 all-nodes-cpu up infinite 4 alloc tenant-name-slurm-compute-dynamic-[01-04] . By regularly using squeue and sinfo, users can manage their jobs more effectively and plan their resource usage according to the availability and current load of the compute cluster. Some nodes show 32G memory on their partition but will not run jobs that have more than #SBATCH –mem 28G. Keep this in mind for other types of overheadthat might prevent a job from launching. ",
    "url": "/pages/slurm_manager.html#example-sinfo-output",
    
    "relUrl": "/pages/slurm_manager.html#example-sinfo-output"
  },"385": {
    "doc": "SLURM monitoring",
    "title": "Note on resource requests in slurm",
    "content": "When submitting jobs to Slurm, it is crucial to ensure that your resource requests match what is actually available on the system. Requesting resources that exceed the system’s capabilities, such as asking for 500GB of memory on a node that offers significantly less, may lead to your job hanging indefinitely without launching or providing any failure notices. To avoid these issues, please use the sinfo command regularly to verify the available resources and configurations on the cluster. This careful checking can help ensure that your job submissions are compatible with the system’s capabilities, preventing unnecessary delays. In this case, I belive the squeue NODELIST(REASON) will show (Resources). ",
    "url": "/pages/slurm_manager.html#note-on-resource-requests-in-slurm",
    
    "relUrl": "/pages/slurm_manager.html#note-on-resource-requests-in-slurm"
  },"386": {
    "doc": "SLURM monitoring",
    "title": "SLURM monitoring",
    "content": "Last update: 20241120 . ",
    "url": "/pages/slurm_manager.html",
    
    "relUrl": "/pages/slurm_manager.html"
  },"387": {
    "doc": "SLURM sbatch headers",
    "title": "SLURM sbatch headers",
    "content": "Last update: 20240820 . Here we outline the usage of SLURM sbatch headers to efficiently manage jobs that process genomic data across multiple samples, chromosomes, or genomic regions. Some nodes show 32G memory on their partition but will not run jobs that have more than #SBATCH –mem 28G. Keep this in mind for other types of overheadthat might prevent a job from launching. ",
    "url": "/pages/slurm_sbatch.html",
    
    "relUrl": "/pages/slurm_sbatch.html"
  },"388": {
    "doc": "SLURM sbatch headers",
    "title": "Key SLURM sbatch directives",
    "content": "Here are examples of SLURM sbatch headers used in our scripts: . #!/bin/bash #SBATCH --nodes 1 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 4 #SBATCH --mem 30G #SBATCH --time 96:00:00 #SBATCH --job-name=genomic_analysis #SBATCH --output=/path/to/log/%x_%A_%a_%J.out #SBATCH --error=/path/to/log/%x_%A_%a_%J.err #SBATCH --partition=all-nodes-cpu #SBATCH --array=1-1215 . Explanation of directives . | --nodes: Number of nodes required. | --ntasks: Number of tasks. | --cpus-per-task: Number of CPUs per task. | --mem: Memory required. | --time: Time limit. | --job-name: Name of the job. | --output and --error: Output and error file paths. | --partition: Specifies the partition. | --array: Job array settings to split tasks. | . Common output and error placeholders: . | %x: Job name specified by --job-name. | %A: Job ID for the array job. | %a: Array index of the specific task within a job array. | %J: Job ID with an optional array task ID, formatted as jobID_arrayID (or just jobID for non-array jobs). | . Additional placeholders: . | %j: The job ID, used when no array is involved. | %N: Short hostname of the first compute node where the job runs. | %n: Node index relative to the job. | %u: Username of the job owner. | . Examples: . | Track job details: --output=/path/to/log/%x_%A_%a_%J.out | Log by node: --output=/path/to/log/%x_%N.out | User-specific logs: --output=/path/to/log/%u_%x_%j.out | . Best practices: . | Organize Logs: Use structured directories and naming conventions. | Use Unique Identifiers: Include %J or %A_%a to prevent overwrites. | Maintain Privacy: Be cautious about sensitive information in filenames. | . ",
    "url": "/pages/slurm_sbatch.html#key-slurm-sbatch-directives",
    
    "relUrl": "/pages/slurm_sbatch.html#key-slurm-sbatch-directives"
  },"389": {
    "doc": "SLURM sbatch headers",
    "title": "Using SLURM_ARRAY_TASK_ID",
    "content": "The SLURM_ARRAY_TASK_ID is used to assign specific tasks within a job array. Each task can be used to process a specific file or a part of the dataset. Example: Genomic analysis per sample . This example shows how to use SLURM_ARRAY_TASK_ID to process individual genomic samples: . #!/bin/bash #SBATCH --array=0-99 # Adjust based on the number of samples # Assuming BAM_FILES is an array containing paths to BAM files BAM_FILES=(\"/path/to/sample1.bam\" \"/path/to/sample2.bam\" ...) SMOOVE=\"singularity exec ${DATA}/smoove_latest.sif smoove\" EXCLUDE_BED=\"/path/to/exclude.bed\" REF_NONZIP=\"/path/to/reference.fasta\" OUTDIR=\"/path/to/output\" SAMPLE_ID=$(basename ${BAM_FILES[$SLURM_ARRAY_TASK_ID]} .bam) $SMOOVE call --outdir $OUTDIR \\ --exclude $EXCLUDE_BED \\ --name $SAMPLE_ID \\ --fasta $REF_NONZIP \\ -p 1 \\ --genotype ${BAM_FILES[$SLURM_ARRAY_TASK_ID]} . Example: Processing genomic data by chromosome . This example demonstrates setting up an array to process data by chromosome: . #!/bin/bash #SBATCH --array=0-24 # For chromosomes 1..22, X, Y, M declare -a CHROMOSOMES=('1' '2' '3' ... '22' 'X' 'Y' 'M') CHROM=${CHROMOSOMES[$SLURM_ARRAY_TASK_ID]} INPUT_DIR=\"/path/to/vcfs\" OUTPUT_DIR=\"/path/to/output\" VCF_FILE=\"${INPUT_DIR}/chr${CHROM}_data.vcf.gz\" OUTPUT_VCF=\"${OUTPUT_DIR}/chr${CHROM}_processed.vcf.gz\" echo \"Processing chromosome: ${CHROM}\" echo \"Input: ${VCF_FILE}\" # Check if the input file exists if [[ ! -f \"$VCF_FILE\" ]]; then echo \"Input file for chromosome ${CHROM} does not exist: $VCF_FILE\" echo \"Skipping processing for this job.\" exit 0 fi # Run processing commands bcftools filter -i 'QUAL&gt;=30 &amp; INFO/DP&gt;=20' -Oz -o ${OUTPUT_VCF} ${VCF_FILE} . ",
    "url": "/pages/slurm_sbatch.html#using-slurm_array_task_id",
    
    "relUrl": "/pages/slurm_sbatch.html#using-slurm_array_task_id"
  },"390": {
    "doc": "SLURM sbatch headers",
    "title": "Conclusion",
    "content": "Utilizing SLURM sbatch headers and the SLURM_ARRAY_TASK_ID variable efficiently parallelizes tasks across a cluster, enhancing throughput for large-scale genomic analyses. ",
    "url": "/pages/slurm_sbatch.html#conclusion",
    
    "relUrl": "/pages/slurm_sbatch.html#conclusion"
  },"391": {
    "doc": "Stats Analysis of methods",
    "title": "Stats Analysis of methods",
    "content": "Last update: 20230916 . | Stats Analysis of methods . | The fianl comparison results . | Scores per sample and Bland-Altman plot | Correlation test result and repeatability coefficients | . | Code | Summary of the analysis of method comparison studies | . | . Altman, D. G., and J. M. Bland. “Measurement in Medicine: The Analysis of Method Comparison Studies.” Journal of the Royal Statistical Society. Series D (The Statistician), vol. 32, no. 3, 1983, pp. 307–17. JSTOR, https://doi.org/10.2307/2987937. The paper is a pivotal guide discussing the analysis of method comparison studies, particularly in the field of medicine. It proposes a pragmatic approach to analyze such studies, stressing the importance of simplicity especially when the results need to be explained to non-statisticians. I work through it here to compare if a new methods is as good or better than an existing one for clinical sepsis scores with example data. For more similar papers see the series of BMJ statistical notes by Altman &amp; Bland ( lit-altman_bland.md ). ",
    "url": "/pages/stats_altman_bland_analysis_of_methods.html",
    
    "relUrl": "/pages/stats_altman_bland_analysis_of_methods.html"
  },"392": {
    "doc": "Stats Analysis of methods",
    "title": "The fianl comparison results",
    "content": "Scores per sample and Bland-Altman plot . Figure 1. Scores per sample and Bland-Altman plot as produced by the provided R code. Correlation test result and repeatability coefficients . Results of the analysis as produced by the provided R code: . $Repeatability_Score1 [1] 4.674812 $Repeatability_Score2 [1] 4.267645 $Correlation_Test Pearson's product-moment correlation data: df$score1 and df$score2 t = 21.693, df = 197, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.7931229 0.8763442 sample estimates: cor 0.8395928 . Repeatability Scores: . | Repeatability Score 1: 4.674812 | Repeatability Score 2: 4.267645 | . The repeatability scores are measures of how consistent each method is. In this case, both methods seem to have fairly similar repeatability scores, indicating a similar level of consistency or reliability within each method. Without context or a benchmark to compare to, it’s challenging to definitively say whether these scores are good or not, but the similarity suggests comparable repeatability. Correlation Test: . | Pearson’s Correlation Coefficient (r): 0.8395928 | 95% Confidence Interval for r: [0.7931229, 0.8763442] | P-value: \\(&lt; 2.2e-16\\) | . The Pearson’s correlation coefficient is quite high, indicating a strong positive linear relationship between the scores from the two methods. The nearly zero p-value (less than 2.2e-16) strongly suggests that this correlation is statistically significant, and it’s highly unlikely that this observed correlation occurred by chance. Conclusion: . Considering the high correlation coefficient and the comparable repeatability scores, it seems that the new method is quite similar to the old one in terms of both reliability (as indicated by the repeatability scores) and agreement (as indicated by the correlation coefficient). ",
    "url": "/pages/stats_altman_bland_analysis_of_methods.html#the-fianl-comparison-results",
    
    "relUrl": "/pages/stats_altman_bland_analysis_of_methods.html#the-fianl-comparison-results"
  },"393": {
    "doc": "Stats Analysis of methods",
    "title": "Code",
    "content": "# Here is a really cool set of notes in BMJ about all kinds of clinical data analysis. # https://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm # For example, if you every like to go further with the score that you working on, this paper is very famous for showing how to compare two such clinical score methods to show if the new one performs better. # https://sci-hub.hkvisa.net/10.2307/2987937 # (Measurement in Medicine: The Analysis of Method Comparison Studies Author(s): D. G. Altman and J. M. Bland 1983) # Example ---- # psofa.score - Total number of organ failures according to 2017 pSOFA definitions (Matics et al. (2017) (PMID 28783810)). The classification of organ failures is based on the worst vital signs and the worst lab values during the first 7 days from blood culture sampling. # pelod.score - Total number of organ failures according to PELOD-2 definitions (Leteurtre et al. (2013) (PMID 23685639)). The classification of organ failures is based on the worst vital signs and the worst lab values during the first 7 days from blood culture sampling. library(ggplot2) library(dplyr) library(tidyr) library(patchwork) # Read data df &lt;- read.csv(file = \"../data/example_sepsis_scores.tsv\", sep = \" \") # Take 200 rows as an example df &lt;- df[1:200, ] # Renaming columns and adding index column df$score1 &lt;- df$psofa.score df$score2 &lt;- df$pelod.score df$index &lt;- rownames(df) # Adding a small amount of random noise to each value # NOTE: THIS WOULD BE USED FOR ADDING SOME NOISE TO MAKE THE DATA MORE ANONYMOUS FOR PUBLISHING AN EXAMPLE FIGURE set.seed(123) # Setting a seed to ensure reproducibility df$score1 &lt;- df$score1 + rnorm(nrow(df), mean = 0, sd = 1) df$score2 &lt;- df$score2 + rnorm(nrow(df), mean = 0, sd = 1) # Calculate necessary statistics: the average and the difference of score1 and score2. df$means &lt;- (df$score1 + df$score2) / 2 df$diffs &lt;- df$score1 - df$score2 # Compute repeatability coefficients for each method separately repeatability_score1 &lt;- sd(df$score1, na.rm = TRUE) repeatability_score2 &lt;- sd(df$score2, na.rm = TRUE) # Compute correlation to check independence of repeatability from the size of the measurements cor_test &lt;- cor.test(df$score1, df$score2, method = \"pearson\") # Average difference (aka the bias) bias &lt;- mean(df$diffs, na.rm = TRUE) # Sample standard deviation sd &lt;- sd(df$diffs, na.rm = TRUE) # Limits of agreement upper_loa &lt;- bias + 1.96 * sd lower_loa &lt;- bias - 1.96 * sd # Additional statistics for confidence intervals n &lt;- nrow(df) conf_int &lt;- 0.95 t1 &lt;- qt((1 - conf_int) / 2, df = n - 1) t2 &lt;- qt((conf_int + 1) / 2, df = n - 1) var &lt;- sd^2 se_bias &lt;- sqrt(var / n) se_loas &lt;- sqrt(3 * var / n) upper_loa_ci_lower &lt;- upper_loa + t1 * se_loas upper_loa_ci_upper &lt;- upper_loa + t2 * se_loas bias_ci_lower &lt;- bias + t1 * se_bias bias_ci_upper &lt;- bias + t2 * se_bias lower_loa_ci_lower &lt;- lower_loa + t1 * se_loas lower_loa_ci_upper &lt;- lower_loa + t2 * se_loas # Create Bland-Altman plot p1 &lt;- df |&gt; ggplot(aes(x = means, y = diffs)) + geom_point(color = \"red\", alpha = .5) + ggtitle(\"Bland-Altman plot for PELOD and pSOFA scores\") + ylab(\"Difference between\\ntwo scores\") + xlab(\"Average of two scores\") + theme_bw() + geom_hline(yintercept = c(lower_loa, bias, upper_loa), linetype = \"solid\") + geom_hline(yintercept = c(lower_loa_ci_lower, lower_loa_ci_upper, upper_loa_ci_lower, upper_loa_ci_upper), linetype=\"dotted\") + geom_hline(yintercept = se_bias) # Create scatter plot with a line of equality p2 &lt;- df |&gt; ggplot(aes(x=score1, y=score2)) + geom_point(color = \"red\", alpha = .5) + geom_abline(intercept = 0, slope = 1) + theme_bw() + ggtitle(\"PELOD and pSOFA scores per sample\") + ylab(\"pelod.score\") + xlab(\"psofa.score\") # Combine plots patch1 &lt;- (p2 / p1) + plot_annotation(tag_levels = 'A') patch1 # Output the correlation test result and repeatability coefficients list( Repeatability_Score1 = repeatability_score1, Repeatability_Score2 = repeatability_score2, Correlation_Test = cor_test ) . ",
    "url": "/pages/stats_altman_bland_analysis_of_methods.html#code",
    
    "relUrl": "/pages/stats_altman_bland_analysis_of_methods.html#code"
  },"394": {
    "doc": "Stats Analysis of methods",
    "title": "Summary of the analysis of method comparison studies",
    "content": "In their landmark paper, D.G. Altman and J.M. Bland outline a structured approach to evaluating whether a new method of medical measurement is as good as or better than an existing one. The approach encapsulates several critical components, emphasizing not only statistical analyses but also the importance of effective communication, especially to non-expert audiences. Key points from the paper: . | Bland-Altman Plot . | Introduced as a graphical method to assess the agreement between two different measurement techniques. This method involves plotting the difference between two methods against their mean, which assists in identifying any biases and analyzing the limits of agreement between the two methods. | . | Bias and Limits of Agreement . | The authors recommend calculating the bias (the mean difference between two methods) and limits of agreement (bias ± 1.96 times the standard deviation of the differences) to quantify the agreement between the two methods. A smaller bias and narrower limits of agreement generally indicate that a new method might be comparable or superior to the existing one. | . | Investigating Relationship with Measurement Magnitude . | Encourages the investigation of whether the differences between the methods relate to the measurement’s magnitude. Transformations or regression approaches might be necessary depending on the observed association, to correct for it. | . | Repeatability . | Assessment: It’s crucial to assess repeatability for each method separately using replicated measurements on a sample of subjects. This analysis derives from the within-subject standard deviation of the replicates. | Graphical Methods and Correlation Tests: Apart from Bland-Altman plots, graphical methods (like plotting standard deviation against the mean) and correlation coefficient tests are suggested for examining the independence of repeatability from the size of measurements. | Potential Influences: It highlights the possible influences on measurements, such as observer variability, time of day, or the position of the subject, and differentiates between repeatability and reproducibility (agreement of results under different conditions). | . | Comparison of Methods . | The core emphasis is on directly comparing results obtained by different methods to determine if one can replace another without compromising accuracy for the intended purpose of the measurement. Initial data plotting is encouraged, ideally plotting the difference between methods against the average of the methods, providing insight into disagreement, outliers, and potential trends. Testing for independence between method differences and the size of measurements is necessary, as it can influence the analysis and interpretation of bias and error. | . | Addressing Alternative Analyses . | The paper discusses alternative approaches like least squares regression, principal component analysis, and regression models with errors in both variables, but finds them to generally add complexity without necessarily improving the simple comparison intended. | . | Effective Communication . | The authors emphasize the importance of communicating results effectively to non-experts, such as clinicians, to facilitate practical application of the findings. | . | Challenges in Method Comparison Studies . | The paper highlights the challenges faced in method comparison studies, primarily due to the lack of professional statistical expertise and reliance on incorrect methods replicated from existing literature. It calls for improved awareness among statisticians about this issue and encourages journals to foster the use of appropriate techniques through peer review. | . | . Thus, we can perform an objective evaluation of whether a new measurement method is as good or potentially better than an existing one by assessing agreement, bias, and repeatability, among other factors. ",
    "url": "/pages/stats_altman_bland_analysis_of_methods.html#summary-of-the-analysis-of-method-comparison-studies",
    
    "relUrl": "/pages/stats_altman_bland_analysis_of_methods.html#summary-of-the-analysis-of-method-comparison-studies"
  },"395": {
    "doc": "Stats CI from P",
    "title": "Stats CI from P",
    "content": "Last update: 20230917 . | Stats CI from P . | How to obtain the confidence interval from a P value | (a) Calculating the confidence interval for a difference | (b) Calculating the confidence interval for a ratio (log transformation needed) | Limitations of the method | P values presented as inequalities | References | . | . This text is largely copied directly from the following source while we build an example closer to our needs. Please see the original source as ollows. This topic is from a series of BMJ statistical notes by Altman &amp; Bland. BMJ 2011; 343 doi: https://doi.org/10.1136/bmj.d2090 (Published 08 August 2011) Cite this as: BMJ 2011;343:d2090. Douglas G Altman, professor of statistics in medicine, J Martin Bland, professor of health statistics. ",
    "url": "/pages/stats_altman_bland_ci_from_p.html",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html"
  },"396": {
    "doc": "Stats CI from P",
    "title": "How to obtain the confidence interval from a P value",
    "content": "Confidence intervals (CIs) are widely used in reporting statistical analyses of research data, and are usually considered to be more informative than P values from significance tests.1 2 Some published articles, however, report estimated effects and P values, but do not give CIs (a practice BMJ now strongly discourages). Here we show how to obtain the confidence interval when only the observed effect and the P value were reported. The method is outlined in the box below in which we have distinguished two cases. Steps to obtain the confidence interval (CI) for an estimate of effect from the P value and the estimate (Est) . | (a) CI for a difference . | calculate the test statistic for a normal distribution test, z, from P3: z = −0.862 + √[0.743 − 2.404×log(P)] | calculate the standard error: SE = Est/z (ignoring minus signs) | calculate the 95% CI: Est –1.96×SE to Est + 1.96×SE. | . | (b) CI for a ratio . | For a ratio measure, such as a risk ratio, the above formulas should be used with the estimate Est on the log scale (eg, the log risk ratio). Step 3 gives a CI on the log scale; to derive the CI on the natural scale we need to exponentiate (antilog) Est and its CI.4 | . | . Notes . | All P values are two sided. | All logarithms are natural (ie, to base e). 4 | For a 90% CI, we replace 1.96 by 1.65; for a 99% CI we use 2.57. | . ",
    "url": "/pages/stats_altman_bland_ci_from_p.html#how-to-obtain-the-confidence-interval-from-a-p-value",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html#how-to-obtain-the-confidence-interval-from-a-p-value"
  },"397": {
    "doc": "Stats CI from P",
    "title": "(a) Calculating the confidence interval for a difference",
    "content": "We consider first the analysis comparing two proportions or two means, such as in a randomised trial with a binary outcome or a measurement such as blood pressure. For example, the abstract of a report of a randomised trial included the statement that “more patients in the zinc group than in the control group recovered by two days (49% v 32%, P=0.032).”5 The difference in proportions was Est = 17 percentage points, but what is the 95% confidence interval (CI)? . Following the steps in the box we calculate the CI as follows: . z = –0.862+ √[0.743 – 2.404×log(0.032)] = 2.141; SE = 17/2.141 = 7.940, so that 1.96×SE = 15.56 percentage points; 95% CI is 17.0 – 15.56 to 17.0 + 15.56, or 1.4 to 32.6 percentage points. ",
    "url": "/pages/stats_altman_bland_ci_from_p.html#a-calculating-the-confidence-interval-for-a-difference",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html#a-calculating-the-confidence-interval-for-a-difference"
  },"398": {
    "doc": "Stats CI from P",
    "title": "(b) Calculating the confidence interval for a ratio (log transformation needed)",
    "content": "The calculation is trickier for ratio measures, such as risk ratio, odds ratio, and hazard ratio. We need to log transform the estimate and then reverse the procedure, as described in a previous Statistics Note.6 . For example, the abstract of a report of a cohort study includes the statement that “In those with a [diastolic blood pressure] reading of 95-99 mm Hg the relative risk was 0.30 (P=0.034).”7 What is the confidence interval around 0.30? . Following the steps in the box we calculate the CI as follows: . | z = \\(–0.862+ √[0.743 – 2.404×log(0.034)] = 2.117\\); | Est = \\(log (0.30) = −1.204\\); | SE = −1.204/2.117 = −0.569 but we ignore the minus sign, so SE = 0.569, and 1.96×SE = 1.115; | 95% CI on log scale = −1.204 − 1.115 to −1.204 + 1.115 = −2.319 to −0.089; | 95% CI on natural scale = exp (−2.319) = 0.10 to exp (−0.089) = 0.91. | Hence the relative risk is estimated to be 0.30 with 95% CI 0.10 to 0.91. | . ",
    "url": "/pages/stats_altman_bland_ci_from_p.html#b-calculating-the-confidence-interval-for-a-ratio-log-transformation-needed",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html#b-calculating-the-confidence-interval-for-a-ratio-log-transformation-needed"
  },"399": {
    "doc": "Stats CI from P",
    "title": "Limitations of the method",
    "content": "The methods described can be applied in a wide range of settings, including the results from meta-analysis and regression analyses. The main context where they are not correct is in small samples where the outcome is continuous and the analysis has been done by a t test or analysis of variance, or the outcome is dichotomous and an exact method has been used for the confidence interval. However, even here the methods will be approximately correct in larger studies with, say, 60 patients or more. ",
    "url": "/pages/stats_altman_bland_ci_from_p.html#limitations-of-the-method",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html#limitations-of-the-method"
  },"400": {
    "doc": "Stats CI from P",
    "title": "P values presented as inequalities",
    "content": "Sometimes P values are very small and so are presented as P&lt;0.0001 or something similar. The above method can be applied for small P values, setting P equal to the value it is less than, but the z statistic will be too small, hence the standard error will be too large and the resulting CI will be too wide. This is not a problem so long as we remember that the estimate is better than the interval suggests. When we are told that P&gt;0.05 or the difference is not significant, things are more difficult. If we apply the method described here, using P=0.05, the confidence interval will be too narrow. We must remember that the estimate is even poorer than the confidence interval calculated would suggest. ",
    "url": "/pages/stats_altman_bland_ci_from_p.html#p-values-presented-as-inequalities",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html#p-values-presented-as-inequalities"
  },"401": {
    "doc": "Stats CI from P",
    "title": "References",
    "content": ". | Gardner MJ, Altman DG. Confidence intervals rather than P values: estimation rather than hypothesis testing. BMJ1986;292:746-50.Abstract/FREE Full TextGoogle Scholar | Moher D, Hopewell S, Schulz KF, Montori V, Gøtzsche PC, Devereaux PJ, et al. CONSORT 2010. Explanation and Elaboration: updated guidelines for reporting parallel group randomised trials. BMJ2010;340:c869.FREE Full TextGoogle Scholar | Lin J-T. Approximating the normal tail probability and its inverse for use on a pocket calculator. Appl Stat1989;38:69-70.CrossRefGoogle Scholar | Bland JM, Altman DG. Statistics Notes. Logarithms. BMJ1996;312:700.FREE Full TextGoogle Scholar | Roy SK, Hossain MJ, Khatun W, Chakraborty B, Chowdhury S, Begum A, et al. Zinc supplementation in children with cholera in Bangladesh: randomised controlled trial. BMJ2008;336:266-8.Abstract/FREE Full TextGoogle Scholar | Altman DG, Bland JM. Interaction revisited: the difference between two estimates. BMJ2003;326:219.FREE Full TextGoogle Scholar | Lindblad U, Råstam L, Rydén L, Ranstam J, Isacsson S-O, Berglund G. Control of blood pressure and risk of first acute myocardial infarction: Skaraborg hypertension project. BMJ1994;308:681. Abstract/FREE Full TextGoogle Scholar | . ",
    "url": "/pages/stats_altman_bland_ci_from_p.html#references",
    
    "relUrl": "/pages/stats_altman_bland_ci_from_p.html#references"
  },"402": {
    "doc": "Stats Correlation, regression and repeated data",
    "title": "Stats Correlation, regression and repeated data",
    "content": "Last update: 20210629 . | Stats Correlation, regression and repeated data . | Introduction | Correlation within subjects, part 1 | Correlation within subjects, part 2 | References | . | . ",
    "url": "/pages/stats_altman_bland_correlation.html",
    
    "relUrl": "/pages/stats_altman_bland_correlation.html"
  },"403": {
    "doc": "Stats Correlation, regression and repeated data",
    "title": "Introduction",
    "content": "This topic is introduced as the first paper bland1994correlation in a series of BMJ statistical notes by Altman &amp; Bland ( lit-altman_bland.md ): 1. Bland JM, Altman DG. (1994) Correlation, regression and repeated data. 308, 896. 1 . It concerns the analysis of paired data where there is more than one observation per subject. They point out that it could be highly misleading to analyse such data by combining repeated observations from several subjects and then calculating the correlation coefficient as if the data were a simple sample. Many researchers would assume that it is acceptable to gather repeated measurements for individuals and put all the data together. They use simulated data showing five pairs of measurements of two uncorrelated variables X and Y for subjects 1, 2, 3, 4, and 5. Using each subject’s mean values, they show correlation coefficient r=-0.67, df=3, P=0.22. However, when they put all 25 observations together they get r=-0.47, df=23, P=0.02. When the calculation is performed as if they have 25 subjects, the number of degrees of freedom for the significance test is increased incorrectly and a spurious significant difference is produced. Thus demonstrating that one should not mix observations from different subjects indiscriminately, whether using correlation or the closely related regression analysis. ",
    "url": "/pages/stats_altman_bland_correlation.html#introduction",
    
    "relUrl": "/pages/stats_altman_bland_correlation.html#introduction"
  },"404": {
    "doc": "Stats Correlation, regression and repeated data",
    "title": "Correlation within subjects, part 1",
    "content": "The methods to use in these circumstances are later discussed in another note for the BMJ series bland1995statistics, number 11 on the list lit-altman_bland.md : 11. Bland JM, Altman DG. (1995) Calculating correlation coefficients with repeated observations: Part 1, correlation within subjects. 310, 446. Notes: I am replacing their terms for my notes: . | X = Paco2 | Y = pHi | . In this note they show an example table using 8 subjects with 4-8 observations for X and Y (table I): . | Subject | Y | X | . | 1 | 6.68 | 3.97 | . | 1 | 6.53 | 4.12 | . | … | … | … | . | If subject’s average Y is related to the subject’s average X . | We can use the correlation between the subject means, which they shall describe in a subsequent note. | . | If an increase in Y within the individual was associated with an increase in X . | We want to remove the differences between subjects and look only at changes within. | . | . To look at variation within the subject we can use multiple regression. | Make one variable, X or Y, the outcome variable and the other variable and the subject the predictor variables. | The subject is treated as a categorical factor using dummy variables and so has seven degrees of freedom. | Using an analysis of variance table for the regression (table II) shows how the variability in Y can be partitioned into components due to different sources. | Also known as analysis of covariance | Equivalent to fitting parallel lines through each subject’s data (Figure 1.) | . | . | Source of variation | Degrees of freedom | Sum of squares | Mean square | Variance ratio (F) | Probability | . | Subjects | 7 | 2.9661 | 0.4237 | 48.3 | \\(&lt;\\)0.0001 | . | X | 1 | 0.1153 | 0.1153 | 13.1 | 0.0008 | . | Residual | 38 | 0.3337 | 0.0088 |   |   | . | Total | 46 | 3.3139 | 0.0720 |   |   | . Table II. Analysis of variance for the data in table I (as shown in Altman &amp; Bland). At the end of this page, this table is reproduced based on the original data and new R code, as shown. (Note that there is a slight varition between the published version and my replicated version of table II and Figure 1 below; probably due to a minor data entry error by the publisher or authors). | The residual sum of squares in table II represents the variation about regression lines. | This removes the variation due to subjects (and any other nuisance variables which might be present) and express the variation in Y due to X as a proportion of what’s left: . | (Sum of squares for X)/(Sum of squares for X + residual sum of squares). | . | The magnitude of the correlation coefficient within subjects is the square root of this proportion. | For table II this is: \\(\\sqrt{ \\frac{0.1153}{0.1153+0.3337} } = 0.51\\) | The sign of the correlation coefficient is given by the sign of the regression coefficient for X. | . | Regression slope is -0.108 | So the correlation coefficient within subjects is -0.51. | The P value is found either from: . | F test in the associated analysis of variance table, | t test for the regression slope. | It doesn’t matter which variable we regress on which; | we get the same correlation coefficient and P value either way. | . | . Incorrectly calculating the correlation coefficient by ignoring the fact that we have 47 observations on only 8 subjects, would produce -0.07, P=0.7. Figure 1. Recreation of “(Y) pH against (X) PaCO2 for eight subjects, with parallel lines fitted for each subject” as used in bland1995statistics. Interestingly, replotting this data shows that their figure was not fully accurate (forgivable before the days of Rstudio in 1995, and not important for this example). ",
    "url": "/pages/stats_altman_bland_correlation.html#correlation-within-subjects-part-1",
    
    "relUrl": "/pages/stats_altman_bland_correlation.html#correlation-within-subjects-part-1"
  },"405": {
    "doc": "Stats Correlation, regression and repeated data",
    "title": "Correlation within subjects, part 2",
    "content": "The second part shows how to find the correlation between the subject means bland1995calculating, number 12 on the list lit-altman_bland.md : 12. Bland JM, Altman DG. (1995) Calculating correlation coefficients with repeated observations: Part 2, correlation between subjects. 310, 633. In this note they show the example table using the same 8 subjects with one mean observation for X and Y: . | Subject | Y | X | Number | . | 1 | 6.49 | 4.04 | 4 | . | 2 | 7.05 | 5.37 | 4 | . | 3 | 7.36 | 4.83 | 9 | . | 4 | 7.33 | 5.31 | 5 | . | 5 | 7.31 | 4.40 | 8 | . | 6 | 7.32 | 4.92 | 6 | . | 7 | 6.91 | 6.60 | 3 | . | 8 | 7.12 | 4.78 | 8 | . | If subject’s average Y is related to the subject’s average X . | We can use the correlation between the subject means. | . | . They calculate the usual correlation coefficient for the mean Y and mean X; r=0.09, P=0.8. Does not take into account the different numbers of measurements on each subject. | Does this matter?: . | Depends on how different the numbers of observations are | whether the measurements within subjects vary much compared with the means between subjects | . | . We can calculate a weighted correlation coefficient using the number of observations as weights. Many computer programs will calculate this, but it is not difficult to do by hand. | They denote the mean Y and X for subject i by \\(\\bar{x}_i\\) and \\(\\bar{y}_i\\), | the number of observations for subject i by \\(m_i\\), | and the number of subjects by \\(n\\). | The weighted mean of the \\(\\bar{x}_i\\) is \\(\\frac{ \\sum{ m_i \\bar{x}_i } }{ \\sum{ m_i } }\\) | . In the usual case, where there is one observation per subject, the \\(m_i\\) are all one and this formula gives the usual mean \\(\\frac{ \\sum{\\bar{x}_i} }{n}\\). An easy way to calculate the weighted correlation coefficient is to replace each individual observation by its subject mean. Thus the table would yield 47 pairs of observations, the first four of which would each be pH=6.49 and Paco2=4.04, and so on. If we use the usual formula for the correlation coefficient on the expanded data we will get the weighted correlation coefficient. However, we must be careful when it comes to the P value. We have only 8 observations (n in general), not 47. We should ignore any P value printed by our computer program, and use a statistical table instead. The formula for a weighted correlation coefficient is: . $$ \\frac{ \\sum{m_i \\bar{x}_i \\bar{y}_i} - \\sum{m_i \\bar{x}_i} \\sum{m_i \\bar{y}_i} \\mathbin{/} \\sum{m_i} }{ \\sqrt{ ( \\sum{m_i \\bar{y}_i^2} - (\\sum{m_i \\bar{y}_i})^2 \\mathbin{/} \\sum{m_i} ) ( \\sum{m_i \\bar{y}_i^2} - (\\sum{m_i \\bar{y}_i})^2 \\mathbin{/} \\sum{m_i} )} } $$ where all summations are from \\(i=1\\) to \\(n\\). When all the \\(m_i\\) are equal they cancel out, giving the usual formula for a correlation coefficient. For the data in the table the weighted correlation coefficient is r=0.08, P=0.9. There is no evidence that subjects with a high Y also have a high X. However, as they have already shown in part 1, within the subject a rise in Y was associated with a fall in X. ## Code and raw data for Table I, Analysis of variance table II, and Figure 1 df &lt;- data.frame ( Subject = c(\"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"4\", \"4\", \"4\", \"4\", \"4\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"7\", \"7\", \"7\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\"), Y = c(6.68, 6.53, 6.43, 6.33, 6.85, 7.06, 7.13, 7.17, 7.4, 7.42, 7.41, 7.37, 7.34, 7.35, 7.28, 7.3, 7.34, 7.36, 7.33, 7.29, 7.3, 7.35, 7.35, 7.3, 7.3, 7.37, 7.27, 7.28, 7.32, 7.32, 7.38, 7.3, 7.29, 7.33, 7.31, 7.33, 6.86, 6.94, 6.92, 7.19, 7.29, 7.21, 7.25, 7.2, 7.19, 6.77, 6.82), X = c(3.97, 4.12, 4.09, 3.97, 5.27, 5.37, 5.41, 5.44, 5.67, 3.64, 4.32, 4.73, 4.96, 5.04, 5.22, 4.82, 5.07, 5.67, 5.1, 5.53, 4.75, 5.51, 4.28, 4.44, 4.32, 3.23, 4.46, 4.72, 4.75, 4.99, 4.78, 4.73, 5.12, 4.93, 5.03, 4.93, 6.85, 6.44, 6.52, 5.28, 4.56, 4.34, 4.32, 4.41, 3.69, 6.09, 5.58) ) # Run the Analysis of Variance with mutiple variable name=aov(Y ~ Subject + X, data = df) #runs the ANOVA test ls(name) #lists the items stored by the test. summary(name) #give the basic ANOVA output. # Output the column totals to match Altman &amp; Bland table df &lt;- as.data.frame(unlist( summary(name) )) sum(df[1:3,]) # Total Degrees of freedom sum(df[4:6,]) # Total Sum Sq sum(df[7:9,]) # Total Mean Sq . |   | Degrees of freedom | Sum of squares | Mean Square | Variance ratio (F) | Probability | . | df$Subject | 7 | 2.8648 | 0.4093 | 46.60 | &lt; 2e-16 | . | df$X | 1 | 0.1153 | 0.1153 | 13.13 | 0.000847 | . | Residuals | 38 | 0.3337 | 0.0088 |   |   | . Replicated version of Table II. Analysis of variance for the data in table I. Default R output headings modified: Degrees of freedom (Df), Sum of squares (Sum Sq), Mean square (Mean Sq), Variance ratio F (F value), Probability (Pr(&gt;F)). (Repeated note: there is a slight varition between the published version and my replicated version of table II and Figure 1; probably due to a minor data entry error by the publisher or authors). # code used to produce Figure 1. require(ggplot2) ggplot(df, aes(x = X, y = Y, group=Subject, color = Subject) ) + geom_point() + geom_smooth(aes(color = Subject), method = \"lm\", formula = y ~ x, , se = FALSE) # The dataset is cited by Bland &amp; Altman 1995 as: \"Boyd O, Mackay CJ, Lamb G, Bland JM, Grounds RM, Bennett ED.Comparison of clinical information gained from routine blood-gas analysis and from gastric tonometry for intramural pH.Lancet1993;341:142–6.\" . ",
    "url": "/pages/stats_altman_bland_correlation.html#correlation-within-subjects-part-2",
    
    "relUrl": "/pages/stats_altman_bland_correlation.html#correlation-within-subjects-part-2"
  },"406": {
    "doc": "Stats Correlation, regression and repeated data",
    "title": "References",
    "content": "Footnote 1 This article is almost identical to the original version in acknowledgment to Altman and Bland. It is adapted here as part of a set of curated, consistent, and minimal examples of statistics required for human genomic analysis. ↩ . ",
    "url": "/pages/stats_altman_bland_correlation.html#references",
    
    "relUrl": "/pages/stats_altman_bland_correlation.html#references"
  },"407": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "Stats Odds ratios, SE &amp; CI",
    "content": ". | Stats Odds ratios, SE &amp; CI . | Introduction | Odds ratio . | Perspective 1 | Perspective 2 | . | Standard error | Confidence interval | References | . | . ",
    "url": "/pages/stats_altman_bland_odds_ratios.html#stats-odds-ratios-se--ci",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html#stats-odds-ratios-se--ci"
  },"408": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "Introduction",
    "content": "Altman &amp; Bland review the use of odds ratio (OR), standard error (SE), and confidence interval (CI) with some examples in bland2000odds; the 42nd paper on the list of statistical notes in their BMJ series, ( lit-altman_bland.md ): 42. Bland JM, Altman DG. (2000) The odds ratio. 320, 1468. 1 . In reproducing their examples I use \\(X\\) and \\(Y\\); . | X = Eczema | Y = Hay fever | . As an example dataset, they cite the following table; Association between hay fever (Y) and eczema (X) in 11 year old children. |   | Y Yes | Y No | Y Total | . | X Yes | 141 | 420 | 561 | . | X No | 928 | 13 525 | 14 453 | . | X Total | 1069 | 13 945 | 15 522 | . ",
    "url": "/pages/stats_altman_bland_odds_ratios.html#introduction",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html#introduction"
  },"409": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "Odds ratio",
    "content": "Perspective 1 . The probability that a child with X will also have Y is estimated by the proportion \\(\\dfrac{141}{561}\\) (25.1%) and odds is estimated by \\(\\dfrac{141}{420}\\). Similarly, for children without X the probability of having Y is estimated by \\(\\dfrac{928}{14 453}\\) (6.4%) and the odds is \\(\\dfrac{928}{13 525}\\). They compare the groups in several ways: . | By the difference between the proportions \\(\\dfrac{141}{561} - \\dfrac{928}{14 453} = 0.187\\) (or 18.7 percentage points). | The ratio of the proportions (also called the relative risk) \\(\\dfrac{ \\left(\\dfrac{141}{561}\\right) }{ \\left(\\dfrac{928}{14 453}\\right) } = 3.91\\). | The OR \\(\\dfrac{ \\left(\\dfrac{141}{420}\\right) }{ \\left(\\dfrac{928}{13 525}\\right) } = 4.89\\). | . Perspective 2 . Looking at the table the other way round, What is the probability that a child with Y will also have X? . The proportion is \\(\\dfrac{141}{1069}\\) (13.2%) and the odds is \\(\\dfrac{141}{928}\\). For a child without Y, the proportion with X is \\(\\dfrac{420}{13 945}\\) (3.0%) and the odds is \\(\\dfrac{420}{13 525}\\). Comparing the proportions this way, . | The difference is \\(\\dfrac{141}{1069} - \\dfrac{420}{13 945} = 0.102\\) (or 10.2 percentage points); . | The ratio (relative risk) \\(\\dfrac{ \\left(\\dfrac{141}{1069}\\right) }{ \\left(\\dfrac{420}{13 945}\\right) } = 4.38\\); | The OR \\(\\dfrac{ \\left(\\dfrac{141}{928}\\right) }{ \\left(\\dfrac{420}{13 525}\\right) } = 4.89\\). | . The OR is the same whichever way round we look at the table, but the difference and ratio of proportions are not. This is because the two OR are . \\(\\dfrac{ 141 / \\textbf{420} }{ \\textbf{928} / 13 525 }\\) and \\(\\dfrac{ 141 / \\textbf{928} }{ \\textbf{420} / 13 525 }\\) which can both be rearranged to give \\(\\dfrac{ 141 \\times 13 525 }{ 928 \\times 420 }\\). Swapping orders for rows and columns produces the same OR. Swapping orders for either only rows or only columns produces the the reciprocal of the OR, \\(1/4.89 = 0.204\\). Thus, OR can indicate the strength of the relationship. OR cannot be negative but is not limited in the positive direction, producing a skew distribution. Reversing the order of categories for one variables simply results in a reversed sign of log OR: . \\(log(4.89) = 1.59\\), . \\(log(0.204) = - 1.59\\). ",
    "url": "/pages/stats_altman_bland_odds_ratios.html#odds-ratio",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html#odds-ratio"
  },"410": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "Standard error",
    "content": "The standard error (SE) can be calculated for the log OR and hence a confidence interval (CI). The SE of log OR is simply estimated by the square root of the sum of the reciprocals of the four frequencies. For the example, . \\[\\text{SE(}log \\text{OR)} = \\sqrt{ \\frac{1}{141} + \\frac{1}{420} + \\frac{1}{928} + \\frac{1}{13 525}} = 0.103\\] ",
    "url": "/pages/stats_altman_bland_odds_ratios.html#standard-error",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html#standard-error"
  },"411": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "Confidence interval",
    "content": "A 95% confidence interval (CI) for the log OR is obtained as 1.96 standard errors on either side of the estimate. For the example, the log OR is \\(log_{e} (4.89) = 1.588\\) and the confidence interval is \\(1.588 \\pm 1.96 \\times 0.103\\), which gives \\(1.386\\) to \\(1.790\\). The antilog of these limits to give a 95% CI for the OR itself, as \\(exp(1.386) = 4.00\\) to \\(exp(1.790) = 5.99\\). The observed OR, 4.89, is not in the centre of the confidence interval because of the asymmetrical nature of the OR scale. For this reason, in graphs ORs are often plotted using a logarithmic scale. The OR is 1 when there is no relationship. We can test the null hypothesis that the OR is 1 by the usual \\({\\chi}^2\\) test for a two by two table. Despite their usefulness, ORs can cause difficulties in interpretation. Altman &amp; Bland review this debate and also discuss ORs in logistic regression and case-control studies in future Statistics Notes. ",
    "url": "/pages/stats_altman_bland_odds_ratios.html#confidence-interval",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html#confidence-interval"
  },"412": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "References",
    "content": "Footnote 1 This article is almost identical to the original version in acknowledgment to Altman and Bland. It is adapted here as part of a set of curated, consistent, and minimal examples of statistics required for human genomic analysis. ↩ . ",
    "url": "/pages/stats_altman_bland_odds_ratios.html#references",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html#references"
  },"413": {
    "doc": "Stats Odds ratios, SE & CI",
    "title": "Stats Odds ratios, SE & CI",
    "content": "Last update: 20210704 . ",
    "url": "/pages/stats_altman_bland_odds_ratios.html",
    
    "relUrl": "/pages/stats_altman_bland_odds_ratios.html"
  },"414": {
    "doc": "Stats Receiver operating characteristic plots",
    "title": "Stats Receiver operating characteristic plots",
    "content": "Last update: 20210716 . | Stats Receiver operating characteristic plots . | Receiver operating characteristic plots | Example ROC cuvre | References | . | . ",
    "url": "/pages/stats_altman_bland_roc_curve.html",
    
    "relUrl": "/pages/stats_altman_bland_roc_curve.html"
  },"415": {
    "doc": "Stats Receiver operating characteristic plots",
    "title": "Receiver operating characteristic plots",
    "content": "This article covers the fifth paper in the series of statistics notes altman1994diagnostic ( lit-altman_bland.md ): 5. Altman DG, Bland JM. (1994) Diagnostic tests 3: receiver operating characteristic plots. 309, 188, and concerns quantitative diagnostic tests. 1 Diagnosis based on yes or no answers are covered in another note by Bland and Altman. The same statistical methods for quantifying yes or no answers can be applied here when there is a cut off threshold for defining normal and abnormal test results. For simplicity, I will call someone who is diagnosed by a clinical test a “case” and someone who is not diagnosed by a test/healthy/normal, a “control”. These terms are incorrect but much simpler to repeatedly read than “people who are diagnosed by a test”. The receiver operating characteristic (ROC) plot can be used measure how test results compare between cases and controls. Altman and Bland mention that this method was developed in the 1950s for evaluating radar signal detection. An aside for history buffs, from wikipedia: . The ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory.[56] Following the attack on Pearl Harbor in 1941, the United States army began new research to increase the prediction of correctly detected Japanese aircraft from their radar signals. For these purposes they measured the ability of a radar receiver operator to make these important distinctions, which was called the Receiver Operating Characteristic.[57] . The example shown in Figure 1 uses graft versus host disease, with an index measurement whose definition is not important. The Yes indicate cases and No indicate controls in our terminology, respectively. The usefulness of the test for predicting graft versus host disease will clearly relate to the degree of non-overlap between the two distributions. A ROC plot is obtained by calculating the . | sensitivity and | specificity of every observed data value and plotting, as in Figure 2, | Y axis = sensitivity, | X axis = 1 - specificity. | . A test that perfectly defines cases and cotrols would have a curve that aligns withe Y axis and top. A test that does not work would produce a straight line matching the centerline. In practice, overlaps always occur such that the curve usually lies somewhere between, as shown in Figure 2. The performance of the test (diagnostic accuracy) is reported as the area under the ROC curve. The area is equal to the probability that a random case has a higher measurement than that of a control. This probability is .5 for a test that does not work (e.g. coin-toss; straight line curve). This discriminatory power assessment is important for a clinical test if it is to be sufficient to discriminate cases and controls. At this stage we have the global assessment of discriminatory power showing that a test can divide cases and control. A cut off for clinical use also requires a local assessment. As per Altman and Bland; the simple approach of minimising “errors” (equivalent to maximising the sum of the sensitivity and specificity) is not necessarily best. We must consider any type of costs of . | false negatives | false positives | and prevalence of disease in the test cohort. | . In their example: . | cancer in general population . | most cases should be detected (high sensitivity) | many false positives (low specificity), who could then be eliminated by a further test. | . | . For comparing two or more measures, the ROC plot is useful. The curve wholly above another is clearly the better test. Altman and Bland cite a review for methods for comparing the areas under two curves for both paired and unpaired data. In my (reccomended) pocket-sized copy of Oxford handbook of medical statistics peacock2011oxford, a clinical example uses a chosen cut-off of sensitivity \\(&gt;81\\%\\) and specificity \\(28\\%\\). The area under ROC curve was .65, thus a moderately high predictive power. The accuracy (proportion of all correctly identified cases) was \\(\\frac{ 30 + 42 }{ 185 } = 39\\%\\) . \\[\\frac{\\text{No. cases above cutoff} + \\text{No. controls below cutoff }}{ \\text{cohort total} }\\] ",
    "url": "/pages/stats_altman_bland_roc_curve.html#receiver-operating-characteristic-plots",
    
    "relUrl": "/pages/stats_altman_bland_roc_curve.html#receiver-operating-characteristic-plots"
  },"416": {
    "doc": "Stats Receiver operating characteristic plots",
    "title": "Example ROC cuvre",
    "content": "To implement this method, I include here an example in R code. # Modified example from https://stackoverflow.com/questions/31138751/roc-curve-from-training-data-in-caret library(caret) library(mlbench) # Dataset data(Sonar) # An example dataset for classification of sonar signals using a neural network. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. Labels: \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). ctrl &lt;- trainControl(method=\"cv\", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T) rfFit &lt;- train(Class ~ ., data=Sonar, method=\"rf\", preProc=c(\"center\", \"scale\"), trControl=ctrl) library(pROC) # Select a parameter setting selectedIndices &lt;- rfFit$pred$mtry == 2 # Plot: plot.roc(rfFit$pred$obs[selectedIndices], rfFit$pred$M[selectedIndices], print.auc=TRUE, print.thres=TRUE, #thresh .557 shown legacy.axes=TRUE) # With ggplot library(ggplot2) library(plotROC) ggplot(rfFit$pred[selectedIndices, ], aes(d=( ifelse(rfFit$pred$obs[selectedIndices]==\"M\",1,0) ), m = M )) + geom_roc(hjust = -0.4, vjust = 1.5) + coord_equal() + style_roc() + annotate(\"text\", x=0.75, y=0.25, label=paste(\"AUC =\", round((calc_auc(g))$AUC, 4))) . ",
    "url": "/pages/stats_altman_bland_roc_curve.html#example-roc-cuvre",
    
    "relUrl": "/pages/stats_altman_bland_roc_curve.html#example-roc-cuvre"
  },"417": {
    "doc": "Stats Receiver operating characteristic plots",
    "title": "References",
    "content": "Footnote 1 This article is almost identical to the original version in acknowledgment to Altman and Bland. It is adapted here as part of a set of curated, consistent, and minimal examples of statistics required for human genomic analysis. ↩ . ",
    "url": "/pages/stats_altman_bland_roc_curve.html#references",
    
    "relUrl": "/pages/stats_altman_bland_roc_curve.html#references"
  },"418": {
    "doc": "Stats Sensitivity and specificity",
    "title": "Stats Sensitivity and specificity",
    "content": "Last update: 20210719 . | Stats Sensitivity and specificity . | Sensitivity and specificity | Confidence intervals | Quantifying the diagnostic ability | References | . | . ",
    "url": "/pages/stats_altman_bland_sensitivity_specificity.html",
    
    "relUrl": "/pages/stats_altman_bland_sensitivity_specificity.html"
  },"419": {
    "doc": "Stats Sensitivity and specificity",
    "title": "Sensitivity and specificity",
    "content": "The third paper on the list of BMJ statistics notes by Altman and Bland, ( lit-altman_bland.md ), altman1994diagnostic1 3. Altman DG, Bland JM. (1994) Diagnostic tests 1: sensitivity and specificity. 308, 1552. 1 . The simple diagnostic test such as an x-ray is used to classify patients into two groups: . | Presence of a symptom or sign . | Yes | No | . | . Altman and Bland use the following cited example; The results of a scan (test) compared to the correct diagnosis (true positive) based on either necropsy, biopsy, or surgical inspection. i.e. How good is the scan for correct diagnosis? . Table 1. Relation between results of liver scan and correct diagnosis. | Liver scan | : Pathology (diagnosis) : |   |   | . |   | Abnormal (+) | Normal (-) | Total | . | Abnormal(+) | 231 | 32 | 263 | . | Normal(-) | 27 | 54 | 81 | . | Total | 258 | 86 | 344 | . Patients who are correctly labelled are: . | Disease signs and abnormal liver . | 258 true positives | . | No signs and healthy liver . | 86 true negatives | . | . The proportions of these two groups that were also correctly diagnosed by the scan were \\(231/258=0.90\\) and \\(54/86=0.63\\), respectively. | Sensitivity . | Proportion of true positives that are correctly identified by the test. | . | Specificity . | Proportion of true negatives that are correctly identified by the test. | . | . Based on Altman and Bland’s example sample, we expect 90% true positives (patients with abnormal pathology to have abnormal (positive) liver scans), and 63% true negatives (those with normal pathology would have normal (negative) liver scans). ",
    "url": "/pages/stats_altman_bland_sensitivity_specificity.html#sensitivity-and-specificity",
    
    "relUrl": "/pages/stats_altman_bland_sensitivity_specificity.html#sensitivity-and-specificity"
  },"420": {
    "doc": "Stats Sensitivity and specificity",
    "title": "Confidence intervals",
    "content": "The sensitivity and specificity are proportions, so confidence intervals can be calculated. This uses standard methods for proportions gardner1989calculating. ",
    "url": "/pages/stats_altman_bland_sensitivity_specificity.html#confidence-intervals",
    
    "relUrl": "/pages/stats_altman_bland_sensitivity_specificity.html#confidence-intervals"
  },"421": {
    "doc": "Stats Sensitivity and specificity",
    "title": "Quantifying the diagnostic ability",
    "content": "Sensitivity and specificity are one approach to quantifying the diagnostic ability of the test. In this case, we already have the final results of tests and diagnosis from the sample set. For an individual patient we only have the test result. We want to quantify how well the test can predict true positives. This is answered in the next statistical note; predictive values. It defines positive and negative predictive values and requires the use of sensitivity, specificity, and prevalence. ",
    "url": "/pages/stats_altman_bland_sensitivity_specificity.html#quantifying-the-diagnostic-ability",
    
    "relUrl": "/pages/stats_altman_bland_sensitivity_specificity.html#quantifying-the-diagnostic-ability"
  },"422": {
    "doc": "Stats Sensitivity and specificity",
    "title": "References",
    "content": "Footnote 1 This article is almost identical to the original version in acknowledgment to Altman and Bland. It is adapted here as part of a set of curated, consistent, and minimal examples of statistics required for human genomic analysis. ↩ . ",
    "url": "/pages/stats_altman_bland_sensitivity_specificity.html#references",
    
    "relUrl": "/pages/stats_altman_bland_sensitivity_specificity.html#references"
  },"423": {
    "doc": "Storage architecture plan",
    "title": "Storage architecture plan",
    "content": ". | Storage architecture plan . | Storage | System overview . | Code and projects (/src/) | Data (/data/) | Media (/media/) | Documentation (/docs/) | Finance (/finance/) | Offsite backup with Backblaze B2 | Summary | NAS and RAID config . | Ugreen NASync DXP4800 Plus | Synology models for reference | . | Decision | . | . | . This page documents the planning for physical data storage and backup architecture. ➡️ See Storage, usage, and Git practices for how we work with project folders, sync, and versioning day to day. Here we provide the details of one pice of hardware in use; a Ugreen NASync DXP4800 Plus with RAID 5 for reliable onsite storage, combined with selective offsite backups (e.g. Backblaze B2) for critical data. This setup provides fast local access, resilience against hardware failure, and secure long-term protection without vendor lock-in. The document outlines the directory structure, NAS configuration, and storage planning rationale as a reference for maintaining a scalable and auditable data environment. ",
    "url": "/pages/storage_architecture_plan.html#storage-architecture-plan",
    
    "relUrl": "/pages/storage_architecture_plan.html#storage-architecture-plan"
  },"424": {
    "doc": "Storage architecture plan",
    "title": "Storage",
    "content": "├── src/ # local git projects (synced to NAS, pushed to GitHub) │ ├── project1/ │ │ ├── analysis/ │ │ ├── scripts/ │ │ ├── latex/ │ │ ├── README.md │ │ └── .git/ │ │ │ ├── project2/ │ │ │ ├── systematic_review/ # separate repo just for a stand alone paper │ │ ├── latex/ │ │ ├── figures/ │ │ ├── README.md │ │ └── .git/ │ └── ... │ ├── data/ # large datasets (mostly stored on NAS, copy locally if needed) │ ├── project1/ │ │ ├── raw/ # not in git, synced to NAS │ │ ├── processed/ # not in git, synced to NAS │ │ ├── release/ # cleaned data for public release (upload to Zenodo, Figshare, etc.) │ │ └── working/ # local scratch (NOT synced) - saved elsewhere when finished (raw, processed) for NAS sync │ │ │ ├── project2/ │ └── ... │ ├── media/ # media files (mostly stored on NAS, copy locally if needed) │ ├── documents/ │ ├── graphics/ │ ├── videos/ │ │ └── working/ # local scratch (NOT synced) - saved elsewhere when finished (raw, processed) for NAS sync │ └── docs/ # company documentation (synced to NAS) │ ├── business_plan.md │ ├── marketing_notes.md │ └── ... │ └── finance/ └── ... ",
    "url": "/pages/storage_architecture_plan.html#storage",
    
    "relUrl": "/pages/storage_architecture_plan.html#storage"
  },"425": {
    "doc": "Storage architecture plan",
    "title": "System overview",
    "content": "All work is organised under ~/so/ on the local hardware, structured into subfolders for code, data, media, and documentation. A local NAS, connected via ethernet to the office mesh Wi-Fi network, provides central storage and backup. The NAS syncs automatically with selected local folders, ensuring data redundancy and protection against hardware failures. Code and projects (/src/) . The /src/ directory contains all software development projects, including analysis scripts and LaTeX or Markdown for writing academic papers. These projects are managed with local git commits and pushed to GitHub for version control and collaboration, while also being synchronised to the NAS for backup. Data (/data/) . Large datasets reside under /data/. Most data remain stored on the NAS to save local disk space, but can be copied locally when running compute-intensive tools. Keeping a clear auditable structure. Within each project’s data folder: . | raw/ and processed/ (or equivalent) directories hold the main files. | release/ is prepared for public sharing through repositories like Zenodo, Figshare, EGA, customer products. | working/ serves as local scratch space for temporary files during analysis, providing faster performance for intermediate steps before results are saved back to the NAS. | . Media (/media/) . The /media/ directory houses content that we generate such as documents, books, graphics, and video projects. Smaller files sync continuously with the NAS. As with analysis data/, for large media work such as videos, raw footage is copied locally into the working/ subfolder to ensure fast read and write speeds during editing, and completed projects are then archived back to the NAS for long-term storage. No sensitive data or critical IP. Documentation (/docs/) . The /docs/ folder stores company-level documentation and internal materials, all synced to the NAS to keep critical records safe and accessible. Keeping a clear auditable structure. Finance (/finance/) . Equivalent to /docs/ with a dedicated space. Keeping a clear auditable structure. Offsite backup with Backblaze B2 . In addition to local and NAS storage, the system uses Backblaze B2 for offsite backup. The NAS connects to Backblaze B2 via Hyper Backup tool, which automatically schedules encrypted backups of selected folders (such as /src/, /data/, /media/, and /docs/). This ensures that all critical files are protected against physical disasters like fire, theft, or catastrophic hardware failure. Hyper Backup allows versioning, so older copies of files can be restored if needed. The cost is low, typically around $5 per terabyte per month. Scratch folders like working/ in both /data/ and /media/ are not included in these backups, as they are meant for temporary local use and cleared when projects are complete. Summary . This setup ensures that software is always run locally from desktops (e.g. Mac) for speed and control, while data and media can reside on the NAS unless local speed is required. Scratch folders in both /data/ and /media/ provide local-only working space for tasks needing high performance, keeping the main storage system organised and efficient. Meanwhile, Backblaze B2 provides a secure, offsite layer of protection for long-term data safety. NAS and RAID config . We use RAID 5 to balance redundancy and capacity for a compact research team. This provides protection against a single disk failure while maximising usable storage capacity. In case of a disk failure, the plan is to replace the failed drive and allow RAID to rebuild from parity. Additional safety comes from offsite backups to Backblaze B2, and from code and documents stored in remote repositories like GitHub, ensuring important media and data can be restored if needed. For now, RAID 5 is sufficient, with the option to switch to RAID 6 in the future if the project expands or data becomes more critical. Currently, our datasets are calculated from public resources which can be reproduced if necessary. For a project of &lt; 10 TB: . | Model | CHF | Drives | RAID Config | Usable Space | CHF per usable TB | . | DS224+ (2×8 TB) | 753 | 2 × 8 TB | RAID 1 | 8 TB | ~CHF 94 / TB | . | DS423+ (4×4 TB) | 919 | 4 × 4 TB | RAID 5 | ~12 TB | ~CHF 76 / TB | . | DS423+ (4×4 TB) | 919 | 4 × 4 TB | RAID 6 | ~8 TB | ~CHF 115 / TB | . | DS923+ (4×4 TB) | 1029 | 4 × 4 TB | RAID 5 | ~12 TB | ~CHF 86 / TB | . | Ugreen DXP4800 Plus (4×4 TB) | 1029 | 4 × 4 TB | RAID 5 | ~12 TB | ~CHF 86 / TB | . | Ugreen DXP4800 Plus (3×8 TB) | 1119 | 3 × 8 TB | RAID 5 | ~16 TB | ~CHF 70 / TB | . Spending: . | Ugreen NASync DXP4800 Plus chassis: CHF 588.– | WD Red Plus 8 TB drives × 3: CHF 177.– each → CHF 531.– | NAS Total: CHF 1119.– | Apple Thunderbolt to Gigabit Ethernet Adapter (MD463ZM/A): CHF 14.89 + CHF 4.20 shipping → Total CHF 19.09 | . Ugreen NASync DXP4800 Plus . The Ugreen NASync DXP4800 Plus is a modern NAS offering high hardware performance and flexibility: . | CPU: Intel Pentium Gold 8505, 5 cores / 6 threads, up to 4.40 GHz | RAM: 8 GB DDR5 (expandable up to 64 GB) | Drive bays: 4 × SATA (2.5″ / 3.5″) + 2 × M.2 NVMe | Maximum storage capacity: up to 104 TB (4 × 26 TB drives) | Network ports: 1 × 10GbE + 1 × 2.5GbE | RAID support: 0, 1, 5, 6, 10, JBOD | Connectivity: HDMI 4K, SD card reader, multiple USB ports (USB-A, USB-C) | Weight: 6.02 kg | Dimensions: 25.75 × 17.80 × 17.80 cm | Country of origin: China | Release date: 7 October 2024 | . Unlike Synology, Ugreen allows the use of third-party drives without vendor lock-in, which reduces future costs and provides flexibility in hardware upgrades. It also features built-in 10GbE networking for high-speed transfers, and offers significant headroom for RAM and CPU-intensive workloads, making it well-suited for future data growth and heavier operations. Synology models for reference . While Synology remains a leader in NAS software (DSM) and integrated cloud services, their hardware offers comparatively modest specifications: . | Lower base RAM (2–4 GB), expandable only on some models | Often requires purchase of additional cards for 10GbE networking | Potential future restrictions on third-party drives, increasing long-term costs | Excellent software ecosystem, with mature support for cloud backups like Backblaze B2 | . Decision . Given the combination of hardware power, freedom from vendor lock-in, built-in 10GbE networking, and expandability, we have chosen the Ugreen NASync DXP4800 Plus as the NAS platform. While Synology remains an excellent solution for those prioritising a mature software environment, Ugreen offers superior hardware value and flexibility, which aligns better with the performance and cost-efficiency goals of the project. ",
    "url": "/pages/storage_architecture_plan.html#system-overview",
    
    "relUrl": "/pages/storage_architecture_plan.html#system-overview"
  },"426": {
    "doc": "Storage architecture plan",
    "title": "Storage architecture plan",
    "content": "Last update: 20250727 . ",
    "url": "/pages/storage_architecture_plan.html",
    
    "relUrl": "/pages/storage_architecture_plan.html"
  },"427": {
    "doc": "Storage estimates",
    "title": "Storage",
    "content": "Last update: 20250120 . ",
    "url": "/pages/storage_estimates.html#storage",
    
    "relUrl": "/pages/storage_estimates.html#storage"
  },"428": {
    "doc": "Storage estimates",
    "title": "Summary estimates",
    "content": "For a full omic analysis we estimate that 250GB of space is used per subject. ",
    "url": "/pages/storage_estimates.html#summary-estimates",
    
    "relUrl": "/pages/storage_estimates.html#summary-estimates"
  },"429": {
    "doc": "Storage estimates",
    "title": "Sample count",
    "content": "The number of subjects changes depending on the data type. During writing, we have 180 subjects in an example project. ",
    "url": "/pages/storage_estimates.html#sample-count",
    
    "relUrl": "/pages/storage_estimates.html#sample-count"
  },"430": {
    "doc": "Storage estimates",
    "title": "Data usage",
    "content": ". | All project data . | total: 45T | . | Estimate per subject total data . | All: 45000/180 = 250 GB per subject | . | Current size prod raw data . | WGS: 12.8T | RNA: 793.0G | . | Estimate per subject raw data . | WGS: 12800/180 = 70 GB per subject | RNA: 800/180 = 4.5 GB per subject | . | . ",
    "url": "/pages/storage_estimates.html#data-usage",
    
    "relUrl": "/pages/storage_estimates.html#data-usage"
  },"431": {
    "doc": "Storage estimates",
    "title": "All users data",
    "content": "`df -h /project/` Filesystem Size Used Avail Use% Mounted on tenant_name 77T 45T 32T 60% /project . ",
    "url": "/pages/storage_estimates.html#all-users-data",
    
    "relUrl": "/pages/storage_estimates.html#all-users-data"
  },"432": {
    "doc": "Storage estimates",
    "title": "Raw prod data",
    "content": "Size of WGS . du -sh prod/*/downloads/WGS* | awk ' { # Convert all sizes to gigabytes for consistency size = $1 sub(/[[:alpha:]]$/, \"\", size) unit = substr($1, length($1)) if (unit == \"T\") size *= 1024 if (unit == \"G\") size += 0 total += size } END { if (total &gt;= 1024) { printf \"%.1fT\\n\", total / 1024 } else { printf \"%.1fG\\n\", total } }' . Size of RNA . du -sh prod/*/downloads/RNA* | awk ' { # Convert all sizes to gigabytes for consistency size = $1 sub(/[[:alpha:]]$/, \"\", size) unit = substr($1, length($1)) if (unit == \"T\") size *= 1024 if (unit == \"G\") size += 0 total += size } END { if (total &gt;= 1024) { printf \"%.1fT\\n\", total / 1024 } else { printf \"%.1fG\\n\", total } }' . ",
    "url": "/pages/storage_estimates.html#raw-prod-data",
    
    "relUrl": "/pages/storage_estimates.html#raw-prod-data"
  },"433": {
    "doc": "Storage estimates",
    "title": "Size of individual batches",
    "content": "du -sh prod/*/downloads/WGS* du -sh prod/*/downloads/RNA* . ",
    "url": "/pages/storage_estimates.html#size-of-individual-batches",
    
    "relUrl": "/pages/storage_estimates.html#size-of-individual-batches"
  },"434": {
    "doc": "Storage estimates",
    "title": "Storage estimates",
    "content": " ",
    "url": "/pages/storage_estimates.html",
    
    "relUrl": "/pages/storage_estimates.html"
  },"435": {
    "doc": "Storage, usage, and Git practices",
    "title": "Storage, usage, and Git practices",
    "content": ". | Storage, usage, and Git practices . | Working directory: ~/so | Central NAS data store: /volume1/data_big | Snapshots | Git version control | Connection speed | Summary | . | . We maintain a structured, auditable setup for managing all data, source code, and project files. This approach balances local development flexibility with reliable centralised storage and offsite backup. Daily work happens in ~/so, versioned with Git and synced to the NAS. Snapshots and offsite backups run in the background. Large datasets stay on the NAS with high-speed link if required. Very large data is processed on HPC. This page explains how the system works and how to use it. Working directory: ~/so . Each team member works locally in the ~/so directory, which is two-way synced with the NAS path /volume1/so. This pulls in source code, documents, releases, test datasets, and other active materials. | ✅ Synced with offsite and cloud backups | ✅ Synced bidirectionally between local and NAS | ✅ Suitable for day-to-day development and collaboration | ⚠️ Deletions are propagated across systems | ❌ Snapshots with NAS but are not synced back to local machines | . Typical synced structure: . ~/so/ ├── src/ ├── docs/ ├── release/ ├── netlify-backend/ ├── media/ ├── org/ └── web/ . Central NAS data store: /volume1/data_big . Large static datasets, reference genomes, media assets, and derived outputs are stored exclusively on the NAS under /volume1/data_big. These are not included in sync operations and must be accessed over the network. | ✅ Accessible via LAN, mount, or SSH | ❌ Not auto synced to desktop | ✅ Snapshotted for recovery | . Example layout: . /volume1/data_big/ ├── data/ ├── db/ ├── davinci_video_production/ ├── ollama/ └── #recycle/ . Snapshots . Snapshots are created automatically on the NAS once per month and retained by count (not date). These are lightweight, incremental, and only use additional space when files are changed or deleted. | ✅ Stored locally on NAS (.snapshot or @snapshot) | ✅ Recoverable even if files are deleted from both NAS and local sync | ❌ Not visible or synced to local machines | . To recover a file manually: . cp /volume1/so/.snapshot/snapshot_20250701/docs/notes/overview.md /volume1/so/docs/notes/ . Git version control . All source code, structured documents, and defined project datasets are versioned in Git. Repositories are linked to the GitHub organisation for traceability and reproducibility. A set of shared scripts helps validate Git usage across the entire ~/so tree. Example output from a Git check: . $ sh org/script/check_git_init.sh Git repo checklist: -- ./org 📁 container ✅ ./org/script 🌐 git@github.com:switzerlandomics/org-script.git -- ./docs 📁 container ✅ ./docs/notes 🌐 git@github.com:switzerlandomics/docs-notes.git ✅ ./docs/resources 🌐 git@github.com:switzerlandomics/docs-resources.git ❌ ./media ⚠️ none ❌ ./media/house_art ⚠️ none . sh org/script/check_git_status.sh Checking git status for projects in ./ ... Project: script Path: ~/so/org/script Last commit: 2025-07-16 13:56:50 +0200 Ahead of remote by: 0 commits Behind remote by: 0 commits ⚠️ Uncommitted changes exist! . Notes: . | ✅ All critical folders must be Git-initialised with a valid remote | ⚠️ Non-versioned folders are intentional (e.g. raw media or exploratory work) | 📁 “Container” folders group multiple Git repositories but are not tracked themselves | . Connection speed . ⚡ When connected via LAN1 (10GbE over Thunderbolt), /volume1/data_big provides fast access to large datasets - typically ~1 Gbps (≈110 MB/s) when using a standard Thunderbolt-to-Gigabit Ethernet adapter. This adapter caps throughput to 1 Gbps, but a true 10 GbE interface is available and supported by the NAS for significantly higher speeds if needed. For even greater performance, our NAS supports SSDs via M.2 NVMe slots, which can be configured for high-speed caching or dedicated SSD storage volumes. This is useful for working with large datasets or video projects where read/write speed is a bottleneck. Summary . Onsite + cloud: . | Path | Sync’d | Git-tracked | Snapshotted | Purpose | . | ~/so/src/quant_db | ✅ | ✅ | ✅ | Active source code | . | ~/so/media/video_assets | ✅ | ❌ | ✅ | Raw, unversioned media | . | /volume1/data_big/db | ❌ | ❌ | ✅ | Reference or derived datasets | . | .snapshot/ | ❌ | ❌ | ✅ | Local to NAS for recovery only | . Onsite mirrored to offsite. This layered setup provides fast access for day-to-day work, robust backup through snapshots, and rigorous auditability through Git. All new folders intended for collaboration or long-term relevance should either be versioned in Git or stored in /volume1/data_big. ",
    "url": "/pages/storage_use_sync_and_versioning.html#storage-usage-and-git-practices",
    
    "relUrl": "/pages/storage_use_sync_and_versioning.html#storage-usage-and-git-practices"
  },"436": {
    "doc": "Storage, usage, and Git practices",
    "title": "Storage, usage, and Git practices",
    "content": "Last update: 20250727 . ",
    "url": "/pages/storage_use_sync_and_versioning.html",
    
    "relUrl": "/pages/storage_use_sync_and_versioning.html"
  },"437": {
    "doc": "Style page guide",
    "title": "Page style guide for documentation",
    "content": ". | Meta data: . | short title for navigation and sidebar. | nav_order set to 5 so that all pages are alphabetically sorted expect highly important ones which can be order 1, 2, etc. | . | Last update: written in plane text before title. We do not add to metadata. | If required, use a table of contents (TOC) after the title. | Images can be stored in assets/images. | For LaTeX format equations see MathJax config | . Here is the style guideline for .md page layout in these documents. --- layout: default title: Page styles (short heading for sidebar/nav) nav_order: 5 --- Last update: 20230828 # Title long format for page heading * TOC {:toc} ## Heading 2 Include images &lt;img src=\"/assets/images/acat/p_acat.png\" width=\"80%\"&gt; . ",
    "url": "/pages/style.html#page-style-guide-for-documentation",
    
    "relUrl": "/pages/style.html#page-style-guide-for-documentation"
  },"438": {
    "doc": "Style page guide",
    "title": "Style page guide",
    "content": "Last update: 20230828 . ",
    "url": "/pages/style.html",
    
    "relUrl": "/pages/style.html"
  },"439": {
    "doc": "Style writing guide",
    "title": "Technical writing style guide",
    "content": "Writing style . | Prefer active voice unless passive helps clarity. | Write for global audiences. Avoid slang, idioms, and metaphors. | Avoid contractions in formal documentation. | Keep sentences short, ideally under 40 words. | Use simple present tense where possible. | Avoid redundant words or phrases. | Never use an em-dash. Do not use it to create complex sentences. Instead, break long sentences into simpler ones or use alternative punctuation. | . Grammar and punctuation . | Ensure pronoun–antecedent and subject–verb agreement. | Use “fewer” for countable nouns, “less” for uncountable nouns. | Do not use possessives for product names or abbreviations. | Prefer “who” for people and “that” or “which” for things. | Avoid run-ons and sentence fragments. | End sentences naturally. Do not force prepositions away from the end of the sentence if it sounds awkward. | Avoid slashes. Use “or” instead of “and/or.” | Use the Oxford comma in lists. | Avoid exclamation marks except in command syntax (e.g. !). | Use hyphens for compound adjectives when needed for clarity. | Avoid title case in headings and titles. Always use sentence case, meaning only the first word and proper nouns are capitalised. | . Inclusive and clear language . | Use gender-neutral pronouns such as “they.” | Prefer “you” instead of first-person references. | Use precise terms. Avoid ambiguous words. | Spell out acronyms and initialisms at first mention unless widely known (e.g. HTML). | Avoid anthropomorphism. Do not write “the product allows…” unless referring to permissions. | . Document structure . | Titles: . | Include the product name, version, and document type. | Prefer gerund forms like “Configuring…” for chapters and sections. | Avoid one-word titles. | . | Headings: . | Use sentence case. | Do not place headings back-to-back without explanatory text between them. | . | Abstracts: . | Briefly state what the document covers, how it is structured, and who it is for. | . | . Technical content . | Match UI element spelling and casing exactly as shown on screen. | Prefer “go to” instead of “navigate to.” | Do not include punctuation from UI elements unless it improves clarity. | When documenting commands: . | Show user prompt symbols appropriately ($ for user, # for root). | Omit optional flags unless they are critical. | Use line continuation characters for long commands. | . | Show only relevant command output. Use ...output omitted... if skipping sections. | Avoid naming specific text editors unless necessary. | Use example domains like example.com and avoid real IP addresses. | Use realistic and diverse fictional names. | . Formatting . | Avoid splitting product names across lines. | Use non-breaking spaces: . | Between words like “Red” and “Hat.” | Between product names and version numbers. | Between numbers and units of measure. | . | Avoid including file names, commands, or markup in headings. | . Cross-references and citations . | Use “refer to” instead of “see.” | Do not use “here” as anchor text. Link meaningful words or phrases instead. | Format citations as: . | Book: Title by Author; Publisher. | Website: Include a URL or footnote. | . | . Admonitions . | Use sparingly: . | Note → Additional information. | Important → Key details the reader must not overlook. | Warning → Risk of damage, data loss, or critical errors. | . | . Lists . | Use bulleted lists when item order does not matter. | Use numbered lists for ordered steps or when referencing list items elsewhere. | Use variable lists for terms followed by definitions. | Use procedures for required steps to complete a task. Always include a title. | Keep nested lists to two levels or fewer. | Avoid excessive use of bulleted lists. Consider whether information might be clearer and more pleasant to read as paragraphs instead, especially if lists become deeply nested. | . Formatting lists for readability . | Add spacing between list items for readability, especially if items include: . | Nested lists | Navigation instructions | Multiple sentences or paragraphs | . | Avoid placing lists inside the middle of a sentence and then continuing the sentence after the list. | Lead-in sentences for lists should be complete sentences. | Ensure list items are grammatically parallel. | Use consistent punctuation: . | Complete sentences in a list end with periods. | Sentence fragments in a list have no ending punctuation unless followed by complete sentences. | . | Avoid graphics in lists except in simple cases. | . Grammatical genders . | Avoid ambiguous pronouns such as “it” or “they” without a clear antecedent. | Clarify references to terms that might differ in gender in other languages. | For initialisms or acronyms that refer to multiple concepts, clarify with a noun instead of relying on a pronoun. | . Using markup correctly . | Mark up file names, commands, and technical terms properly in documentation. | Avoid embedding unmarked literals into running text. | Never embed technical terms into a sentence without correct markup formatting. | Example: . | Instead of: In /usr/local/bin/, grep for XYZ. | Use: In the /usr/local/bin/ directory, use the grep command to search for \"XYZ\". | . | . Code blocks . | Keep explanatory comments outside of code blocks unless they are part of the literal code syntax. | Example of incorrect usage: # Display disk usage df -h . | Correct usage: Run the following command to display disk usage: df -h . | Do not include commentary lines that might confuse translation or readers. | . Entities . | Avoid using custom SGML or XML entities for translatable terms. | Limit entities to those required for the build process, such as: . | PRODUCT | BOOKID | YEAR | HOLDER | . | Do not create entities for common terms (e.g. &amp;VERSION;) or cultural references (e.g. &amp;BIBLE;). | Use built-in entities if your documentation tool provides them. | . Using cross-references effectively . | Cross-references should add value, not distract readers. | Only link to additional background information. Do not link away from core task instructions. | . Example: . Incorrect: See Appendix B for file naming conventions. Correct: Use lowercase letters and hyphens for file names. For more details, refer to Appendix B. | Avoid excessive links in a single paragraph. No more than two links per paragraph is recommended. | Summarise references at the end of sections instead of scattering links throughout text. | . Repetition . | Repeating vital information is acceptable if it avoids forcing readers to navigate to another section. | Repeat information if: . | It is less than half a page. | It appears in multiple contexts where readers may not follow links. | . | . Resources . | Follow company-specific style guides first. | When needed, refer to: . | IBM Style | The Chicago Manual of Style | Merriam-Webster Dictionary | . | These resources may conflict. Always align with your organisation’s style preferences. | . Content types covered: . | Software manuals | User guides | Training courses | White papers | . Content excluded: . | Marketing content | Corporate branding content | . ",
    "url": "/pages/style_writing.html#technical-writing-style-guide",
    
    "relUrl": "/pages/style_writing.html#technical-writing-style-guide"
  },"440": {
    "doc": "Style writing guide",
    "title": "Style writing guide",
    "content": "Last update: 20250711 . ",
    "url": "/pages/style_writing.html",
    
    "relUrl": "/pages/style_writing.html"
  },"441": {
    "doc": "Structural variation detection",
    "title": "Structural variation detection",
    "content": "Last update: 20240820 . ",
    "url": "/pages/sv.html",
    
    "relUrl": "/pages/sv.html"
  },"442": {
    "doc": "Structural variation detection",
    "title": "Understanding Structural Variation Detection with smoove, lumpy, and duphold",
    "content": "Structural variations (SVs) such as deletions, duplications, inversions, and translocations play a critical role in genomic diversity and disease. Detecting these variations accurately in whole-genome sequencing data requires sophisticated bioinformatics tools. Here, we discuss how Smoove, Lumpy, and Duphold work together to detect and interpret SVs, focusing on their methodologies rather than specific commands or installation procedures. | smoove - https://github.com/brentp/smoove | lumpy-sv - https://github.com/arq5x/lumpy-sv | duphold - https://github.com/brentp/duphold | . Example results quoted from “https://github.com/brentp/duphold”: . A clear deletion will have rapid drop in depth at the left and increase in depth at the right and a lower mean coverage. Duphold annotated this with: DHBFC: 0.6. These indicate that both break-points are consistent with a deletion and that the coverage is ~60% of expected. So this is a clear deletion. BND - when lumpy decides that a cluster of evidence does not match a DUP or DEL or INV, it creates a BND with 2 lines in the VCF. Sometimes these are actual deletions. For example this shows where a deletion is bounded by 2 BND calls. duphold annotates this with: DHBFC: 0.01 indicating a homozygous deletion with clear break-points. Smoove: Streamlining Lumpy’s Process . Smoove is a software that enhances Lumpy’s SV calling capabilities. It wraps around Lumpy and other tools to simplify and speed up the process, especially for large-scale genomic studies. Here’s how Smoove enhances the detection of structural variations: . | Read Filtering: . | Smoove employs lumpy_filter to extract split reads and discordant reads, which are critical for identifying potential SVs. | It filters out low-quality signals such as reads with excessive soft-clipping or multiple mismatches, and those aligned to multiple locations. | Reads in specified exclude regions or those contributing to regions of abnormally high coverage (suggestive of repetitive sequences) are also discarded. | . | Parallel Processing: . | Smoove streams Lumpy’s output directly into svtyper, facilitating simultaneous genotyping across different genomic regions. This significantly reduces the computational time and memory requirements. | . | Population-Level Calling: . | For larger datasets, Smoove enables joint calling by first calling variants in smaller groups (e.g., by family) and then merging these calls. This is followed by re-genotyping at these combined sites, further refined by integrating all findings into a comprehensive VCF using tools like svtools and bcftools. | . | . Lumpy: A Probabilistic Framework for SV Discovery . Lumpy serves as the foundational tool for SV detection used by Smoove. It offers a flexible and highly sensitive approach to detect SVs across different types and sizes by analyzing the signatures of split reads and discordant read pairs: . | Probabilistic Modeling: . | Lumpy uses a probabilistic model to integrate evidence from various read types, including paired-end and split reads, to predict SVs. | This method allows for the detection of complex SVs that might not be identified through traditional read-mapping approaches. | . | . Duphold: Enhancing SV Calls with Depth Information . Duphold complements Smoove and Lumpy by adding depth-based annotations to SV calls, which helps in assessing the confidence of these variants: . | Depth Annotations: . | DHFC: Compares the depth of coverage within the SV to the chromosome average. | DHBFC: Compares the depth within the SV to genomic regions with similar GC content, which helps control for sequencing biases. | DHFFC: Compares the depth within the SV to its immediate flanking regions, providing a localized assessment of depth changes. | . | Annotation Against SNP/Indel VCFs: . | Duphold can use existing SNP/Indel data to annotate SVs, helping to identify unlikely deletions that contain multiple heterozygous SNP calls, for example. | . | . Practical Application in Cohort Studies . In practice, these tools enable researchers to effectively identify and validate structural variations across large cohorts. By integrating read-based and depth-based evidence, researchers can filter out spurious calls and focus on variants that are most likely to be true positives. Visualisations such as histograms of depth changes and variant allele frequencies, along with comparative plots across samples or groups, provide intuitive insights into the data, aiding further in the interpretation of results. Conclusion . The combination of Smoove, Lumpy, and Duphold offers a robust framework for the detection and interpretation of structural variations in genomic datasets. By leveraging their individual strengths in read processing, probabilistic modeling, and depth annotation, researchers can achieve a high level of accuracy and efficiency in SV detection, essential for advancing our understanding of genomic structure and its impact on health and disease. ",
    "url": "/pages/sv.html#understanding-structural-variation-detection-with-smoove-lumpy-and-duphold",
    
    "relUrl": "/pages/sv.html#understanding-structural-variation-detection-with-smoove-lumpy-and-duphold"
  },"443": {
    "doc": "Structural variation detection",
    "title": "Running Smoove using Singularity",
    "content": "Singularity provides an effective way to utilize Docker containers in environments where Docker itself may not be suitable, such as shared HPC systems. Here’s a generalized example of how you can deploy and run smoove using a Singularity container, which is useful for calling and genotyping structural variants in genomic data. Step-by-Step Guide: . | Obtaining the Smoove Singularity Image: . | First, pull the smoove Docker image from Docker Hub and convert it into a Singularity image file (.sif). This step typically requires admin privileges or may be handled by your system administrator: singularity pull docker://brentp/smoove . | This command creates a .sif file that can be used directly on any system where Singularity is installed. | . | Preparing the Environment: . | Load necessary modules, if applicable, such as samtools. Ensure all dependencies like reference genome files and exclude region BED files are accessible. | . | Running Smoove: . | Use the Singularity exec command to run smoove within the Singularity container. It’s analogous to using docker run but tailored for Singularity environments. Here’s an example command structure based on a SLURM script for batch processing: singularity exec smoove_latest.sif smoove call --outdir results-smoove/ \\ --exclude exclude_regions.bed \\ --name sample_name \\ --fasta reference.fasta \\ -p 1 \\ --genotype sample.bam . | This command runs smoove to call structural variants for a single sample. It specifies an output directory, an exclusion BED file for problematic regions, the sample name, the reference genome, and the input BAM file. | . | Handling Multiple Samples: . | If processing multiple samples, consider using SLURM’s array job feature to parallelize the process. Each job can process a different sample, effectively distributing the workload across multiple compute nodes or cores. | . | Post-Processing: . | After generating VCF files for individual samples, you may need to merge these using smoove merge to create a combined VCF file that includes variants from all samples: singularity exec smoove_latest.sif smoove merge --name merged -f reference.fasta --outdir ./ results-smoove/*.genotyped.vcf.gz . | Subsequently, genotype each sample at the merged sites and optionally run duphold to add depth annotations, enhancing the interpretability of the SV calls. | . | Output and Validation: . | The final output will typically be a set of VCF files containing the called and genotyped structural variants for your cohort. These can be further analyzed or visualized using additional tools to assess the quality and implications of the detected variants. | . | . ",
    "url": "/pages/sv.html#running-smoove-using-singularity",
    
    "relUrl": "/pages/sv.html#running-smoove-using-singularity"
  },"444": {
    "doc": "Synthetic data",
    "title": "Synthetic data",
    "content": ". | Synthetic data . | Key Components of the Script: | Purpose and Benefits: | Code example . | 1. Example with numeric dataset (mtcars) | 2. Example with categorical dataset (iris) | 3. Example of data with no existing version (medical) | . | . | . This script will show you how to make synthetic (fake example) data. The data should match what you expect to see in a real dataset. The provided script demonstrates a practical approach for creating and utilising synthetic datasets in R, aimed at continuing data analysis development when access to original data is restricted due to legal or privacy concerns. This method allows developers and analysts to develop and test analytical models and data processing pipelines without compromising data governance policies. ",
    "url": "/pages/synth_data.html#synthetic-data",
    
    "relUrl": "/pages/synth_data.html#synthetic-data"
  },"445": {
    "doc": "Synthetic data",
    "title": "Key Components of the Script:",
    "content": ". | Libraries Used: . | dplyr for data manipulation. | ggplot2 for data visualisation. | synthpop for synthesising datasets, ensuring the synthetic versions maintain similar statistical distributions as the originals. | patchwork for combining plots into a single image for easier analysis. | . | Datasets: . | The script utilises well-known R datasets, mtcars and iris, which exemplify numeric and categorical data types, respectively. | . | Synthesising Data: . | The syn() function generates synthetic datasets, with subsequent verification steps to ensure these datasets match the original data in terms of structure and type. | . | Adding Sample Names: . | Using sprintf() to generate formatted sample names provides unique identifiers that aid in managing and referencing samples during analyses. | . | Data Visualisation and Comparison: . | Various plotting functions are crafted to compare the distributions between original and synthetic datasets through density plots, bar plots, and linear regression analyses, adapting the visualisation method based on the data’s characteristics (numeric vs. categorical). | . | Saving Visual Outputs: . | The ggsave() function is utilised to save generated plots to files, enabling easy distribution and review. | . | . ",
    "url": "/pages/synth_data.html#key-components-of-the-script",
    
    "relUrl": "/pages/synth_data.html#key-components-of-the-script"
  },"446": {
    "doc": "Synthetic data",
    "title": "Purpose and Benefits:",
    "content": ". | Development Continuity: This approach supports the continuous development of data analysis tools in scenarios where actual data cannot be used. | Model Testing: It facilitates preliminary testing of statistical models, ensuring they will perform as expected with real-world data. | Privacy Compliance: By using synthetic data, the method adheres to data privacy laws, ensuring that personal or sensitive information is not disclosed. | . This technique is especially valuable in sectors such as healthcare or finance, where data sensitivity is critical. It provides a viable solution to data access limitations while ensuring compliance and development efficiency. ",
    "url": "/pages/synth_data.html#purpose-and-benefits",
    
    "relUrl": "/pages/synth_data.html#purpose-and-benefits"
  },"447": {
    "doc": "Synthetic data",
    "title": "Code example",
    "content": "1. Example with numeric dataset (mtcars) . Figure 1: Comparison of numeric data distributions for MPG, HP, and WT between original and synthetic data from the mtcars dataset. Each panel shows a density plot for one variable. Figure 2: Linear regression comparisons between MPG vs HP and WT vs MPG for original and synthetic mtcars data. Each plot includes a point scatter overlaid with a linear regression line. library(dplyr) library(ggplot2) library(synthpop) library(patchwork) # 1. example with mtcars numeric dataset ---- # Load mtcars dataset data(\"mtcars\") df &lt;- mtcars # Check dimensions of the data: rows and columns dim(df) # Check column types sapply(df, class) # Create a synthetic version of the dataset syn_df &lt;- syn(df, seed = 1234) # Generate synthetic data maintaining the distribution, mean, etc. synthetic_data &lt;- syn_df$syn # Print dimensions to confirm they match original data dim(synthetic_data) # Generate sample names using sprintf to ensure proper zero padding synthetic_data$Sample_Names &lt;- sprintf(\"AAA_%03d\", seq_len(nrow(synthetic_data))) # Function to compare distributions between original and synthetic datasets compare_plot &lt;- function(original, synthetic, col_name) { original_df &lt;- data.frame(Value = original[[col_name]], Type = \"Original\") synthetic_df &lt;- data.frame(Value = synthetic[[col_name]], Type = \"Synthetic\") combined_df &lt;- rbind(original_df, synthetic_df) ggplot(combined_df, aes(x = Value, fill = Type)) + geom_density(alpha = 0.5) + labs(title = paste(\"Density Plot for\", col_name)) + theme_minimal() } # Create comparison plots for a few example columns p1 &lt;- compare_plot(df, synthetic_data, \"mpg\") p2 &lt;- compare_plot(df, synthetic_data, \"hp\") p3 &lt;- compare_plot(df, synthetic_data, \"wt\") # save the combined plots patch1 &lt;- p1 / p2 / p3 ggsave(patch1, file =\"./patch1.png\") # Function to compare distributions and regression lines between original and synthetic datasets compare_plot_with_lm &lt;- function(original, synthetic, x_col, y_col) { original_df &lt;- data.frame(X = original[[x_col]], Y = original[[y_col]], Type = \"Original\") synthetic_df &lt;- data.frame(X = synthetic[[x_col]], Y = synthetic[[y_col]], Type = \"Synthetic\") combined_df &lt;- rbind(original_df, synthetic_df) ggplot(combined_df, aes(x = X, y = Y, color = Type)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + labs(title = paste(\"Comparison of Linear Regression\\nbetween Original and Synthetic Data for\", x_col, \"vs\", y_col)) + theme_minimal() } # Example plot for 'mpg' vs 'hp' p4 &lt;- compare_plot_with_lm(df, synthetic_data, \"mpg\", \"hp\") p5 &lt;- compare_plot_with_lm(df, synthetic_data, \"wt\", \"mpg\") # save the combined plots patch2 &lt;- p4 / p5 ggsave(patch2, file =\"./patch2.png\") . 2. Example with categorical dataset (iris) . Figure 3: Categorical and numerical data comparison from the iris dataset showing Sepal Length and Species distribution. The top panels are density and bar plots for Sepal Length and Species, respectively. The bottom panel shows a boxplot distribution of Sepal Length across different Species. # 2. example with iris categorical dataset ---- # Load iris dataset data(\"iris\") df &lt;- iris # Check dimensions of the data: rows and columns dim(df) # Check column types sapply(df, class) # Convert 'Species' to a factor if not already df$Species &lt;- as.factor(df$Species) # Create a synthetic version of the dataset syn_df &lt;- syn(df, seed = 1234) # Generate synthetic data maintaining the distribution, mean, etc. synthetic_data &lt;- syn_df$syn # Generate sample names using sprintf to ensure proper zero padding synthetic_data$Sample_Names &lt;- sprintf(\"BBB_%03d\", seq_len(nrow(synthetic_data))) # Print dimensions to confirm they match original data dim(synthetic_data) # Function to compare distributions between original and synthetic datasets compare_plot &lt;- function(original, synthetic, col_name) { original_df &lt;- data.frame(Value = original[[col_name]], Type = \"Original\") synthetic_df &lt;- data.frame(Value = synthetic[[col_name]], Type = \"Synthetic\") combined_df &lt;- rbind(original_df, synthetic_df) # Adjust the plot type based on data type if(is.numeric(original[[col_name]])) { plot &lt;- ggplot(combined_df, aes(x = Value, fill = Type)) + geom_density(alpha = 0.5) + labs(title = paste(\"Density Plot for\", col_name)) + theme_minimal() } else { plot &lt;- ggplot(combined_df, aes(x = Value, fill = Type)) + geom_bar(position = \"dodge\") + labs(title = paste(\"Bar Plot for\", col_name)) + theme_minimal() } print(plot) } # Create comparison plots for a few example columns, including the categorical 'Species' p6 &lt;- compare_plot(df, synthetic_data, \"Sepal.Length\") p7 &lt;- compare_plot(df, synthetic_data, \"Species\") # save the combined plots patch3 &lt;- p6 / p7 ggsave(patch3, file =\"./patch3.png\") # Function to compare distributions between original and synthetic datasets using boxplots compare_distribution &lt;- function(original, synthetic, x_col, y_col) { original_df &lt;- data.frame(X = original[[x_col]], Y = original[[y_col]], Type = \"Original\") synthetic_df &lt;- data.frame(X = synthetic[[x_col]], Y = synthetic[[y_col]], Type = \"Synthetic\") combined_df &lt;- rbind(original_df, synthetic_df) ggplot(combined_df, aes(x = X, y = Y, fill = Type)) + geom_boxplot(outlier.shape = NA) + # Hide outliers for clearer comparison geom_jitter() + # Add jitter to show data points labs(title = paste(\"Distribution of\", y_col, \"by\", x_col, \"\\nComparison between Original and Synthetic Data\"), x = x_col, y = y_col) + theme_minimal() } # Compare the distributions of 'Sepal.Length' across 'Species' p8 &lt;- compare_distribution(df, synthetic_data, \"Species\", \"Sepal.Length\") # save the combined plots patch3 &lt;- (p6 + p7) / p8 ggsave(patch3, file =\"./patch3.png\") . 3. Example of data with no existing version (medical) . Figure 4: Combined analysis of synthetic healthcare data. Shows the age distribution of patients categorized by disease status, with separate histograms for ‘Healthy’ and ‘Infected’. Displays the distribution of interferon levels across different hospitals. Figure 5: Interferon Levels by Age and Hospital, Colored by Disease Status. This scatter plot displays interferon levels versus age for patients, categorized by disease status and facetted by hospital. # 3. example of data with no existing version ---- # Define hospital names hospitals &lt;- c(\"City Hospital\", \"Regional Medical Center\", \"Downtown Clinic\", \"Suburban Hospital\", \"Community Health Center\") # Simulate data set.seed(123) # for reproducibility synthetic_data &lt;- data.frame( Hospital = sample(hospitals, 100, replace = TRUE), # Randomly assign hospital names Age = round(runif(100, 18, 90)), # Random ages between 18 and 90 Interferon = rnorm(100, 50, 20), # Normal distribution of interferon levels Disease_Status = sample(c(\"Healthy\", \"Infected\"), 100, replace = TRUE, prob = c(0.7, 0.3)) # 70% healthy, 30% infected ) # Generate sample names using sprintf to ensure proper zero padding synthetic_data$Sample_Names &lt;- sprintf(\"AAA_%03d\", seq_len(nrow(synthetic_data))) # View the first few rows of the synthetic data head(synthetic_data) # Plotting age distribution by disease status p9 &lt;- ggplot(synthetic_data, aes(x = Age, fill = Disease_Status)) + geom_histogram(bins = 15, alpha = 0.6, position = \"identity\") + labs(title = \"Age Distribution by Disease Status\", x = \"Age\", y = \"Count\") + theme_minimal() # Plotting Interferon levels by hospital p10 &lt;- ggplot(synthetic_data, aes(x = Interferon, fill = Hospital)) + geom_density(alpha = 0.7) + labs(title = \"Interferon Levels Distribution by Hospital\", x = \"Interferon Level\", y = \"Density\") + theme_minimal() # Plotting Interferon levels by hospital p11 &lt;- ggplot(synthetic_data, aes(x = Age, y = Interferon, color = Disease_Status)) + geom_point(alpha = 0.7) + labs(title = \"Interferon Levels Distribution by Hospital\") + facet_wrap(.~Hospital) + theme_minimal() patch4 &lt;- p9 + p10 p11 # Save plots ggsave(\"./patch4.png\", patch4) ggsave(\"./patch5.png\", p11) . ",
    "url": "/pages/synth_data.html#code-example",
    
    "relUrl": "/pages/synth_data.html#code-example"
  },"448": {
    "doc": "Synthetic data",
    "title": "Synthetic data",
    "content": "Last update: 20240819 . ",
    "url": "/pages/synth_data.html",
    
    "relUrl": "/pages/synth_data.html"
  },"449": {
    "doc": "Variables",
    "title": "Centralised environment variables",
    "content": "Last update: 20240820 . ",
    "url": "/pages/variables.html#centralised-environment-variables",
    
    "relUrl": "/pages/variables.html#centralised-environment-variables"
  },"450": {
    "doc": "Variables",
    "title": "Overview",
    "content": "To facilitate consistent paths and settings across multiple scripts, we use a single source file, variables.sh, which contains all the necessary environment variables. This method ensures that changes to paths or settings need to be updated in only one place, reducing errors and simplifying script maintenance. | For each script, shared variables will be sourced from the variables file. | Each pipeline can have its own custom variables file which will be sourced in its entirety or selectively from the master. | The variables file contains entries such as: | . DATABASE=\"./sph/database/\" REF_GRCh38=\"${DATABASE}/ref/grch38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz\" . | For all new permanent datasets, tools, etc. we add it to the index table. | We will assign the locations for all shared datasets, tools, etc. | See annotation table for the list of datasets. (This will be updated to include variables.) | . We will automatically generate the master variables file from the index table which contains meta data about dates, versions, application, etc. for each of the tools, databases, etc. Therefore, the only manual curation required is for the index table, rather than individual variables file/files. To be integrated on annotation table. +-- .. |-- (sph) |-- database |-- ref | |-- grch37 | |-- grch38 | +-- .. | |-- vep | |-- .. | |-- .. | +-- .. | |-- gnomad | |-- .. | |-- .. | +-- .. | |-- (other files, pages with no children) | +-- .. |-- tools |-- gatk | |-- vep | |-- .. | +-- .. | |-- vt | | +-- .. |-- (sph) +-- .. ",
    "url": "/pages/variables.html#overview",
    
    "relUrl": "/pages/variables.html#overview"
  },"451": {
    "doc": "Variables",
    "title": "Usage",
    "content": "This script is intended to be sourced, not executed directly. To use the variables defined in this script, you should source it at the beginning of your scripts: . source /path/to/variables.sh . ",
    "url": "/pages/variables.html#usage",
    
    "relUrl": "/pages/variables.html#usage"
  },"452": {
    "doc": "Variables",
    "title": "Variables defined",
    "content": "Raw data . Variables related to the location and structure of raw data: . | PROD: Root directory for production-level data. | TRANSFER: Subdirectory for transferred data. | BATCH: Specific batch identifier for datasets. | RAWDATA: Full path to the raw data. | CHECKSUM: Path to the checksum file for verifying data integrity. | . PROD=\"/project/data/prod\" TRANSFER=\"h2030gc04072023/downloads\" BATCH=\"WGS_NDS_SwissPedHealth_Oct22\" RAWDATA=\"${PROD}/${TRANSFER}/${BATCH}\" CHECKSUM=\"${RAWDATA}/..._deliverables_final.md5\" . Processed data . Variables associated with directories for processed data: . | HOME: Home directory for project-specific data. | DATA: Root directory for all genomic data. | FASTP_DIR: Directory for FASTP outputs. | FASTP_METRICS_DIR: Directory for FASTP metrics. | BAM_DIR: Directory for BAM files. | SAMSORT_DIR: Directory for sorted SAM files. | RMDUP_DIR: Directory for duplicate-removed data. | RMDUP_MERGE_DIR: Directory for merged BAM files post-duplication removal. | BQSR_DIR: Directory for Base Quality Score Recalibration (BQSR) outputs. | BQSR_DIR_HOLD: Temporary holding directory for BQSR data. | . HOME=\"/project/home/lawless\" DATA=\"${HOME}/data/wgs\" FASTP_DIR=\"${DATA}/fastp\" FASTP_METRICS_DIR=\"${DATA}/fastp_reports\" BAM_DIR=\"${DATA}/bam\" SAMSORT_DIR=\"${DATA}/samsort\" RMDUP_DIR=\"${DATA}/rmdup\" RMDUP_MERGE_DIR=\"${DATA}/rmdup_merge\" BQSR_DIR=\"${DATA}/bqsr\" BQSR_DIR_HOLD=\"${DATA}/bqsr_hold\" . ",
    "url": "/pages/variables.html#variables-defined",
    
    "relUrl": "/pages/variables.html#variables-defined"
  },"453": {
    "doc": "Variables",
    "title": "Best practices",
    "content": ". | Source Reliability: Always ensure the variables.sh script is sourced at the start of each script to maintain consistency across the workflow. | Path Validation: After sourcing, it is good practice to check that critical paths exist or are accessible to prevent runtime errors: . if [ ! -d \"$BQSR_DIR\" ]; then echo \"Error: BQSR directory does not exist.\" exit 1 fi . | Security and Permissions: Regularly check that permissions and ownerships are correctly set on directories to prevent unauthorized access or data loss. | . ",
    "url": "/pages/variables.html#best-practices",
    
    "relUrl": "/pages/variables.html#best-practices"
  },"454": {
    "doc": "Variables",
    "title": "Conclusion",
    "content": "Using a centralized script for environment variables helps streamline configuration management in large projects, especially those involving multiple stages of data processing in genomic research. ",
    "url": "/pages/variables.html#conclusion",
    
    "relUrl": "/pages/variables.html#conclusion"
  },"455": {
    "doc": "Variables",
    "title": "Variables",
    "content": " ",
    "url": "/pages/variables.html",
    
    "relUrl": "/pages/variables.html"
  },"456": {
    "doc": "Variant to RDF concept",
    "title": "Variant features to RDF concept metadata",
    "content": "This process is starting with sequencing_assay, which includes library_preparation, sequencing_run, etc. We will continue through the pipeline until we reach the final end-point required to report a pathogenic variant. This documentation outlines the transformation of variant information from whole genome sequence (WGS) data to a format adhering to RDF structure data concepts. The aim is to ensure that the omic output from genomic analyses can be seamlessly integrated into clinical data warehouses with high fidelity and clarity. Number of variables: . | All SPHN RDF concept info (see SPHN_dataset_release_2024_2_20240502.xlsx) = 1503 | Subset of relevant concepts = 76 (see example_subset_concepts.tsv) | Relevant WGS pipeline logs = 62 (see example_report.tsv) | Currently automated match = 13 (see example_report_concepts.Rds) . This repository uses a public dataset of example genetic variants and sequencing/analysis log data. | . ",
    "url": "/pages/variant_concept.html#variant-features-to-rdf-concept-metadata",
    
    "relUrl": "/pages/variant_concept.html#variant-features-to-rdf-concept-metadata"
  },"457": {
    "doc": "Variant to RDF concept",
    "title": "Overview",
    "content": "The process begins with the extraction of variant data from a genomic study, (no sensitive data is included in the public example set). The key variant features such as Chromosome (CHROM), Position (POS), Reference Allele (REF), and Alternate Allele (ALT) are formatted alongside metadata that describes their relationship to RDF concepts. This ensures downstream users can map these data accurately within clinical and research frameworks. This document is to be updated as we improve the linking of result terms to SPHN_dataset_release_2024_2_20240502.xlsx which is critical so that downstream users can correctly map data. ",
    "url": "/pages/variant_concept.html#overview",
    
    "relUrl": "/pages/variant_concept.html#overview"
  },"458": {
    "doc": "Variant to RDF concept",
    "title": "Aims",
    "content": ". | Data preparation: Start with the extracted variant information from the genomic pipeline. | Key term identification: Focus on essential genomic terms like CHROM, POS, REF, and ALT, Sequencing run, Sequencing instrument. | Metadata addition: Attach metadata columns that specify RDF concept requirements such as type and cardinality. | Validation checklist: . | Do we have all necessary variant descriptors present? | Is there inclusion and accuracy of all metadata explanations? | Is there alignment of metadata with SPHN omic concepts? | Downstream users (mapping) can choose from TSV, HTML, JSON, and Rds. Any others needed? | . | . ",
    "url": "/pages/variant_concept.html#aims",
    
    "relUrl": "/pages/variant_concept.html#aims"
  },"459": {
    "doc": "Variant to RDF concept",
    "title": "Current version",
    "content": "The observation column is highlighted in GREEN. It contains the data which we report as output from the pipeline for use in our database Here is the completed concept observations (this is file example_report_concepts.html): . | cardinalityViolated | concept_reference_general_concept_name | concept_reference | general_concept_name | observation | release | unique_ID | IRI | active_status_(yes/no) | deprecated_in | replaced_by | concept_or_concept_compositions_or_inherited | general_description | contextualized_concept_name | contextualized_description | parent | type | excluded_type_descendants | standard | value_set_or_subset | meaning_binding | additional_information | cardinality_for_composedOf | cardinality_for_concept_to_Administrative_Case | cardinality_for_concept_to_Data_Provider | cardinality_for_concept_to_Subject_Pseudo_Identifier | cardinality_for_concept_to_Source_System | sensitive_(yes/no) | color_inherited | color_reference | color_observation | color_cardinality | . | FALSE | sequencing_assay_sequencing_assay | sequencing_assay | sequencing_assay | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#sequencingassay | yes | NA | NA | concept | an_assay_that_exploits_a_sequencer_as_the_instrument_to_generate_results | sequencing_assay | an_assay_that_exploits_a_sequencer_as_the_instrument_to_generate_results | assay | assay | NA | NA | NA | efo:0003740_|assay_by_sequencer| | NA | NA | 0:n | 1:1 | 0:n | 1:n | NA | #7CCAFF | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_standard_operating_procedure | sequencing_assay | standard_operating_procedure | wgs_with_illumina_novaseq_6000 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasstandardoperatingprocedure | yes | NA | NA | inherited | standard_operating_procedure_associated_to_the_concept | standard_operating_procedure | standard_operating_procedure_that_was_followed_for_this_sequencing_assay | sphnattributeobject | standard_operating_procedure | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_predecessor | sequencing_assay | predecessor | kispi_custom_sample_prep_v1 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#haspredecessor | yes | NA | NA | inherited | process_preceding_this_concept | predecessor | sample_processing_preceding_the_sequencing_assay | sphnattributeobject | sample_processing | NA | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_code | sequencing_assay | code | efo_0022396 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hascode | yes | NA | NA | inherited | coded_information_specifying_the_concept | code | code_specifying_the_type_of_sequencing_assay | sphnattributeobject | code | NA | efo;_obi_or_other | for_efo:_descendant_of:_efo:0001455_|assay|;_for_obi:_descendant_of:_obi:0000070_|assay| | NA | NA | 1:1 | NA | NA | NA | NA | NA | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_identifier | sequencing_assay | identifier | obo:obi_002117_(wgs) | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasidentifier | yes | NA | NA | inherited | unique_identifier_identifying_the_concept | identifier | unique_identifier_identifying_the_sequencing_assay | sphnattributedatatype | string | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_start_datetime | sequencing_assay | start_datetime | jul 01 2023 01:01:01 gmt / v0.9.0 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasstartdatetime | yes | NA | NA | inherited | datetime_at_which_the_concept_started | start_datetime | datetime_at_which_the_sequencing_assay_was_first_executed | hasdatetime | temporal | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | yes | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_data_file | sequencing_assay | data_file | out.fastq | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasdatafile | yes | NA | NA | inherited | data_file_associated_to_the_concept | data_file | data_file_associated_to_the_sequencing_assay | sphnattributeobject | data_file | time_series_data_file | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_sample | sequencing_assay | sample | blood_sample_1 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hassample | yes | NA | NA | inherited | sample_associated_to_the_concept | sample | material_that_is_being_sequenced_by_this_sequencing_assay | sphnattributeobject | sample | tumor_specimen;_isolate | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #abb1cf | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_library_preparation | sequencing_assay | library_preparation | illumina_truseq_dna_pcr-free | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#haslibrarypreparation | yes | NA | NA | composedof | library_preparation_associated_to_the_concept | library_preparation | the_library_preparation_that_is_part_of_the_sequencing_assay | sphnattributeobject | library_preparation | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_sequencing_instrument | sequencing_assay | sequencing_instrument | a00485 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hassequencinginstrument | yes | NA | NA | composedof | device_associated_to_the_concept | sequencing_instrument | the_device_which_is_used_to_perform_the_sequencing_assay | sphnattributeobject | sequencing_instrument | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_sequencing_run | sequencing_assay | sequencing_run | 334 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hassequencingrun | yes | NA | NA | composedof | sequencing_run_associated_to_the_concept | sequencing_run | sequencing_run_performed_as_part_of_the_sequencing_assay | sphnattributeobject | sequencing_run | NA | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #92a8d1 | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_intended_read_length | sequencing_assay | intended_read_length | 150 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasintendedreadlength | yes | NA | NA | composedof | intended_read_length_associated_to_the_concept | intended_read_length | the_number_of_nucleotides_intended_to_be_ordered_from_each_side_of_a_nucleic_acid_fragment_obtained_after_the_completion_of_a_sequencing_assay | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_assay_intended_read_depth | sequencing_assay | intended_read_depth | 30x | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasintendedreaddepth | yes | NA | NA | composedof | intended_read_depth_associated_to_the_concept | intended_read_depth | the_number_of_times_a_particular_locus_(site,_nucleotide,_amplicon,_region)_was_intended_to_be_sequenced_as_part_of_the_sequencing_assay | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7cac9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_library_preparation | library_preparation | library_preparation | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#librarypreparation | yes | NA | NA | concept | process_which_results_in_the_creation_of_a_library_from_fragments_of_dna | library_preparation | process_which_results_in_the_creation_of_a_library_from_fragments_of_dna | sampleprocessing | sample_processing | NA | NA | NA | obi:0000711_|library_preparation| | NA | NA | 0:n | 1:1 | 0:n | 1:n | NA | #7CCAFF | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_code | library_preparation | code | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hascode | yes | NA | NA | inherited | coded_information_specifying_the_concept | code | code_specifying_the_type_of_library_preparation | sphnattributeobject | code | NA | obi;_efo_or_other | for_obi:_descendant_of:_obi:0000711_|library_preparation| | NA | NA | 0:1 | NA | NA | NA | NA | NA | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_input | library_preparation | input | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasinput | yes | NA | NA | inherited | input_associated_to_the_concept | input | the_sample_for_which_a_library_is_created | sphnattributeobject | sample | NA | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_output | library_preparation | output | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasoutput | yes | NA | NA | inherited | output_associated_to_the_concept | output | the_ngs_library_that_is_produced | sphnattributeobject | sample | tumor_specimen | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_start_datetime | library_preparation | start_datetime | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasstartdatetime | yes | NA | NA | inherited | datetime_at_which_the_concept_started | start_datetime | start_of_library_preparation | hasdatetime | temporal | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | yes | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_quality_control_metric | library_preparation | quality_control_metric | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasqualitycontrolmetric | yes | NA | NA | inherited | quality_control_metric_associated_to_the_concept | quality_control_metric | quality_control_metric_related_to_the_output_of_the_library_preparation | sphnattributeobject | quality_control_metric | NA | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_predecessor | library_preparation | predecessor | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#haspredecessor | yes | NA | NA | inherited | process_preceding_this_concept | predecessor | process_preceding_this_library_preparation | sphnattributeobject | sample_processing | NA | NA | NA | NA | NA | 0:n | NA | NA | NA | NA | NA | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_standard_operating_procedure | library_preparation | standard_operating_procedure | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasstandardoperatingprocedure | yes | NA | NA | inherited | standard_operating_procedure_associated_to_the_concept | standard_operating_procedure | standard_operating_procedure_that_was_followed_for_this_library_preparation | sphnattributeobject | standard_operating_procedure | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #abb1cf | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_kit_code | library_preparation | kit_code | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#haskitcode | yes | NA | NA | composedof | coded_information_specifying_the_kit_associated_to_the_concept | library_preparation_kit_code | pre-filled,_ready-to-use_reagent_cartridges_intended_to_improve_chemistry,_cluster_density_and_read_length_as_well_as_improve_quality_(q)_scores_for_this_sample._reagent_components_are_encoded_to_interact_with_the_sequencing_system_to_validate_compatibility_with_user-defined_applications. | hascode | code | NA | efo,_genepio,_fairgenomes_or_other | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_target_enrichment_kit_code | library_preparation | target_enrichment_kit_code | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hastargetenrichmentkitcode | yes | NA | NA | composedof | coded_information_specifying_the_target_enrichment_kit_associated_to_the_concept | target_enrichment_kit_code | indicates_which_target_enrichment_kit_was_used_to_prepare_this_sample._target_enrichment_is_a_pre-sequencing_dna_preparation_step_where_dna_sequences_are_either_directly_amplified_(amplicon_or_multiplex_pcr-based)_or_captured_(hybrid_capture-based)_in_order_to_only_focus_on_specific_regions_of_a_genome_or_dna_sample. | hascode | code | NA | efo,_genepio,_fairgenomes_or_other | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_intended_insert_size | library_preparation | intended_insert_size | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasintendedinsertsize | yes | NA | NA | composedof | intended_insert_size_associated_to_the_concept | intended_insert_size | in_paired-end_sequencing,_the_dna_between_the_adapter_sequences_is_the_insert._the_length_of_this_sequence_is_known_as_the_insert_size,_not_to_be_confused_with_the_inner_distance_between_reads._so,_fragment_length_equals_read_adapter_length_(2x)_plus_insert_size,_and_insert_size_equals_read_length_(2x)_plus_inner_distance. | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | library_preparation_gene_panel | library_preparation | gene_panel | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasgenepanel | yes | NA | NA | composedof | gene_panel_associated_to_the_concept | gene_panel | collection_of_genes_that_are_the_focus_of_sequencing | sphnattributeobject | gene_panel | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7f6c9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_instrument_sequencing_instrument | sequencing_instrument | sequencing_instrument | a00485 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#sequencinginstrument | yes | NA | NA | concept | a_sequencing_instrument_that_is_used_in_a_sequencing_assay | sequencing_instrument | a_sequencing_instrument_that_is_used_in_a_sequencing_assay | sphnconcept | NA | NA | NA | NA | efo:0000548_|instrument| | NA | NA | NA | 1:1 | NA | NA | NA | #7CCAFF | #f7e7c9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_instrument_code | sequencing_instrument | code | a00485 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hascode | yes | NA | NA | composedof | coded_information_specifying_the_concept | code | code_specifying_the_type_of_sequencing_instrument | sphnattributeobject | code | NA | obi;_efo_or_other | for_obi:_descendant_of:_obi:0400103_|dna_sequencer|;_for_efo:_descendant_of:_efo:0003739_|sequencer| | NA | NA | 1:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7e7c9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_sequencing_run | sequencing_run | sequencing_run | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#sequencingrun | yes | NA | NA | concept | the_valid_and_completed_operation_of_a_high-throughput_sequencing_instrument_associated_with_a_sequencing_assay | sequencing_run | the_valid_and_completed_operation_of_a_high-throughput_sequencing_instrument_associated_with_a_sequencing_assay | sphnconcept | NA | NA | NA | NA | ncit:c148088_|sequencing_run| | NA | NA | NA | 1:1 | NA | NA | NA | #7CCAFF | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_identifier | sequencing_run | identifier | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasidentifier | yes | NA | NA | composedof | unique_identifier_identifying_the_concept | identifier | unique_identifier_identifying_the_sequencing_run | sphnattributedatatype | string | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_datetime | sequencing_run | datetime | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasdatetime | yes | NA | NA | composedof | datetime_of_the_concept | datetime | datetime_the_sequencing_run_was_performed | sphnattributedatatype | temporal | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | yes | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_read_count | sequencing_run | read_count | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasreadcount | yes | NA | NA | composedof | ready_count_associated_with_to_concept | read_count | the_number_of_sequencing_reaction_results_that_were_pooled_to_assemble_a_sequence_for_a_genomic_region_of_interest_in_a_sequencing_run | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_average_insert_size | sequencing_run | average_insert_size | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasaverageinsertsize | yes | NA | NA | composedof | average_insert_size_associated_to_the_concept | average_insert_size | the_average_insert_size_found_during_the_nucleic_acid_sequencing_run | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_average_read_length | sequencing_run | average_read_length | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasaveragereadlength | yes | NA | NA | composedof | average_read_length_associated_to_the_concept | average_read_length | the_average_length_for_nucleic_acid_sequencing_reads_generated_in_a_sequencing_run | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_mean_read_depth | sequencing_run | mean_read_depth | NA | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasmeanreaddepth | yes | NA | NA | composedof | mean_read_depth_associated_to_the_concept | mean_read_depth | the_number_of_times_a_particular_locus_(site,_nucleotide,_amplicon,_region)_was_sequenced_in_a_sequencing_run | hasquantity | quantity | NA | NA | NA | NA | NA | 0:1 | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_data_file | sequencing_run | data_file | ../out/example_wgs_sequencing_report_deliverable_summary.tsv | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasdatafile | yes | NA | NA | composedof | data_file_associated_to_the_concept | data_file | data_file_associated_to_the_sequencing_run | sphnattributeobject | data_file | time_series_data_file | NA | NA | NA | NA | 1:n | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . | FALSE | sequencing_run_quality_control_metric | sequencing_run | quality_control_metric | 5f4dcc3b5aa765d61d8327deb882cf99 | 2024.1 | NA | https://www.biomedit.ch/rdf/sphn-schema/sphn/2024/1#hasqualitycontrolmetric | yes | NA | NA | composedof | quality_control_metric_associated_to_the_concept | quality_control_metric | quality_control_metric_associated_with_the_sequencing_run | sphnattributeobject | quality_control_metric | NA | NA | NA | NA | NA | 1:n | NA | NA | NA | NA | NA | #92a8d1 | #f7dac9 | #8ed3a0 | #8ed3a0 | . ",
    "url": "/pages/variant_concept.html#current-version",
    
    "relUrl": "/pages/variant_concept.html#current-version"
  },"460": {
    "doc": "Variant to RDF concept",
    "title": "Semantic evidence network",
    "content": "I have added the following method to automatically plot semantic evidence networks which show how evidence provenance has been generated. The dataset is organised into three hierarchical grouping levels based on the column concept_or_concept_compositions_or_inherited. The top level, Level 1, includes entries where this column equals “concept”. The subsequent levels, Level 2 and Level 3, contain entries where this column does not equal “concept”. The distinction between Levels 2 and 3 lies in the presence of distinct observations; Level 3 specifically represents the final observation associated with the general concept names from Level 2, differentiated further by non-empty values in the observation column, making Level 3 essentially a detailed continuation of Level 2. Nodes within the network are structured with the following attributes: general_concept_name, id, group, name, and observation, where general_concept_name recurs in both Level 2 and Level 3 but differs based on the associated observation. Edges within this hierarchical setup link nodes from Level 1 to Level 2 and from Level 2 to Level 3 using general_concept_name as a consistent link identifier, facilitating a connection between the initial abstract concept level and its more detailed observational breakdowns. ",
    "url": "/pages/variant_concept.html#semantic-evidence-network",
    
    "relUrl": "/pages/variant_concept.html#semantic-evidence-network"
  },"461": {
    "doc": "Variant to RDF concept",
    "title": "Downloads",
    "content": "Example output (in mutiple filetypes) can be downloaded from the public set: . | File Name | Download Link | . | example_report_concepts.tsv | Download | . | example_report_concepts.html | Download | . | example_report_concepts.Rds | Download | . | example_report_concepts.Rds | Download | . | example_report_concepts.Rds | Download | . | pdf plot_semantic_evidence_plot_network.pdf | Download | . | pdf plot_semantic_evidence_plot_sankey.pdf | Download | . | html plot_semantic_evidence_plot_network.html | Download | . | html plot_semantic_evidence_plot_sankey.html | Download | . Example inputs can be downloaded from the public set: . | File Name | Download Link | . | Canton_001_NGS000012345_NA_S46_L001_R1_001.fastq_head.text | Download | . | Canton_001_NGS000012345_NA_S46_L001_R1_001_sample_seq_assay_log.text | Download | . | SPHN_dataset_release_2024_2_20240502.xlsx | Download | . | sequencing assay_van_der_Horst2023.txt | Download | . | bwa_10351_101_10453.out.text | Download | . | example_variant.Rds | Download | . | example_variant.tsv | Download | . ",
    "url": "/pages/variant_concept.html#downloads",
    
    "relUrl": "/pages/variant_concept.html#downloads"
  },"462": {
    "doc": "Variant to RDF concept",
    "title": "Process Steps for Variant Features to RDF Concept Mapping",
    "content": "This section outlines the sequential processing steps from data extraction through to the final merged dataset, prepared for RDF concept mapping. Each step corresponds to a specific script and handles distinct data types or stages in data preparation and merging. | Export Variant Data from Study . | Extracts variant data from genomic projects focusing on specific genes and filtering for high-impact variants, saving them in formats like RDS and TSV for further processing. | . | Read Variant Report Data . | Loads and transforms variant data into a long format to facilitate metadata annotation, preparing the data by adding a column for metadata requirements. | . | Read Sequencing Assay Data . | Extracts key sequencing assay data such as identifiers, read depth, and file formats from logs or metadata files, providing crucial context for sequencing parameters. | . | Read BWA Read Group Data . | Parses BWA and samtools log files to extract detailed read group information, including metadata about the sequencing run such as machine, file paths, and read group specifications. | . | Read Fastq Header Data - Analyzes headers from FASTQ files to extract sequencing instrument details and run metrics, offering a granular look at the sequencing runs which is instrumental in validating sequencing quality and parameters. | Merge Datasets . | Combines all processed data from the previous steps into a single dataset, aligning them by common identifiers and ensuring consistency across data types. | . | Map Pipeline Output to SPHN Concepts . | Maps the merged dataset to standard SPHN RDF concepts, ensuring each data point is correctly classified according to standardized ontology, thus aligning detailed genomic data with broader healthcare data standards. | . | . ",
    "url": "/pages/variant_concept.html#process-steps-for-variant-features-to-rdf-concept-mapping",
    
    "relUrl": "/pages/variant_concept.html#process-steps-for-variant-features-to-rdf-concept-mapping"
  },"463": {
    "doc": "Variant to RDF concept",
    "title": "Terms used in WGS logging",
    "content": "Descriptions for sequencing assay (WGS) terms . | Column Name | Description | . | seq_assay_identifier | The unique identifier for the sequencing assay, typically a standard ontology term such as obo:OBI_002117 for Whole Genome Sequencing. | . | seq_assay_intended_read_depth | The targeted depth of coverage for the sequencing assay, indicating how many times each base is expected to be sequenced; in this case, 150x. | . | seq_assay_intended_read_length | The expected length of each read in the sequencing process, measured in base pairs; here, 20 bp. | . | data_file_identifier | Identifier for the data file output from the sequencing, used to trace and access the file within data systems. | . | data_file_format | The format of the sequencing data files, specifying the standard used; here, EDAM format 1931, which is typical for FASTQ files from Illumina platforms. | . | quality_control_name | The name of the metric used to assess the quality of the sequencing data; in this case, the Phred quality score. | . | quality_control_value | The actual quality score achieved, indicating the reliability of the sequencing reads; 78.33% in this context. | . | library_prep_kit | Specifies the kit used for preparing DNA libraries for sequencing, critical for understanding the sample preparation methodology; Illumina TruSeq DNA PCR-Free is noted for high fidelity. | . | sample_identifier | The unique identifier for the sample being sequenced, used for tracking and reference throughout the sequencing process. | . | sample_material_type | The type of biological material from which the sample was derived, with its specific ontology code; snomed:119297000 denotes a blood sample. | . | seq_instrument_code | The identifier for the sequencing instrument used, linking to specific equipment details; obo: OBI_0002630 refers to the Illumina NovaSeq 6000. | . | sop_name | The name of the Standard Operating Procedure followed during the sequencing, ensuring consistency and reproducibility; here, “WGS with Illumina NovaSeq 6000”. | . | sop_description | A brief description of the SOP, providing context and specifics about the sequencing approach used. | . | sop_version | The version number of the SOP, which helps in identifying any changes or updates that might affect the sequencing output or interpretation. | . Descriptions for FASTQ/BAM readgroup terms . | Column Name | Description | . | START AT | The start timestamp of the sequencing or analysis process. | . | END AT | The end timestamp of the sequencing or analysis process. | . | sample_read_id | A unique identifier for each sample read in the process. | . | rel_dir | Relative directory path where sequencing data is stored. | . | dir_id | Directory identifier combining the project and sample ID. | . | FILE1 | Path to the first FASTQ file generated by sequencing. | . | FILE2 | Path to the second FASTQ file generated by sequencing. | . | output_file | Path to the final BAM file generated after processing. | . | ID | Internal identifier used to track the sample in analysis. | . | SM | Sample name or identifier used within the BAM file. | . | PL | Sequencing platform used, indicating technology type. | . | PU | Platform unit (PU) tag, often a barcode identifier. | . | LB | Library ID which is crucial for distinguishing between libraries prepared differently. | . | RG | Read group identifier in a BAM file, encapsulating all other identifiers. | . Descriptions for genetic variants terms . | Column Name | Description | . | sample.id | Unique identifier for each sample. | . | rownames | Row names corresponding to data entries. | . | CHROM | Chromosome number where the variant is located. | . | REF | Reference allele at the variant locus. | . | ALT | Alternate allele at the variant locus. | . | POS | Position of the variant on the chromosome. | . | start | Start position of the variant. | . | end | End position of the variant. | . | width | Width of the variant region. | . | Gene | Gene name associated with the variant. | . | SYMBOL | Gene symbol. | . | HGNC_ID | HUGO Gene Nomenclature Committee ID. | . | HGVSp | Human Genome Variation Society protein nomenclature. | . | HGVSc | Human Genome Variation Society coding DNA sequence nomenclature. | . | Consequence | Consequence of the variant. | . | IMPACT | Impact of the variant on the gene or protein function. | . | genotype | Genotype showing the variant alleles. | . | Feature_type | Type of genomic feature (e.g., transcript, regulatory). | . | Feature | Specific feature affected by the variant (e.g., exon, intron). | . | BIOTYPE | Biological type of the feature affected (e.g., protein_coding, miRNA). | . | VARIANT_CLASS | Classification of the variant based on its genomic context. | . | CANONICAL | Indicates if the transcript is the canonical transcript. | . | CHROM_Metadata | Type: Chromosome; Cardinality: 1:1; Value Set: SNOMED CT: 91272006, LOINC:48000-4 | . | POS_Metadata | Type: Genomic Position; Cardinality: 1:1; Value Set: GENO:0000902 | . | REF_Metadata | Type: Reference Allele; Cardinality: 1:1; Value Set: string | . | ALT_Metadata | Type: Alternate Allele; Cardinality: 1:1; Value Set: string | . ",
    "url": "/pages/variant_concept.html#terms-used-in-wgs-logging",
    
    "relUrl": "/pages/variant_concept.html#terms-used-in-wgs-logging"
  },"464": {
    "doc": "Variant to RDF concept",
    "title": "Variant to RDF concept",
    "content": "Last update: 20240920 . | Variant features to RDF concept metadata . | Overview | Aims | Current version | Semantic evidence network | Downloads | Process Steps for Variant Features to RDF Concept Mapping | Terms used in WGS logging . | Descriptions for sequencing assay (WGS) terms | Descriptions for FASTQ/BAM readgroup terms | Descriptions for genetic variants terms | . | . | . ",
    "url": "/pages/variant_concept.html",
    
    "relUrl": "/pages/variant_concept.html"
  },"465": {
    "doc": "VCF - Variant Call Format",
    "title": "VCF - Variant Call Format",
    "content": "This page has been directly cloned from GATK / Technical Documentation / Glossary https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format. Please check the original source if it is available for updates. The VCF specifications can be read directly from the original documentation. However, this page is very useful and is thus preserved here for reference. | GATK Team | June 25, 2024 08:15 Updated | . Contents . | Overview | Structure of a VCF file | Interpreting the header information | Structure of variant call records | Interpreting genotype and other sample-level information | Basic operations: validating, subsetting and exporting from a VCF | Merging VCF files | . ",
    "url": "/pages/vcf.html#vcf---variant-call-format",
    
    "relUrl": "/pages/vcf.html#vcf---variant-call-format"
  },"466": {
    "doc": "VCF - Variant Call Format",
    "title": "1. Overview",
    "content": "VCF, or Variant Call Format, It is a standardized text file format used for representing SNP, indel, and structural variation calls. The VCF specification used to be maintained by the 1000 Genomes Project, but its management and further development has been taken over by the Genomic Data Toolkit team of the Global Alliance for Genomics and Health. The full format spec can be found in the Samtools/Hts-specs repository, along with other useful specifications like SAM/BAM/CRAM. We highly encourage you to take a look at those documents, as they contain a lot of useful information that we do not go over in this document. VCF is the primary (and only well-supported) format used by the GATK for variant calls. We prefer it above all others because while it can be a bit verbose, the VCF format is very explicit about the exact type and sequence of variation as well as the genotypes of multiple samples for this variation. That being said, this highly detailed information can be challenging to understand. The information provided by the GATK tools that infer variation from high-throughput sequencing data, such as the HaplotypeCaller, is especially complex. This document describes the key features and annotations that you need to know about in order to understand VCF files output by the GATK tools. Note that VCF files are plain text files, so you can open them for viewing or editing in any text editor, with the following caveats: . | Some VCF files are very large, so your personal computer may struggle to load the whole file into memory. In such cases, you may need to use a different approach, such as using UNIX tools to access the part of the dataset that is relevant to you, or subsetting the data using tools like GATK's SelectVariants. | NEVER EDIT A VCF IN A WORD PROCESSOR SUCH AS MICROSOFT WORD BECAUSE IT WILL SCREW UP THE FORMAT! You have been warned :) . | Do not write home-brewed VCF parsing scripts — it never ends well. | . ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"467": {
    "doc": "VCF - Variant Call Format",
    "title": "2. Structure of a VCF file",
    "content": "A valid VCF file is composed of two main parts: the header, and the variant call records. The header contains information about the dataset and relevant reference sources (e.g. the organism, genome build version etc.), as well as definitions of all the annotations used to qualify and quantify the properties of the variant calls contained in the VCF file. The header of VCFs generated by GATK tools also include the command line that was used to generate them. Some other programs also record the command line in the VCF header, but not all do so as it is not required by the VCF specification. For more information about the header, see the next section. The actual data lines will look something like this: . [HEADER LINES] #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878 20 10001019 . T G 364.77 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.699;ClippingRankSum=0.00;DP=34;ExcessHet=3.0103;FS=3.064;MLEAC=1;MLEAF=0.500;MQ=42.48;MQRankSum=-3.219e+00;QD=11.05;ReadPosRankSum=-6.450e-01;SOR=0.537 GT:AD:DP:GQ:PL 0/1:18,15:33:99:393,0,480 20 10001298 . T A 884.77 . AC=2;AF=1.00;AN=2;DP=30;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.49;SOR=1.765 GT:AD:DP:GQ:PL 1/1:0,30:30:89:913,89,0 20 10001436 . A AAGGCT 1222.73 . AC=2;AF=1.00;AN=2;DP=29;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=25.36;SOR=0.836 GT:AD:DP:GQ:PL 1/1:0,28:28:84:1260,84,0 20 10001474 . C T 843.77 . AC=2;AF=1.00;AN=2;DP=27;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=31.25;SOR=1.302 GT:AD:DP:GQ:PL 1/1:0,27:27:81:872,81,0 20 10001617 . C A 493.77 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.63;ClippingRankSum=0.00;DP=38;ExcessHet=3.0103;FS=1.323;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=12.99;ReadPosRankSum=0.170;SOR=1.179 GT:AD:DP:GQ:PL 0/1:19,19:38:99:522,0,480 . After the header lines and the field names, each line represents a single variant, with various properties of that variant represented in the columns. Note that all the lines shown in the example above describe SNPs and indels, but other variation types could be described (see the VCF specification for details). Depending on how the callset was generated, there may only be records for sites where a variant was identified, or there may also be \"invariant\" records, ie records for sites where no variation was identified. You will sometimes come across VCFs that have only 8 columns, and contain no FORMAT or sample-specific information. These are called \"sites-only\" VCFs, and represent variation that has been observed in a population. Generally, information about the population of origin should be included in the header. ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"468": {
    "doc": "VCF - Variant Call Format",
    "title": "3. Interpreting the header information",
    "content": "The following is a valid VCF header produced by GenotypeGVCFs on an example data set (derived from our favorite test sample, NA12878). You can download similar test data from our resource bundle and try looking at it yourself. ##fileformat=VCFv4.2 ##ALT=&lt;ID=NON_REF,Description=\"Represents any possible alternative allele at this location\"&gt; ##FILTER=&lt;ID=LowQual,Description=\"Low quality\"&gt; ##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Allelic depths for the ref and alt alleles in the order listed\"&gt; ##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"&gt; ##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"&gt; ##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt; ##FORMAT=&lt;ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum DP observed within the GVCF block\"&gt; ##FORMAT=&lt;ID=PGT,Number=1,Type=String,Description=\"Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another\"&gt; ##FORMAT=&lt;ID=PID,Number=1,Type=String,Description=\"Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group\"&gt; ##FORMAT=&lt;ID=PL,Number=G,Type=Integer,Description=\"Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification\"&gt; ##FORMAT=&lt;ID=RGQ,Number=1,Type=Integer,Description=\"Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)\"&gt; ##FORMAT=&lt;ID=SB,Number=4,Type=Integer,Description=\"Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.\"&gt; ##GATKCommandLine.HaplotypeCaller=&lt;ID=HaplotypeCaller,Version=3.7-0-gcfedb67,Date=\"Fri Jan 20 11:14:15 EST 2017\",Epoch=1484928855435,CommandLineOptions=\"[command-line goes here]\"&gt; ##GATKCommandLine=&lt;ID=GenotypeGVCFs,CommandLine=\"[command-line goes here]\",Version=4.beta.6-117-g4588584-SNAPSHOT,Date=\"December 23, 2017 5:45:56 PM EST\"&gt; ##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\"&gt; ##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele Frequency, for each ALT allele, in the same order as listed\"&gt; ##INFO=&lt;ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\"&gt; ##INFO=&lt;ID=BaseQRankSum,Number=1,Type=Float,Description=\"Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities\"&gt; ##INFO=&lt;ID=ClippingRankSum,Number=1,Type=Float,Description=\"Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases\"&gt; ##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth; some reads may have been filtered\"&gt; ##INFO=&lt;ID=DS,Number=0,Type=Flag,Description=\"Were any of the samples downsampled?\"&gt; ##INFO=&lt;ID=END,Number=1,Type=Integer,Description=\"Stop position of the interval\"&gt; ##INFO=&lt;ID=ExcessHet,Number=1,Type=Float,Description=\"Phred-scaled p-value for exact test of excess heterozygosity\"&gt; ##INFO=&lt;ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\"&gt; ##INFO=&lt;ID=HaplotypeScore,Number=1,Type=Float,Description=\"Consistency of the site with at most two segregating haplotypes\"&gt; ##INFO=&lt;ID=InbreedingCoeff,Number=1,Type=Float,Description=\"Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation\"&gt; ##INFO=&lt;ID=MLEAC,Number=A,Type=Integer,Description=\"Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed\"&gt; ##INFO=&lt;ID=MLEAF,Number=A,Type=Float,Description=\"Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed\"&gt; ##INFO=&lt;ID=MQ,Number=1,Type=Float,Description=\"RMS Mapping Quality\"&gt; ##INFO=&lt;ID=MQRankSum,Number=1,Type=Float,Description=\"Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities\"&gt; ##INFO=&lt;ID=QD,Number=1,Type=Float,Description=\"Variant Confidence/Quality by Depth\"&gt; ##INFO=&lt;ID=RAW_MQ,Number=1,Type=Float,Description=\"Raw data for RMS Mapping Quality\"&gt; ##INFO=&lt;ID=ReadPosRankSum,Number=1,Type=Float,Description=\"Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias\"&gt; ##INFO=&lt;ID=SOR,Number=1,Type=Float,Description=\"Symmetric Odds Ratio of 2x2 contingency table to detect strand bias\"&gt; ##contig=&lt;ID=20,length=63025520&gt; ##reference=file:///data/ref/ref.fasta ##source=GenotypeGVCFs . This is a lot of lines, so let us break it down into digestible bits. Note that the header lines are always listed in alphabetical order. VCF spec version . The first line: . ##fileformat=VCFv4.2 . tells you the version of the VCF specification to which the file conforms. This may seem uninteresting but it can have some important consequences for how to handle and interpret the file contents. As genomics is a fast moving field, the file formats are evolving fairly rapidly, so some of the encoding conventions change. If you run into unexpected issues while trying to parse a VCF file, be sure to check the version and the spec for any relevant format changes. FILTER lines . The FILTER lines tell you what filters have been applied to the data. In our test file, one filter has been applied: . ##FILTER=&lt;ID=LowQual,Description=\"Low quality\"&gt; . Records that fail any of the filters listed here will contain the ID of the filter (here, LowQual) in its FILTER field (see how records are structured further below). FORMAT and INFO lines . These lines define the annotations contained in the FORMAT and INFO columns of the VCF file, which we explain further below. If you ever need to know what an annotation stands for, you can always check the VCF header for a brief explanation (at least if you are using a civilized program that writes definition lines to the header). GATKCommandLine . The GATKCommandLine lines contain all the parameters that went used by the tool that generated the file. Here, GATKCommandLine.HaplotypeCaller refers to a command line invoking HaplotypeCaller. These parameters include all the arguments that the tool accepts, along with the values that were applied (if you do not pass one, a default is applied); so it is not just the arguments specified explicitly by the user in the command line. Contig lines and Reference . These contain the contig names, lengths, and which reference assembly was used with the input BAM file. This can come in handy when someone gives you a callset but does not tell you which reference it was derived from -- remember that for many organisms, there are multiple reference assemblies, and you should always make sure to use the appropriate one! . For more information on genome references, see the corresponding Dictionary entry. ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"469": {
    "doc": "VCF - Variant Call Format",
    "title": "4. Structure of variant call records",
    "content": "For each site record, the information is structured into columns (also called fields) as follows: . #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878 [other samples...] . The first 8 columns of the VCF records (up to and including INFO) represent the properties observed at the level of the variant (or invariant) site. Keep in mind that when multiple samples are represented in a VCF file, some of the site-level annotations represent a summary or average of the values obtained for that site from the different samples. Sample-specific information such as genotype and individual sample-level annotation values are contained in the FORMAT column (9th column) and in the sample-name columns (10th and beyond). In the example above, there is one sample called NA12878; if there were additional samples there would be additional columns to the right. Most programs order the sample columns alphabetically by sample name, but this is not always the case, so be aware that you can not depend on ordering rules for parsing VCF output! . Site-level properties and annotations . These first 7 fields are required by the VCF format and must be present, although they can be empty (in practice, there has to be a dot, ie . to serve as a placeholder). CHROM and POS . The contig and genomic coordinates on which the variant occurs. Note that for deletions the position given is actually the base preceding the event. ID . An optional identifier for the variant. Based on the contig and position of the call and whether a record exists at this site in a reference database such as dbSNP. A typical identifier is the dbSNP ID, which in human data would look like rs28548431, for example. REF and ALT . The reference allele and alternative allele(s) observed in a sample, set of samples, or a population in general (depending how the VCF was generated). The REF and ALT alleles are the only required elements of a VCF record that tell us whether the variant is a SNP or an indel (or in complex cases, a mixed-type variant). If we look at the following two sites, we see the first is a SNP, the second is an insertion and the third is a deletion: . 20 10001298 . T A 884.77 . [CLIPPED] GT:AD:DP:GQ:PL 1/1:0,30:30:89:913,89,0 20 10001436 . A AAGGCT 1222.73 . [CLIPPED] GT:AD:DP:GQ:PL 1/1:0,28:28:84:1260,84,0 20 10004769 . TAAAACTATGC T 622.73 . [CLIPPED] GT:AD:DP:GQ:PL 0/1:18,17:35:99:660,0,704 . Note that REF and ALT are always given on the forward strand. For insertions, the ALT allele includes the inserted sequence as well as the base preceding the insertion so you know where the insertion is compared to the reference sequence. For deletions, the ALT allele is the base before the deletion. QUAL . The Phred-scaled probability that a REF/ALT polymorphism exists at this site given sequencing data. Because the Phred scale is -10 * log(1-p), a value of 10 indicates a 1 in 10 chance of error, while a 100 indicates a 1 in 10^10 chance (see the Technical Documentation). These values can grow very large when a large amount of data is used for variant calling, so QUAL is not often a very useful property for evaluating the quality of a variant call. See our documentation on filtering variants for more information on this topic. Not to be confused with the sample-level annotation GQ; see this FAQ article for an explanation of the differences in what they mean and how they should be used. FILTER . This field contains the name(s) of any filter(s) that the variant fails to pass, or the value PASS if the variant passed all filters. If the FILTER value is ., then no filtering has been applied to the records. It is extremely important to apply appropriate filters before using a variant callset in downstream analysis. See our documentation on filtering variants for more information on this topic. INFO . Various site-level annotations. This field is not required to be present in the VCF. The annotations contained in the INFO field are represented as tag-value pairs, where the tag and value are separated by an equal sign, ie =, and pairs are separated by colons, ie ; as in this example: MQ=99.00;MQ0=0;QD=17.94. They typically summarize context information from the samples, but can also include information from other sources (e.g. population frequencies from a database resource). Some are annotated by default by the GATK tools that produce the callset, and some can be added on request. They are always defined in the VCF header, so that is an easy way to check what an annotation means if you do not recognize it. You can also find additional information on how they are calculated and how they should be interpreted in the Tool Index. Sample-level annotations . At this point you have met all the fields up to INFO in this lineup: . #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878 [other samples...] . All the rest is going to be sample-level information. Sample-level annotations are tag-value pairs, like the INFO annotations, but the formatting is a bit different. The short names of the sample-level annotations are recorded in the FORMAT field. The annotation values are then recorded in corresponding order in each sample column (where the sample names are the SM tags identified in the read group data). Typically, you will at minimum have information about the genotype and confidence in the genotype for the sample at each site. See the next section on genotypes for more details. ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"470": {
    "doc": "VCF - Variant Call Format",
    "title": "5. Interpreting genotype and other sample-level information",
    "content": "The sample-level information contained in the VCF (also called \"genotype fields\") may look a bit complicated at first glance, but they are actually not that hard to interpret once you understand that they are just sets of tags and values. Let us take a look at three of the records shown earlier, simplified to just show the key genotype annotations: . 20 10001019 . T G 364.77 . [CLIPPED] GT:AD:DP:GQ:PL 0/1:18,15:33:99:393,0,480 20 10001298 . T A 884.77 . [CLIPPED] GT:AD:DP:GQ:PL 1/1:0,30:30:89:913,89,0 20 10001436 . A AAGGCT 1222.73 . [CLIPPED] GT:AD:DP:GQ:PL 1/1:0,28:28:84:1260,84,0 . Looking at that last column, here is what the tags mean: . GT . The genotype of this sample at this site. For a diploid organism, the GT field indicates the two alleles carried by the sample, encoded by a 0 for the REF allele, 1 for the first ALT allele, 2 for the second ALT allele, etc. When there is a single ALT allele (by far the more common case), GT will be either: . - 0/0 : the sample is homozygous reference - 0/1 : the sample is heterozygous, carrying 1 copy of each of the REF and ALT alleles - 1/1 : the sample is homozygous alternate . In the three sites shown in the example above, NA12878 is observed with the allele combinations T/G, A/A and AAGGCT/AAGGCT respectively. For non-diploids, the same pattern applies; in the haploid case there will be just a single value in GT (e.g. 1); for polyploids there will be more, e.g. 4 values for a tetraploid organism (e.g. 0/0/1/1). AD and DP . Allele depth (AD) and depth of coverage (DP). These are complementary fields that represent two important ways of thinking about the depth of the data for this sample at this site. AD is the unfiltered allele depth, i.e. the number of reads that support each of the reported alleles. All reads at the position (including reads that did not pass the variant caller’s filters) are included in this number, except reads that were considered uninformative. Reads are considered uninformative when they do not provide enough statistical evidence to support one allele over another. DP is the filtered depth, at the sample level. This gives you the number of filtered reads that support each of the reported alleles. You can check the variant caller’s documentation to see which filters are applied by default. Only reads that passed the variant caller’s filters are included in this number. However, unlike the AD calculation, uninformative reads are included in DP. See the Tool Documentation for more details on AD (DepthPerAlleleBySample) and DP (Coverage) for more details. PL . \"Normalized\" Phred-scaled likelihoods of the possible genotypes. For the typical case of a monomorphic site (where there is only one ALT allele) in a diploid organism, the PL field will contain three numbers, corresponding to the three possible genotypes (0/0, 0/1, and 1/1). The PL values are \"normalized\" so that the PL of the most likely genotype (assigned in the GT field) is 0 in the Phred scale. We use \"normalized\" in quotes because these are not probabilities. We set the most likely genotype PL to 0 for easy reading purpose. The other values are scaled relative to this most likely genotype. Keep in mind, if you are not familiar with the statistical lingo, that when we say PL is the \"Phred-scaled likelihood of the genotype\", we mean it is \"How much less likely that genotype is compared to the best one\". Have a look at this article for an example of how PL is calculated. GQ . The Genotype Quality represents the Phred-scaled confidence that the genotype assignment (GT) is correct, derived from the genotype PLs. Specifically, the GQ is the difference between the PL of the second most likely genotype, and the PL of the most likely genotype. As noted above, the values of the PLs are normalized so that the most likely PL is always 0, so the GQ ends up being equal to the second smallest PL, unless that PL is greater than 99. In GATK, the value of GQ is capped at 99 because larger values are not more informative, but they take more space in the file. So if the second most likely PL is greater than 99, we still assign a GQ of 99. Basically the GQ gives you the difference between the likelihoods of the two most likely genotypes. If it is low, you can tell there is not much confidence in the genotype, i.e. there was not enough evidence to confidently choose one genotype over another. See the FAQ article on the Phred scale to get a sense of what would be considered low. Not to be confused with the site-level annotation QUAL; see this FAQ article for an explanation of the differences in what they mean and how they should be used. A few examples . With all the definitions out of the way, let us interpret the genotype information for a few records from our NA12878 callset, starting with at position 10001019 on chromosome 20: . 20 10001019 . T G 364.77 . [CLIPPED] GT:AD:DP:GQ:PL 0/1:18,15:33:99:393,0,480 . At this site, the called genotype is GT = 0/1, which corresponds to a heterozygous genotype with alleles T/G. The confidence indicated by GQ = 99 is very good; there were a total of 33 informative reads at this site (DP=33), 18 of which supported the REF allele (=had the reference base) and 15 of which supported the ALT allele (=had the alternate base) (indicated by AD=18,15). The degree of certainty in our genotype is evident in the PL field, where PL(0/1) = 0 (the normalized value that corresponds to a likelihood of 1.0) as is always the case for the assigned allele; the next PL is PL(0/0) = 393, corresponding to 10^(-39.3), or 5.0118723e-40 which is a very small number indeed; and the next one will be even smaller. The GQ ends up being 99 because of the capping as explained above. Now let us look at a site where our confidence is quite a bit lower: . 20 10024300 . C CTT 43.52 . [CLIPPED] GT:AD:DP:GQ:PL 0/1:1,4:6:20:73,0,20 . Here we have an indel -- specifically an insertion of TT after the reference C base at position 10024300. The called genotype is GT = 0/1 again, but this time the GQ = 20 indicates that even though this is probably a real variant (the QUAL is not too bad), we are not sure we have the right genotype. Looking at the coverage annotations, we see we only had 6 reads there, of which 1 supported REF and 4 supported ALT (and one read must have been considered uninformative, possibly due to quality issues). With so little coverage, we ca not be sure that the genotype should not in fact be homozygous variant. Finally, let us look at a more complicated example: . 20 10009875 . A G,AGGGAGG 1128.77 . [CLIPPED] GT:AD:DP:GQ:PL 1/2:0,11,5:16:99:1157,230,161,487,0,434 . This site is a doozy; two credible ALT alleles were observed, but the REF allele was not -- so technically this is a biallelic site in our sample, but will be considered multiallelic because there are more than two alleles notated in the record. It is also a mixed-type record, since one of the ALTs by itself would make it an A-&gt;G SNP, and the other would make it an insertion of GGGAGG after the reference A. The called genotype is GT = 1/2, which means it is a heterozygous genotype composed of two different ALT alleles. The coverage was not great, and was not all that balanced between the two ALTs (since one was supported by 11 reads and the other by 5) but it was sufficient for the program to have high confidence in its call. ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"471": {
    "doc": "VCF - Variant Call Format",
    "title": "6. Basic operations: validating, subsetting and exporting from a VCF",
    "content": "These are a few common things you may want to do with your VCFs that do not deserve their own tutorial. Let us know if there are other operations you think we should cover here. Validate your VCF . Validation, or checking that the format of the file is correct, follows the specification, and will therefore not break any well-behave tool you choose to run on it. You can do this very simply with ValidateVariants. Note that ValidateVariants can also be used on GVCFs if you use the --gvcf argument. Subset records from your VCF . Sometimes you want to subset just one or a few samples from a big cohort. Sometimes you want to subset to just a genomic region. Sometimes you want to do both at the same time! Well, the same tool can do both, and more; it is called SelectVariants and has a lot of options for doing this in that way (including operating over intervals in the usual way). There are many options for setting the selection criteria, depending on what you want to achieve. For example, given a single VCF file, one or more samples can be extracted from the file, based either on a complete sample name, or on a pattern match. Variants can also be selected based on annotated properties, such as depth of coverage or allele frequency. This is done using JEXL expressions. Other VCF files can also be used to modify the selection based on concordance or discordance between different callsets (see --discordance / --concordance arguments in the Tool Doc. Important notes about subsetting operations . | In the output VCF, some annotations such as AN (number of alleles), AC (allele count), AF (allele frequency), and DP (depth of coverage) are recalculated as appropriate to accurately reflect the composition of the subset callset. | By default, SelectVariants will keep all ALT alleles, even if they are no longer supported by any samples after subsetting. This is the correct behavior, as reducing samples down should not change the character of the site, only the AC in the subpopulation. In some cases this will produce monomorphic records, i.e. where no ALT alleles are supported. The tool accepts flags that exclude unsupported alleles and/or monomorphic records from the output. | . Extract information from a VCF in a sane, (mostly) straightforward way . Use VariantsToTable. No, really, do not write your own parser if you can avoid it. This is not a comment on how smart or how competent we think you are -- it is a comment on how annoyingly obtuse and convoluted the VCF format is. Seriously. The VCF format lends itself really poorly to parsing methods like regular expressions, and we hear sob stories all the time from perfectly competent people whose home-brewed parser broke because it could not handle a more esoteric feature of the format. We know we broke a bunch of people's scripts when we introduced a new representation for spanning deletions in multisample callsets. OK, we ended up replacing it with a better representation a month later that was a lot less disruptive and more in line with the spirit of the specification -- but the point is, that first version was technically legal according to the 4.2 spec, and that sort of thing can happen at any time. So yes, the VCF is a difficult format to work with, and one way to deal with that safely is to not home-brew parsers. (Why are we sticking with it anyway? Because, as Winston Churchill famously put it, VCF is the worst variant call representation, except for all the others.) . ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"472": {
    "doc": "VCF - Variant Call Format",
    "title": "7. Merging VCF files",
    "content": "There are three main reasons why you might want to combine variants from different files into one, and the tool you use depends on what you are trying to achieve. | The most common case is when you have been parallelizing your variant calling analyses, e.g. running HaplotypeCaller per-chromosome, producing separate VCF files (or GVCF files) per-chromosome. For that case, you can use the Picard tool MergeVcfs to merge the files. See the relevant Tool Doc page for usage details. | The second case is when you have been using HaplotypeCaller in -ERC GVCF or -ERC BP_RESOLUTION to call variants on a large cohort, producing many GVCF files. You then need to consolidate them before joint-calling variants with GenotypeGVCFs (for performance reasons). This can be done with either CombineGVCFs or ImportGenomicsDB tools, both of which are specifically designed to handle GVCFs in this way. See the relevant Tool Doc pages for usage details and the Best Practices workflow documentation to learn more about the logic of this workflow. | The third case is when you want to compare variant calls that were produced from the same samples but using different methods, for comparison. For example, if you are evaluating variant calls produced by different variant callers, different workflows, or the same but using different parameters. For this case, we recommend taking a different approach; rather than merging the VCF files (which can have all sorts of complicated consequences), you can us the VariantAnnotator tool to annotate one of the VCFs with the other treated as a resource. See the relevant Tool Doc page for usage details. | . There is actually one more reason why you might want to combine variants from different files into one, but we do not recommend doing it: you have produced variant calls from various samples separately, and want to combine them for analysis. This is how people used to do variant analysis on large numbers of samples, but we do not recommend proceeding this way because that workflow suffers from serious methodological flaws. Instead, you should follow our recommendations as laid out in the Best Practices documentation. ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"473": {
    "doc": "VCF - Variant Call Format",
    "title": "VCF - Variant Call Format",
    "content": "Last update: 20241214 . ",
    "url": "/pages/vcf.html",
    
    "relUrl": "/pages/vcf.html"
  },"474": {
    "doc": "VCF and gVCF",
    "title": "GVCF - Genomic Variant Call Format",
    "content": "This page has been directly cloned from GATK / Technical Documentation / Glossary https://gatk.broadinstitute.org/hc/en-us/articles/360035531812-GVCF-Genomic-Variant-Call-Format. Please check the original source if it is available for updates. The VCF specifications can be read directly from the original documentation. However, this page is very useful and is thus preserved here for reference. | GATK Team | June 25, 2024 08:17 Updated | . GVCF stands for Genomic VCF. A GVCF is a kind of VCF, so the basic format specification is the same as for a regular VCF (see the spec documentation here), but a Genomic VCF contains extra information. This document explains what that extra information is and how you can use it to empower your variant discovery analyses. Important notes . The term GVCF is sometimes used simply to describe VCFs that contain a record for every position in the genome (or interval of interest) regardless of whether a variant was detected at that site or not (such as VCFs produced by UnifiedGenotyper with --output_mode EMIT_ALL_SITES). GVCFs produced by HaplotypeCaller in GATK versions 3.x and 4.x contain additional information that is formatted in a very specific way. Read on to find out more. GVCF files produced by HaplotypeCaller from GATK versions 3.x and 4.x are not substantially different. While we don't recommend mixing versions, and we have not tested this ourselves, it should be okay to use GVCFs made by different versions if the annotations and the GVCFBlock definitions (see below) are the same. ",
    "url": "/pages/vcf_gvcf.html#gvcf---genomic-variant-call-format",
    
    "relUrl": "/pages/vcf_gvcf.html#gvcf---genomic-variant-call-format"
  },"475": {
    "doc": "VCF and gVCF",
    "title": "General comparison of VCF vs. GVCF",
    "content": "The key difference between a regular VCF and a GVCF is that the GVCF has records for all sites, whether there is a variant call there or not. The goal is to have every site represented in the file in order to do joint analysis of a cohort in subsequent steps. The records in a GVCF include an accurate estimation of how confident we are in the determination that the sites are homozygous-reference or not. This estimation is generated by the HaplotypeCaller's built-in reference model. Note that some other tools (including the GATK's own UnifiedGenotyper) may output an all-sites VCF that looks superficially like the BP_RESOLUTION GVCFs produced by HaplotypeCaller, but they do not provide an accurate estimate of reference confidence, and therefore cannot be used in joint genotyping analyses. The two types of GVCFs . As you can see in the figure above, there are two options you can use with emit-ref-confidence (ERC) -ERC: GVCF and BP_RESOLUTION. With BP_RESOLUTION, you get a GVCF with an individual record at every site: either a variant record, or a non-variant record. With GVCF, you get a GVCF with individual variant records for variant sites, but the non-variant sites are grouped together into non-variant block records that represent intervals of sites for which the genotype quality (GQ) is within a certain range or band. The GQ ranges are defined in the ##GVCFBlock line of the GVCF header. The purpose of the blocks (also called banding) is to keep file size down, so we recommend using the -GVCF option over BP_RESOLUTION. Mode for emitting reference confidence (ERC) scores mode makes it possible to emit a per-bp or summarized confidence estimate for a site being strictly homozygous-reference. See https://software.broadinstitute.org/gatk/documentation/article.php?id=4017 for information about GVCFs. GVCF - Reference model emitted with condensed non-variant blocks, i.e. the GVCF format. ",
    "url": "/pages/vcf_gvcf.html",
    
    "relUrl": "/pages/vcf_gvcf.html"
  },"476": {
    "doc": "VCF and gVCF",
    "title": "Example GVCF file",
    "content": "This is a banded GVCF produced by HaplotypeCaller with the -GVCF option. Header: . As you can see in the first line, the basic file format is a valid version 4.2 VCF: . ##fileformat=VCFv4.2 ##ALT=&lt;ID=NON_REF,Description=\"Represents any possible alternative allele at this location\"&gt; ##FILTER=&lt;ID=LowQual,Description=\"Low quality\"&gt; ##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Allelic depths for the ref and alt alleles in the order listed\"&gt; ##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"&gt; ##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"&gt; ##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt; . One FORMAT annotation is unique to the GVCF format: . ##FORMAT=&lt;ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum DP observed within the GVCF block\"&gt; . This defines what was the minimum amount of coverage observed at any one site within a block of records. The header goes on: . ##FORMAT=&lt;ID=PGT,Number=1,Type=String,Description=\"Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another\"&gt; ##FORMAT=&lt;ID=PID,Number=1,Type=String,Description=\"Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group\"&gt; ##FORMAT=&lt;ID=PL,Number=G,Type=Integer,Description=\"Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification\"&gt; ##FORMAT=&lt;ID=SB,Number=4,Type=Integer,Description=\"Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.\"&gt; ##GATKCommandLine=&lt;ID=HaplotypeCaller,CommandLine=\"[full command line goes here]\",Version=4.beta.6-117-g4588584-SNAPSHOT,Date=\"December 23, 2017 4:04:34 PM EST\"&gt; . At this point in the header we see the GVCFBlock definitions, which indicate the GQ ranges used for banding: . [individual blocks from 1 to 55] ##GVCFBlock55-56=minGQ=55(inclusive),maxGQ=56(exclusive) ##GVCFBlock56-57=minGQ=56(inclusive),maxGQ=57(exclusive) ##GVCFBlock57-58=minGQ=57(inclusive),maxGQ=58(exclusive) ##GVCFBlock58-59=minGQ=58(inclusive),maxGQ=59(exclusive) ##GVCFBlock59-60=minGQ=59(inclusive),maxGQ=60(exclusive) ##GVCFBlock60-70=minGQ=60(inclusive),maxGQ=70(exclusive) ##GVCFBlock70-80=minGQ=70(inclusive),maxGQ=80(exclusive) ##GVCFBlock80-90=minGQ=80(inclusive),maxGQ=90(exclusive) ##GVCFBlock90-99=minGQ=90(inclusive),maxGQ=99(exclusive) ##GVCFBlock99-100=minGQ=99(inclusive),maxGQ=100(exclusive) . In recent versions of GATK, the banding strategy has been tuned to provide high resolution at lower values of GQ (59 and below) and more compression at high values (60 and above). Note that since GQ is capped at 99, records where the corresponding PL is greater than 99 are lumped into the 99-100 band. After that, the header goes on: . ##INFO=&lt;ID=BaseQRankSum,Number=1,Type=Float,Description=\"Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities\"&gt; ##INFO=&lt;ID=ClippingRankSum,Number=1,Type=Float,Description=\"Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases\"&gt; ##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth; some reads may have been filtered\"&gt; ##INFO=&lt;ID=DS,Number=0,Type=Flag,Description=\"Were any of the samples downsampled?\"&gt; ##INFO=&lt;ID=END,Number=1,Type=Integer,Description=\"Stop position of the interval\"&gt; ##INFO=&lt;ID=ExcessHet,Number=1,Type=Float,Description=\"Phred-scaled p-value for exact test of excess heterozygosity\"&gt; ##INFO=&lt;ID=InbreedingCoeff,Number=1,Type=Float,Description=\"Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation\"&gt; ##INFO=&lt;ID=MLEAC,Number=A,Type=Integer,Description=\"Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed\"&gt; ##INFO=&lt;ID=MLEAF,Number=A,Type=Float,Description=\"Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed\"&gt; ##INFO=&lt;ID=MQ,Number=1,Type=Float,Description=\"RMS Mapping Quality\"&gt; ##INFO=&lt;ID=MQRankSum,Number=1,Type=Float,Description=\"Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities\"&gt; ##INFO=&lt;ID=RAW_MQ,Number=1,Type=Float,Description=\"Raw data for RMS Mapping Quality\"&gt; ##INFO=&lt;ID=ReadPosRankSum,Number=1,Type=Float,Description=\"Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias\"&gt; ##contig=&lt;ID=20,length=63025520,assembly=GRCh37&gt; ##source=HaplotypeCaller . Records . The first thing you'll notice, hopefully, is the &lt;NON_REF&gt; symbolic allele listed in every record's ALT field. This provides us with a way to represent the possibility of having a non-reference allele at this site, and to indicate our confidence either way. The second thing to look for is the END tag in the INFO field of non-variant block records. This tells you at what position the block ends. For example, the first line is a non-variant block that starts at position 20:10001567 and ends at 20:10001616. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878 20 10001567 . A &lt;NON_REF&gt; . END=10001616 GT:DP:GQ:MIN_DP:PL 0/0:38:99:34:0,101,1114 20 10001617 . C A,&lt;NON_REF&gt; 493.77 . BaseQRankSum=1.632;ClippingRankSum=0.000;DP=38;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQ=136800.00;ReadPosRankSum=0.170 GT:AD:DP:GQ:PL:SB 0/1:19,19,0:38:99:522,0,480,578,538,1116:11,8,13,6 20 10001618 . T &lt;NON_REF&gt; . END=10001627 GT:DP:GQ:MIN_DP:PL 0/0:39:99:37:0,105,1575 20 10001628 . G A,&lt;NON_REF&gt; 1223.77 . DP=37;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQ=133200.00 GT:AD:DP:GQ:PL:SB 1/1:0,37,0:37:99:1252,111,0,1252,111,1252:0,0,21,16 20 10001629 . G &lt;NON_REF&gt; . END=10001660 GT:DP:GQ:MIN_DP:PL 0/0:43:99:38:0,102,1219 . 20 10001661 . T C,&lt;NON_REF&gt; 1779.77 . DP=42;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQ=151200.00 GT:AD:DP:GQ:PGT:PID:PL:SB 1/1:0,42,0:42:99:0|1:10001661_T_C:1808,129,0,1808,129,1808:0,0,26,16 . 20 10001662 . T &lt;NON_REF&gt; . END=10001669 GT:DP:GQ:MIN_DP:PL 0/0:44:99:43:0,117,1755 . 20 10001670 . T G,&lt;NON_REF&gt; 1773.77 . DP=42;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQ=151200.00 GT:AD:DP:GQ:PGT:PID:PL:SB 1/1:0,42,0:42:99:0|1:10001661_T_C:1802,129,0,1802,129,1802:0,0,25,17 . 20 10001671 . G &lt;NON_REF&gt; . END=10001673 GT:DP:GQ:MIN_DP:PL 0/0:43:99:42:0,120,1800 20 10001674 . A &lt;NON_REF&gt; . END=10001674 GT:DP:GQ:MIN_DP:PL 0/0:42:96:42:0,96,1197 20 10001675 . A &lt;NON_REF&gt; . END=10001695 GT:DP:GQ:MIN_DP:PL 0/0:41:99:39:0,105,1575 . Note that toward the end of this snippet, you see multiple consecutive non-variant block records. These were not merged into a single record because the sites they contain belong to different ranges of GQ (which are defined in the header). ",
    "url": "/pages/vcf_gvcf.html",
    
    "relUrl": "/pages/vcf_gvcf.html"
  },"477": {
    "doc": "VCF and gVCF",
    "title": "Compressing file sizes with reblocking",
    "content": "Reblocking refers to the process of compressing GVCFs in order to reduce file storage footprints. This can help with joint genotyping pipelines by removing alt alleles that do not appear in the called genotype. This can be implemented using the ReblockGVCF (BETA) in GATK, or the WARP VariantCalling WDL script which is imported by both the Exome and Whole Genome workflows in WARP. &lt;/div&gt; . ",
    "url": "/pages/vcf_gvcf.html",
    
    "relUrl": "/pages/vcf_gvcf.html"
  },"478": {
    "doc": "VCF and gVCF",
    "title": "VCF and gVCF",
    "content": "Last update: 20241214 . ",
    "url": "/pages/vcf_gvcf.html",
    
    "relUrl": "/pages/vcf_gvcf.html"
  },"479": {
    "doc": "Virtual gene panels",
    "title": "Inborn errors of metabolism",
    "content": "Invitae panels . | Usage: Not in use. | While these a good gene lists, they do not include inheritance pattern information (i.e. autosomal dominant/recessive) which is required for more accurate ACMGuru classifications: . | Methylmalonic Acidemia and Homocystinuria (GTR000553763.2) . | https://www.ncbi.nlm.nih.gov/gtr/tests/553763/ | . | Cobalamin/Propionate/Homocysteine Metabolism Related Disorders (GTR000519388.10) | . | . Genomics England panels . | Usage: currently imported as metabol in ACMGuru. | Likely inborn error of metabolism - targeted testing not possible (Version 5.24) . | https://panelapp.genomicsengland.co.uk/panels/467/ | This gene panel from Genomics England contains inheritance pattern information (i.e. autosomal dominant/recessive). | Relevant disorders: Likely inborn error of metabolism - targeted testing not possible, Likely inborn error of metabolism, Inborn errors of metabolism, R98 | Panel types: GMS Rare Disease Virtual, Component Of Super Panel, GMS signed-off | Latest signed off version: v5.0 (1 May 2024) | . | . ",
    "url": "/pages/virtual_panels.html#inborn-errors-of-metabolism",
    
    "relUrl": "/pages/virtual_panels.html#inborn-errors-of-metabolism"
  },"480": {
    "doc": "Virtual gene panels",
    "title": "Inborn errors of immunity",
    "content": ". | Usage: currently imported as iuis in ACMGuru. | International Union of Immunological Societies (IUIS) Inborn Errors of Immunity Committee (IEI) . | See https://lawlessgenomics.com/topic/iuis-iei-table-page | . | . ",
    "url": "/pages/virtual_panels.html#inborn-errors-of-immunity",
    
    "relUrl": "/pages/virtual_panels.html#inborn-errors-of-immunity"
  },"481": {
    "doc": "Virtual gene panels",
    "title": "Virtual gene panels",
    "content": "Last update: 20240724 . ",
    "url": "/pages/virtual_panels.html",
    
    "relUrl": "/pages/virtual_panels.html"
  },"482": {
    "doc": "VSAT with SKAT",
    "title": "Variant Set Association Testing (VSAT) with burden testing, SKAT, and SKAT-O",
    "content": " ",
    "url": "/pages/vsat.html#variant-set-association-testing-vsat-with-burden-testing-skat-and-skat-o",
    
    "relUrl": "/pages/vsat.html#variant-set-association-testing-vsat-with-burden-testing-skat-and-skat-o"
  },"483": {
    "doc": "VSAT with SKAT",
    "title": "Introduction",
    "content": "Variant Set Association Testing (VSAT) involves statistical methods like burden testing, SKAT (Sequence Kernel Association Test), and SKAT-O (Optimal Unified SKAT), which help identify associations between groups of genetic variants and phenotypic traits. These methods are crucial for understanding the genetic basis of diseases, particularly when analyzing rare variants. Methods for variant collapse 1, https://www.nature.com/articles/s41576-019-0177-4/. One of the most important starting positions in our analysis is determined on variant collapse. Each project and biological system has unique features that determine how variants should be grouped for joint analysis. Some more context is available in this blogpost https://lawlessgenomics.com/2021/05/28/pathway_analysis.html. ",
    "url": "/pages/vsat.html#introduction",
    
    "relUrl": "/pages/vsat.html#introduction"
  },"484": {
    "doc": "VSAT with SKAT",
    "title": "Historical context and evolution",
    "content": "VSAT has evolved from simpler burden tests to more sophisticated models that can handle complex variant interactions and incorporate various data types. Key publications have shaped the field: . Main papers in order . | Methods for detecting associations with rare variants for common diseases: application to analysis of sequence data2. | A groupwise association test for rare mutations using a weighted sum statistic3. | Introduced a weighted sum statistic to improve the power of groupwise association tests for rare mutations. | . | An evaluation of statistical approaches to rare variant analysis in genetic association studies4. | Pooled association tests for rare variants in exon-resequencing studies5. | Testing for an unusual distribution of rare variants6. | Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test7. | Proposed SKAT, which uses a kernel-based approach to account for both common and rare variants in association tests. | . | Optimal tests for rare variant effects in sequencing association studies8. | Developed SKAT-O, an optimal approach that blends SKAT and burden testing methods to maximize statistical power across different genetic architectures. | . | Optimal Unified Approach for Rare-Variant Association Testing with Application to Small-Sample Case-Control Whole-Exome Sequencing Studies9. | Sequence Kernel Association Tests for the Combined Effect of Rare and Common Variants10. | . Major classes of tests . | Burden/Collapsing tests | Supervised/Adaptive Burden/Collapsing tests | Variance component (similarity) based tests | Omnibus tests: hedge against difference scenarios | . These methods address the challenge of detecting the collective effect of multiple genetic variants, acknowledging that these effects might vary in direction and magnitude. ",
    "url": "/pages/vsat.html#historical-context-and-evolution",
    
    "relUrl": "/pages/vsat.html#historical-context-and-evolution"
  },"485": {
    "doc": "VSAT with SKAT",
    "title": "Burden testing",
    "content": "Burden tests aggregate the effects of multiple variants within a genomic region into a single score, which is then tested for association with the phenotype. These tests are powerful when most variants in the region contribute to the phenotype in a similar manner. ",
    "url": "/pages/vsat.html#burden-testing",
    
    "relUrl": "/pages/vsat.html#burden-testing"
  },"486": {
    "doc": "VSAT with SKAT",
    "title": "SKAT and SKAT-O",
    "content": "Sequence Kernel Association Test (SKAT) and SKAT-Optimal (SKAT-O) are advanced statistical methods used for identifying associations between sets of genetic variants and a phenotype. They are particularly useful for complex genetic traits influenced by multiple rare variants. Data Notation and Setup . | Subjects: Let \\(n\\) be the number of subjects. | Variants: Assume \\(p\\) variant sites are observed within a region. | Phenotype: Denote \\(y_i\\) as the phenotype for subject \\(i\\). | Genotypes: \\(G_i = (G_{i1}, G_{i2}, \\ldots, G_{ip})\\) represents the genotypes for the \\(p\\) variants, where \\(G_{ij}\\) can be 0, 1, or 2 (copies of the minor allele). | Covariates: \\(X_i\\) includes covariates such as age, gender, and principal components to adjust for population stratification. | . SKAT Model . The SKAT model assesses the impact of multiple genetic variants simultaneously by considering the variants’ collective effect on the phenotype. It is based on a regression model: . \\[y_i = \\beta_0 + X_i'\\beta_X + G_i'\\beta_G + \\epsilon_i,\\] where: . | \\(\\beta_0\\) is the intercept. | \\(\\beta_X\\) are the coefficients for the covariates. | \\(\\beta_G\\) are the effects of the genetic variants. | \\(\\epsilon_i\\) is the error term. | . Kernel function . SKAT uses a kernel function to measure the genetic similarity between subjects based on their genotypes. This function is crucial as it allows SKAT to model the correlation structure among genetic variants. The kernel \\(K\\) is defined as: . \\[K = G W G^T,\\] where \\(W\\) is a diagonal matrix of weights for the genetic variants. These weights can be based on variant properties such as minor allele frequency or predicted functional impact. Variance component test . SKAT performs a variance component test using the kernel matrix to test the null hypothesis \\(H_0: \\beta_G = 0\\) against the alternative \\(H_a: \\beta_G \\neq 0\\). Under the null hypothesis, there is no effect of the genetic variants on the phenotype. Implementation steps . | Compute the Kernel Matrix: Calculate \\(K\\) using the genotypes and variant weights. | Fit the Null Model: Estimate the coefficients for the covariates without including genetic effects. | Compute the SKAT Statistic: Use the kernel matrix to calculate the SKAT statistic, which follows a chi-squared distribution under the null hypothesis. | . SKAT-O . SKAT-Optimal (SKAT-O) combines the burden test and SKAT to maximize power. It computes a weighted sum of the burden test statistic and the SKAT statistic: . \\[Q_{SKAT-O} = \\omega Q_{Burden} + (1 - \\omega) Q_{SKAT},\\] where \\(\\omega\\) is a weight factor that is optimally chosen based on the data to maximize the test’s power. ",
    "url": "/pages/vsat.html#skat-and-skat-o",
    
    "relUrl": "/pages/vsat.html#skat-and-skat-o"
  },"487": {
    "doc": "VSAT with SKAT",
    "title": "Practical implementation",
    "content": ". | Data Preparation: Requires a genotype matrix, phenotypic data, and potentially other covariates. Typical plink format is best. | Statistical Testing: Uses regression-based methods to associate genetic variability with phenotypes while adjusting for covariates. | Result Interpretation: Involves understanding p-values in the context of multiple testing, requiring adjustments such as Bonferroni (usually this) or false discovery rate (FDR) corrections. | . ",
    "url": "/pages/vsat.html#practical-implementation",
    
    "relUrl": "/pages/vsat.html#practical-implementation"
  },"488": {
    "doc": "VSAT with SKAT",
    "title": "Challenges and considerations",
    "content": ". | Rare Variants: Both SKAT and burden tests are particularly useful for analyzing rare variants, which may be overlooked by traditional GWAS. | Direction of Effects: Unlike burden tests, SKAT and SKAT-O can handle variants that have both protective and deleterious effects. | Computational Complexity: The choice of kernel functions in SKAT and the integration of multiple tests in SKAT-O can increase computational demands. | . ",
    "url": "/pages/vsat.html#challenges-and-considerations",
    
    "relUrl": "/pages/vsat.html#challenges-and-considerations"
  },"489": {
    "doc": "VSAT with SKAT",
    "title": "Conclusion",
    "content": "The development of VSAT methods like SKAT and SKAT-O represents a significant advancement in the field of genetic epidemiology, offering tools that can detect subtle and complex genetic influences on phenotypes. These tools are crucial for uncovering the genetic architecture of complex traits, especially in the context of rare genetic variants. ",
    "url": "/pages/vsat.html#conclusion",
    
    "relUrl": "/pages/vsat.html#conclusion"
  },"490": {
    "doc": "VSAT with SKAT",
    "title": "References",
    "content": ". | Povysil, G. et al., 2019. Rare-variant collapsing analyses for complex traits: guidelines and applications. Nature Reviews Genetics, 20(12), pp.747-759. DOI: 10.1038/s41576-019-0177-4. &#8617; . | Li, B. and Leal, S.M., 2008. Methods for detecting associations with rare variants for common diseases: application to analysis of sequence data. The American Journal of Human Genetics, 83(3), pp.311-321. &#8617; . | Madsen, B.E. and Browning, S.R., 2009. A groupwise association test for rare mutations using a weighted sum statistic. PLoS Genet, 5(2), e1000384. &#8617; . | Morris, A.P. and Zeggini, E., 2010. An evaluation of statistical approaches to rare variant analysis in genetic association studies. Genetic epidemiology, 34(2), pp.188-193. &#8617; . | Price, A.L. et al., 2010. Pooled association tests for rare variants in exon-resequencing studies. The American Journal of Human Genetics, 86(6), pp.832-838. &#8617; . | Neale, B.M. et al., 2011. Testing for an unusual distribution of rare variants. PLoS genetics, 7(3), e1001322. &#8617; . | Wu, M.C. et al., 2011. Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test. The American Journal of Human Genetics, 89(1), pp.82-93. &#8617; . | Lee, S., Wu, M.C., and Lin, X., 2012. Optimal tests for rare variant effects in sequencing association studies. Biostatistics, 13(4), pp.762-775. &#8617; . | Lee, S. et al., 2012. Optimal Unified Approach for Rare-Variant Association Testing with Application to Small-Sample Case-Control Whole-Exome Sequencing Studies. The American Journal of Human Genetics, 91(2), pp.224-237. &#8617; . | Ionita-Laza, I. et al., 2013. Sequence Kernel Association Tests for the Combined Effect of Rare and Common Variants. The American Journal of Human Genetics, 92(6), pp.841-853. &#8617; . | . ",
    "url": "/pages/vsat.html#references",
    
    "relUrl": "/pages/vsat.html#references"
  },"491": {
    "doc": "VSAT with SKAT",
    "title": "VSAT with SKAT",
    "content": "Last update: 20230425 . ",
    "url": "/pages/vsat.html",
    
    "relUrl": "/pages/vsat.html"
  },"492": {
    "doc": "SetID for VSAT",
    "title": "SetID for VSAT",
    "content": "When preparing a dataset for a Variant Set Association Test (VSAT) using SKAT (Sequence Kernel Association Test), it’s crucial to create a setID which lists the variants in a set (e.g. gene). This allows for collapsing SNPs into genes for gene-level analysis. The same is true for pathway sets, or any other virtual panel. The R package for SKAT, SKAT-O, etc. is https://github.com/leelabsg/SKAT. ",
    "url": "/pages/vsat_setID.html#setid-for-vsat",
    
    "relUrl": "/pages/vsat_setID.html#setid-for-vsat"
  },"493": {
    "doc": "SetID for VSAT",
    "title": "1. Prepare plink format data",
    "content": "Convert from VCF to plink https://www.cog-genomics.org/plink2/. Here is an example plink.bim file: .bim (PLINK extended MAP file) https://www.cog-genomics.org/plink2/formats#bim. | Chr | SNPID | Misc | Position | Allele 1 | Allele 2 | . | 1 | . | 0 | 10153 | G | A | . | 1 | . | 0 | 10159 | G | A | . | 1 | . | 0 | 10403 | * | ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC | . | 1 | . | 0 | 10403 | A | ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC | . | 1 | . | 0 | 10492 | T | C | . | 1 | . | 0 | 16068 | C | T | . | 1 | . | 0 | 16103 | G | T | . | 1 | . | 0 | 17385 | A | G | . | 1 | . | 0 | 17406 | T | C | . | 1 | . | 0 | 17407 | A | G | . Extended variant information file accompanying a .bed binary genotype table. (–make-just-bim can be used to update just this file.) A text file with no header line, and one line per variant with the following six fields: . | Chromosome code (either an integer, or ‘X’/’Y’/’XY’/’MT’; ‘0’ indicates unknown) or name | Variant identifier | Position in morgans or centimorgans (safe to use dummy value of ‘0’) | Base-pair coordinate (1-based; limited to 231-2) | Allele 1 (corresponding to clear bits in .bed; usually minor) | Allele 2 (corresponding to set bits in .bed; usually major) | Allele codes can contain more than one character. Variants with negative bp coordinates are ignored by PLINK. | . Set SNP variant ID . We can set the missing variant IDs in a bim file with plink --set-missing-var-ids @_#\\$r_\\$a to give: 1_10153_G_A in the .bim SNPID column. ",
    "url": "/pages/vsat_setID.html#1-prepare-plink-format-data",
    
    "relUrl": "/pages/vsat_setID.html#1-prepare-plink-format-data"
  },"494": {
    "doc": "SetID for VSAT",
    "title": "2. Downloading Ensembl mart data",
    "content": "The first step involves acquiring gene position data. This will let one map SNPs to specific genes. This data is downloaded from the Ensembl BioMart: http://mart.ensembl.org/biomart/martview. | Data Acquired: Gene stable ID, start and end positions, chromosome/scaffold name, and additional annotations like gene names and MIM morbid descriptions. | Source: Ensembl BioMart, providing comprehensive and up-to-date gene information corresponding to the GRCh38.p14 assembly of the human genome. | Example output of mart.tsv: | . | Gene stable ID | Gene start (bp) | Gene end (bp) | Chromosome/scaffold name | Gene name | MIM morbid description | . | ENSG00000210049 | 577 | 647 | MT | MT-TF |   | . | ENSG00000211459 | 648 | 1601 | MT | MT-RNR1 |   | . | ENSG00000210077 | 1602 | 1670 | MT | MT-TV |   | . | ENSG00000210082 | 1671 | 3229 | MT | MT-RNR2 |   | . | ENSG00000209082 | 3230 | 3304 | MT | MT-TL1 |   | . | ENSG00000198888 | 3307 | 4262 | MT | MT-ND1 | LEBER OPTIC ATROPHY;;LEBER HEREDITARY OPTIC NEUROPATHY; LHON | . | ENSG00000198888 | 3307 | 4262 | MT | MT-ND1 | MITOCHONDRIAL MYOPATHY, ENCEPHALOPATHY, LACTIC ACIDOSIS, AND STROKE-LIKE EPISODES; MELAS;;MELAS SYNDROME | . | ENSG00000210100 | 4263 | 4331 | MT | MT-TI |   | . | ENSG00000210107 | 4329 | 4400 | MT | MT-TQ |   | . | ENSG00000210112 | 4402 | 4469 | MT | MT-TM |   | . You can also get this via R biomaRt: . # Install and load the biomaRt package if (!requireNamespace(\"biomaRt\", quietly = TRUE)) { install.packages(\"BiocManager\") BiocManager::install(\"biomaRt\") } library(biomaRt) . ",
    "url": "/pages/vsat_setID.html#2-downloading-ensembl-mart-data",
    
    "relUrl": "/pages/vsat_setID.html#2-downloading-ensembl-mart-data"
  },"495": {
    "doc": "SetID for VSAT",
    "title": "2. Preparing the File.SetID",
    "content": "The file format for SKAT SetID is described in the package documentation https://cran.r-project.org/web//packages/SKAT/SKAT.pdf. Using the downloaded Ensembl Mart data, the File.SetID is prepared. This file is critical as it establishes a relationship between SNPs in the dataset and their respective genes: . | Integration of SNP Data: The BIM file from a PLINK dataset is utilized, where SNPs are annotated with chromosome and position data. | Mapping SNPs to Genes: The BIM data (SNP data) is merged with the Ensembl Mart data. This involves filtering and aligning SNPs to their corresponding gene based on the chromosome and positional data. | Output: A SetID file that lists each SNP under its respective gene, facilitating the collapsing of SNPs for gene-level analysis in SKAT. | . The SetID file is a white-space (space or tab) delimitered file with 2 columns: SetID and SNP_ID. Please keep in mind that there should be no header! The SNP_IDs and SetIDs should be less than 50 characters, otherwise, it will return an error message. | Example File.SetID | . | SNPID | SetID | . | 1_10153_G_A | Gene1 | . | 1_10159_G_A | Gene1 | . | 1_16068_C_T | Gene2 | . | 1_16103_G_T | Gene2 | . ",
    "url": "/pages/vsat_setID.html#2-preparing-the-filesetid",
    
    "relUrl": "/pages/vsat_setID.html#2-preparing-the-filesetid"
  },"496": {
    "doc": "SetID for VSAT",
    "title": "3. Running SKAT for a VSAT Per Gene",
    "content": "After preparing the SetID file, SKAT is executed to assess the association of gene-level variants (collapsed SNPs) with the phenotype of interest. Here’s a simplified explanation of how SKAT is run for VSAT per gene: . | Input Files: Besides the SetID file, necessary inputs include BED, BIM, and FAM files that provide the genotype and family data. | SetID File: It defines SNP sets where each set corresponds to a gene, and SNPs within each set are those mapped to the gene. | Kernel Choice and Adjustment: SKAT uses kernel-based methods to assess the aggregate effect of multiple SNPs. Choices of kernel can affect the sensitivity and specificity of the results, especially in the context of different genetic architectures. | Statistical Framework: SKAT employs a variance-component score test framework. This framework accommodates various types of complex genetic models, including those with multiple causal variants of varying effect sizes. | . ",
    "url": "/pages/vsat_setID.html#3-running-skat-for-a-vsat-per-gene",
    
    "relUrl": "/pages/vsat_setID.html#3-running-skat-for-a-vsat-per-gene"
  },"497": {
    "doc": "SetID for VSAT",
    "title": "Technical Details",
    "content": "SKAT provides various options to fine-tune the analysis: . | Kernels: Options include linear, linear.weighted, and others, each suitable for different scenarios and hypotheses about genetic effects. | Weighting Schemes: Influence the emphasis on rare vs. common variants, often based on minor allele frequencies. | Imputation of Missing Data: Methods like fixed, bestguess, or random can handle missing genotypes, crucial for maintaining robustness in genetic data analysis. | Phenotypic Adjustments: Continuous or dichotomous phenotypes are supported, with options to adjust for covariates and complex sample structures. | . By preparing the dataset and understanding the intricacies of SKAT, you can effectively conduct VSAT to screen for an enriched set (.e.g. gene or pathway) by running one SKAT-O test per setID. ",
    "url": "/pages/vsat_setID.html#technical-details",
    
    "relUrl": "/pages/vsat_setID.html#technical-details"
  },"498": {
    "doc": "SetID for VSAT",
    "title": "Interpretation of VSAT with Archipelago plot",
    "content": "You might use our Archipelago package to illustrate the results of complex VSAT studies https://github.com/DylanLawless/archipelago. This can be used to combine variant-level GWAS with gene-level, pathway-level, or other variant set tests. “Summary and illustration of variant set association test statistics. Variant set association tests (VSAT), particularly those incorporating minor allele frequency variants, have become invaluable in genetic association studies by allowing robust statistical analysis with variant collapse. Unlike single variant tests, VSAT statistics cannot be assigned to a genomic coordinate for visual interpretation by default. To address these challenges, we introduce the Archipelago plot, a graphical method for interpreting both VSAT p-values and individual variant contributions. The Archipelago method assigns a meaningful genomic coordinate to the VSAT p-value, enabling its simultaneous visualization alongside individual variant p-values. This results in an intuitive and rich illustration akin to an archipelago of clustered islands, enhancing the understanding of both collective and individual impacts of variants. The Archipelago plot is applicable in any genetic association study that uses variant collapse to evaluate both individual variants and variant sets, and its customizability facilitates clear communication of complex genetic data. By integrating two dimensions of genetic data into a single visualization, VSAT results can be easily read and aid in identification of potential causal variants in variant sets such as protein pathways.” . ",
    "url": "/pages/vsat_setID.html#interpretation-of-vsat-with-archipelago-plot",
    
    "relUrl": "/pages/vsat_setID.html#interpretation-of-vsat-with-archipelago-plot"
  },"499": {
    "doc": "SetID for VSAT",
    "title": "SetID for VSAT",
    "content": "Last update: 20230823 . | SetID for VSAT . | 1. Prepare plink format data . | Set SNP variant ID | . | 2. Downloading Ensembl mart data | 2. Preparing the File.SetID | 3. Running SKAT for a VSAT Per Gene | Technical Details | Interpretation of VSAT with Archipelago plot | . | . ",
    "url": "/pages/vsat_setID.html",
    
    "relUrl": "/pages/vsat_setID.html"
  }
}
