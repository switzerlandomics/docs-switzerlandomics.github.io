Directory Structure:
.
├── aggregate_multiplex.md
├── annotation_table.md
├── bookmarks.md
├── bwa.md
├── concepts.md
├── data_stream.md
├── design_doc
│   ├── design_doc.md
│   ├── dna_germline_short.md
│   ├── gatk_1.md
│   ├── gatk_bsqr.md
│   ├── gatk_dbimport.md
│   ├── gatk_duplicates.md
│   ├── gatk_genotypegvcf.md
│   ├── gatk_genotyperefine.md
│   ├── gatk_hc.md
│   ├── gatk_vqsr.md
│   ├── images
│   │   ├── VEP_consequences.jpg
│   │   └── variant_annotation_graph.png
│   ├── pre_anno_maf.md
│   ├── pre_annoprocess.md
│   └── rna_seq.md
├── directory_structure.txt
├── documentation.sh
├── fastp.md
├── fastq.md
├── git.md
├── hpc.md
├── layout.md
├── present
│   ├── 20230516.pdf
│   ├── 20230703.pdf
│   ├── presentations.md
│   └── template
│       ├── 20230703.tex
│       ├── images
│       │   ├── SwissPedHealth_Pipeline_Devs.png
│       │   ├── SwissPedHealth_Pipeline_Devs_wide_1.png
│       │   ├── SwissPedHealth_Pipeline_Devs_wide_2.png
│       │   └── swisspedhealth_pipedev_omic_concepts.png
│       └── references.bib
├── progress.md
├── progress.pdf
├── progress.sh
├── read_group.md
├── ref.md
├── template.tex
└── variables.md

5 directories, 44 files

-e 

File: ./present/presentations.md

---
layout: default
title: Presentations
nav_order: 5
---

# Presentations
{: .no_toc }
<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
---


Here are the presentations that have been used previously or prepared pre-emptively for use on short notice.

## 2023

* [Plan overview 20230516.pdf](20230516.pdf)

-e 

File: ./ref.md

---
layout: default
title: Reference genome
date: 2023-07-27 00:00:01
nav_order: 5
---

Last update: 20230727


## Share

Reference genome datasets are prepared and stored at:
* `/poject/data/ref`
    * Read-only
    * Datamanager control
    * Includes: README.md
    * Includes: ref.sh creation

## GRCh38
### Choice

Reference genome choice is discussed succinctly in many difference places.
Therefore, we link other usefull sources.

* Heng Li - Which human reference genome to use?
<https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use>

* Illumina review
<https://www.illumina.com/science/genomics-research/articles/dragen-demystifying-reference-genomes.html>

Our reference genome was donwloaded and installed by `ref.sh` which does the following:

### Installation
* Get local copy
```
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`
```

* Get checksum
```
md5 GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz.md5
```

* Transfer to cluster
```
sftp username@cluster
cd data/ref
put GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz
put GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz.md5
put ref.sh
```

* Preparation
* Once downloaded we need the index which is done by
```
bwa index GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz
```

## Other builds
We use GRCh38 but for some old prepared data we must use the existing version with the reference genome used at that time.
The mentioned refernce is "human_g1k_v37_decoy_chr.fasta".
There are 4 common "hg19" references, and they are NOT directly interchangeable:
<https://gatk.broadinstitute.org/hc/en-us/articles/360035890711-GRCh37-hg19-b37-humanG1Kv37-Human-Reference-Discrepancies>

-e 

File: ./data_stream.md

---
layout: default
title: Data stream
date: 2023-06-16 00:00:01
nav_order: 5
---

Last update: 20230616

## Omic data sources
* The Swiss Multi-Omics Center (SMOC)
	* <http://smoc.ethz.ch>.

## SMOC has 3 branches
* CGAC - Genomics.
* CPAC - Proteotyping.
* CMAC - Metabolomics & Lipidomics.

## Where the facilities are
* CGAC is from [Health2030 genome center](https://www.health2030genome.ch) Geneva.
* CMAC is from [ETHZ Metabolomics & Biophysics](https://fgcz.ch/omics_areas/met.html) Zurich.
* CPAC is from [ETHZProteomics](https://fgcz.ch/omics_areas/prot.html) Zurich.

## Naming convention
* BiomedIT portal data transfer uses the names:
	* CGAC = health_2030_genome_center
	* CMAC = phrt_cmac
	* CPAC = phrt_cpac

## Examples of omic tech and output

{: .note }
The following are place-holder links until we write up pages specfic to our usage.

* <https://lawlessgenomics.com/topic/omics-mc-hilic>
* <https://lawlessgenomics.com/topic/omics-mc-lc>
* <https://lawlessgenomics.com/topic/omics-proteomics>
* <https://lawlessgenomics.com/topic/omics-rplc>
* <https://lawlessgenomics.com/topic/omics-targeted>

## BioMedIT
Read about the high-performance-computing (HPC) infrastructure here: 
[HPC infrastructure](hpc.html)

## How data is collected and stored
Omic data that we will ultimately use is collected and hosted under the following hierarchy.

* The [Swiss Personalised Health Network (SPHN)](https://sphn.ch).
* Branch of SPHN: [Data Coordination Center (DCC)](https://sphn.ch/network/data-coordination-center/).
* DCC is also a part of [Swiss Institute of Bioinformatics (SIB)](https://www.sib.swiss/personalized-health-informatics).
* The data is to be stored and processed on [BioMedIT network](https://www.biomedit.ch).
* Our tenant on BioMedIT is called MOMIC and this is on the [sciCORE infrastructure](https://scicore.ch).

{: .note}
While the SPHN organisation hierarchy is illustrated based on the final storage location for the omic data belonging to this project, there are many other branches of SPHN not shown. It is also worth noting that ethical and legal responsibilities of data access may rest on the ethics/consent form signatories, such as individual project leaders.

### SPHN and BioMedIT
> The Swiss Personalized Health Network (SPHN) is a national initiative under the leadership of the Swiss Academy of Medical Sciences (SAMS). In collaboration with the SIB Swiss Institute of Bioinformatics it contributes to the development, implementation and validation of coordinated data infrastructures in order to make health-relevant data interoperable and shareable for research in Switzerland.
> BioMedIT is an integral part of SPHN and was developed along the needs of SPHN funded projects.
> SPHN and BioMedIT are funded by the Swiss Government through the ERI-dispatches (2017-2020 and 2021-2024).
(from <https://www.biomedit.ch>)

### BioMedIT structure
> The three BioMedIT sites (nodes) form a shared security zone, such that once data is brought into the platform, it can be accessed and processed on any of the nodes. Projects from research consortia or single researchers wishing to use sensitive data are set up as research projects on one of the nodes in a BioMedIT project-specific environment, called a 'B-space'.
> Research groups can access their B-spaces via the BioMedIT Portal, where they can manage users and encryption keys, initiate data transfers and access support and other tools and services offered by BioMedIT.
> Researchers securely access the system using two factor authentication, and extract only nonsensitive or aggregated research results while leaving the sensitive data in the platform. 
(from <https://www.biomedit.ch>)

## Accessing BioMedIT
> BioMedIT is open to all Swiss universities, research institutes, hospitals, service providers and other interested partners. In addition, and specifically within the framework of the two Swiss national initiatives SPHN and PHRT, the BioMedIT Network can be used for mono- and multi-site, individual and collaborative research projects. BioMedIT is a project of the SIB Swiss Institute of Bioinformatics
(from <https://www.biomedit.ch>)

![Alt text](https://www.biomedit.ch/.imaging/mte/biomedit-theme/large/dam/Illustrations/biomedit-network/BioMedIT-Graphic---SIB-3-line_with-title_Large.png/jcr:content/BioMedIT%20Graphic%20-%20SIB%203%20line_with%20title_Large.png "biomedit image")

-e 

File: ./design_doc/gatk_hc.md

---
layout: default
title: GATK Haplotype caller
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 3
---

## Haplotype Calling

### Overview
Haplotype calling is a critical step in our pipeline, involving the identification of variants from sequenced DNA by constructing haplotypes. This step uses the Genome Analysis Toolkit's (GATK) HaplotypeCaller, which allows for calling high-confidence variants.

### Implementation
The script `07_haplotype_caller.sh` is designed to manage this intensive computational task effectively, utilizing SLURM for job scheduling to handle potentially large genomic datasets.

#### Script Description
Configured to maximize efficiency given computational constraints:

- **Nodes**: 1
- **Memory**: 4G
- **CPUs per Task**: 2
- **Time Limit**: 48:00:00
- **Job Name**: hc
- **Job Array**: Capable of handling 1-68 samples concurrently.

The script begins by setting up the necessary computing environment, sourcing variables, and preparing input and output directories.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilized for the HaplotypeCaller tool, which is executed under specific java options to manage memory usage and parallel processing capabilities.

#### Process Flow
1. **File Preparation**:
   - Finds recalibrated BAM files from the previous BQSR step.
   - Each BAM file is assigned to a SLURM job based on its array task ID.

2. **Variant Calling Execution**:
   - HaplotypeCaller runs with parameters to produce a genomic VCF (gVCF) for each sample, which includes variant calls along with confidence scores.
   - Outputs are genomic VCF files named after each sample, stored in the designated output directory.

3. **Optimization and Debugging**:
   - Detailed logging of the script's execution, including start and end times, input and output details, and memory settings, helps in troubleshooting and ensuring reproducibility.

### Quality Assurance
This stage of the pipeline includes detailed logging and error checking to ensure that the haplotype calling process is robust against computational failures and produces reliable results.

### Conclusion
The Haplotype Calling step is pivotal for identifying variants accurately, setting the stage for subsequent processes such as variant annotation and interpretation. The use of high-performance computing resources ensures that our pipeline can handle large datasets efficiently and reliably.


-e 

File: ./design_doc/gatk_dbimport.md

---
layout: default
title: GATK Genomic db import
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 4
---

## Genomics Database Import

### Overview
The Genomics Database Import step is crucial for compiling variant data from multiple samples into a unified database, enhancing the efficiency of downstream variant analysis processes such as joint genotyping.

### Implementation
The script `08_genomics_db_import.sh` is designed to handle the consolidation of genomic variant call files (gVCFs) into a GenomicsDB workspace using GATK's `GenomicsDBImport` tool. This script is configured to process the data using SLURM job arrays, enabling parallel processing of genomic data.

#### Script Description
The script is optimized for high-throughput computational requirements:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 2
- **Time Limit**: 72:00:00
- **Job Name**: genomics_db_import
- **Job Array**: Supports handling 25 chromosome sets in a batch processing manner.

The script begins by establishing the environment, sourcing necessary variables, and setting up directories for input and output data.

#### Tools Used
- **GATK (v4.4.0.0)**: Used for its `GenomicsDBImport` tool, which allows for the efficient aggregation of gVCF files into a single database that can be queried and analyzed more efficiently.

#### Process Flow
1. **Input Preparation**:
   - Identifies all gVCF files from the Haplotype Calling step.
   - Constructs a command to include all these files in the GenomicsDB workspace.

2. **Database Creation**:
   - For each chromosome or chromosome set specified by the job array, a separate GenomicsDB workspace is created.
   - The process is customized to include optimizations for shared POSIX filesystems, which improves performance on distributed computing systems.

3. **Execution Details**:
   - The script uses dynamic allocation of memory and CPU resources to handle the intense demands of processing large genomic datasets.
   - Outputs include a GenomicsDB workspace for each chromosome, facilitating rapid access and manipulation in subsequent analytical steps.

### Quality Assurance
Robust logging, detailed output management, and stringent error handling are implemented to ensure the reliability and reproducibility of the Genomics Database Import process.

### Conclusion
The consolidation of gVCF files into a GenomicsDB workspace is a pivotal step in our pipeline. It not only optimizes the storage and querying of genomic data but also sets the stage for efficient joint genotyping and variant analysis across multiple samples. This process leverages advanced computational tools and techniques to handle the complexities of large-scale genomic datasets effectively.

---


-e 

File: ./design_doc/gatk_genotyperefine.md

---
layout: default
title: GATK Genotype refine
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 6
---

## Genotype Refinement

### Overview
Genotype refinement is a critical process in our pipeline aimed at enhancing the reliability of genotype calls by utilizing additional population data. This step applies statistical methods to refine genotype probabilities and filter variants based on genotype quality (GQ) scores.

### Implementation
The `11_genotype_refinement.sh` script uses tools from the Genome Analysis Toolkit (GATK) to refine genotypes based on a model that incorporates population-wide allele frequencies. The script is set up to process data efficiently across multiple chromosomes.

#### Script Description
Configured for substantial computational tasks:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: genotype_refine
- **Job Array**: Designed to handle multiple chromosomal segments in one pass.

The script initiates by setting up the required computational environment, loading modules, and preparing directories for input and output.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilizes `CalculateGenotypePosteriors` for refining genotype probabilities and `VariantFiltration` to apply quality filters to genotypes based on predefined thresholds.

#### Process Flow
1. **Input Preparation**:
   - The script retrieves recalibrated VCF files from the VQSR step and checks their existence before proceeding.

2. **Refinement Execution**:
   - **CalculateGenotypePosteriors**: This tool is used to adjust genotype likelihoods based on allele frequency data from large reference populations (e.g., gnomAD).
   - **VariantFiltration**: Applies a filter to flag genotypes with a GQ score less than 20, helping to ensure that only high-confidence genotypes are used in subsequent analyses.

3. **Outputs**:
   - Generates two sets of VCF files:
     - A refined VCF file with updated genotype likelihoods.
     - A filtered VCF file that excludes genotypes below a quality threshold, ensuring the dataset's integrity for further analysis.

### Quality Assurance
The process includes comprehensive logging and condition checks to ensure accuracy and efficiency. Error handling mechanisms are in place to address any issues during file handling or processing, enhancing the robustness of the pipeline.

### Conclusion
Genotype refinement is an essential step to ensure the high quality and reliability of variant calls in genomic studies. By integrating additional genomic data and applying rigorous quality control measures, this step helps in producing highly accurate genetic analyses.

---



-e 

File: ./design_doc/pre_annoprocess.md

---
layout: default
title: Pre-annotation processing
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 7
---

## Pre-Annotation Processing

### Overview
The Pre-Annotation Processing step refines VCF files prior to detailed annotation. This involves filtering, normalizing, and decomposing variants to ensure that the data fed into the annotation tools is of high quality and structured correctly.

### Implementation
The `12_pre_annotation_processing.sh` script employs a combination of bioinformatics tools including bcftools, GATK, and vt to refine VCF files. This script is designed to handle large datasets efficiently, processing data across multiple chromosomal segments.

#### Script Description
Optimized for high-throughput computing:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: pre_annotation
- **Job Array**: Capable of processing up to 25 chromosome segments in one pass.

The script starts by establishing the necessary computational environment and directories for input and output data.

#### Tools Used
- **bcftools**: Used for initial filtering based on quality metrics such as quality score, depth, and genotype quality.
- **GATK (v4.4.0.0)**: Utilized for selecting variants based on filtering criteria.
- **vt**: Applied for decomposing and normalizing variants to a canonical form, simplifying subsequent annotation processes.

#### Process Flow
1. **Initial Filtering**:
   - Filters variants using bcftools based on predefined quality criteria to ensure that only high-quality variants are processed further.

2. **Compression and Indexing**:
   - Compresses the filtered VCF files using bgzip and indexes them with tabix, preparing them for further processing.

3. **GATK Selection**:
   - Selects variants using GATK’s SelectVariants to exclude filtered and non-variant entries, refining the dataset.

4. **Normalization and Decomposition**:
   - Uses vt to decompose multiallelic sites into simpler allelic forms and normalizes them against the reference genome. This step ensures that variants are represented in their simplest form, aiding accurate annotation.

5. **Final Compression and Clean-up**:
   - Compresses and re-indexes the final VCF files for efficient storage and access.
   - Cleans up intermediate files to free storage space and maintain a tidy workspace.

### Quality Assurance
This step includes extensive error handling and logging to ensure that each sub-process is completed successfully. Detailed logs facilitate troubleshooting and ensure reproducibility.

### Conclusion
Pre-Annotation Processing is crucial for preparing VCF files for detailed annotation. By ensuring that the data is high-quality and properly formatted, this step lays the groundwork for accurate and efficient genomic annotation, which is critical for downstream genomic analyses.


-e 

File: ./design_doc/gatk_duplicates.md

---
layout: default
title: GATK Duplicates
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 1
---

## Duplicate Marking and Merging

### Overview
The DNA Germline Short Variant Discovery pipeline includes a crucial step of marking duplicates and merging BAM files. This process is essential for ensuring the accuracy of variant calling by eliminating potential biases introduced by duplicate reads resulting from sequencing artifacts.

### Implementation
The `rmdup_merge.sh` script orchestrates this process using high-throughput computational resources managed by the SLURM job scheduler. The script is tailored for operation on a distributed computing environment and is designed to handle large-scale datasets efficiently.

#### Script Description
The script is executed with the following SLURM settings:

- **Nodes**: 1
- **Memory**: 22G
- **Tasks**: 16
- **CPUs per Task**: 1
- **Time Limit**: 32:00:00
- **Job Name**: rmdup_merge
- **Array Jobs**: Supports batch processing for 1-140 samples, allowing parallel processing of multiple samples.

The script starts by setting environment variables and loading necessary modules such as Java and GATK. It then reads from a predefined list of BAM files and processes each according to its SLURM array task ID.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilized for its `MarkDuplicatesSpark` tool which identifies and marks duplicate reads in BAM files. This tool is chosen for its ability to handle large datasets and its integration with Apache Spark for improved performance.

#### Process Flow
1. **Input and Output Directories**: Specified through environment variables.
2. **File Preparation**: Reads from a list containing paths to BAM files and maps them to their respective subject IDs.
3. **Execution**:
   - For each subject (or sample), the script merges and marks duplicates across multiple BAM files using `MarkDuplicatesSpark`.
   - It utilizes Spark to perform operations locally but is configured to work under the computational limits specified by SLURM job parameters.
   - Outputs are directed to specified output directories, and each merged BAM file is named after the subject ID.
   
4. **Post-Processing**:
   - Generation of metrics files for each processed BAM to evaluate the quality of the merging and marking steps.
   - Cleanup operations include removing temporary directories created during the job execution to conserve storage space.

### Quality Assurance
This script incorporates error handling and robust logging mechanisms to ensure that each step of the process is recorded for audit and troubleshooting purposes. Output and error logs are saved to specified directories, allowing easy access to detailed run-time logs and error messages.

### Conclusion
The duplicate marking and merging step is pivotal for preparing sequenced data for subsequent analysis phases like variant calling. By integrating robust tools and leveraging high-performance computing resources, our pipeline ensures that data processed through this stage is of the highest fidelity, setting a strong foundation for accurate and reliable variant discovery.


-e 

File: ./design_doc/dna_germline_short.md

---
layout: default
title: DNA germline short variant discovery
parent: Design documents
has_children: true
---

<h1>Germline short variant discovery (SNPs + Indels) and interpretation</h1>
<h2>Design document</h2>

{: .no_toc }
<details open markdown="block">
<summary>Table of contents</summary>
{: .text-delta }
- TOC
{:toc}
</details>

---


## Introduction
DNA sequence data is received from the sequencing facility in FASTQ format.
It is generally considered that approximately 20-40% of rare genetic disease may be detectable by DNA germline short variant discovery and variant interpretation
Specifically, this is tailored to single nucleotide variants (SNVs) and short insertion/deletions (INDELs) in protein coding genes and splice sites.

To produce our final output (e.g. clinical grade reports, downstream statistical analysis), we must process the data as follows:
1. QC for pre-processing
2. Decide on protocol, alignment build, threshold criteria
3. Annotate known effect
4. Interpret impact
5. Report clinical grade genetics
6. Cohort analysis for variant association
7. Any additional custom outputs

<img 
src="{{ "pages/design_doc/images/variant_annotation_graph.png" | relative_url }}"
width="100%">
Figure: Summary of DNA germline short variant discovery pipeline plan.

## Summary of requirements

### Quality control
We use a number of tools such as [github.com/OpenGene/fastp](https://github.com/OpenGene/fastp) 
for comprehensive quality profiling for both before and after filtering data.

### Alignment
* We use [BWA](https://bio-bwa.sourceforge.net) for alignment with GRCh38.
* Our reference genome build is GRCh38 from following source:

```
ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz
```

This genome build is currently considered the best for our application.
You can read more about it on
[Illumina's website](https://www.illumina.com/science/genomics-research/articles/dragen-demystifying-reference-genomes.html) and 
[Heng Li's website](https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use).

* HPC requirements
    * Array of jobs - 1 task per fastq file
    * 8 cpu per task
    * 6 hours per task
    * 12G memory per task
    * Example testing: requires 2hr 15min for a 5.3Gb fastq pair

### Variant calling

We will implement the genome analysis tool kit 
[GATK](https://gatk.broadinstitute.org/hc/en-us)
best practices workflow for 
[germline short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels) (open source licence [here](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT)).
This workflow will be designed to operate on a set of samples constituting a study cohort; 
specifically, a set of per-sample BAM files that have been pre-processed as described in the GATK Best Practices for data pre-processing.

The following pages go into details:
* [GATK Duplicates](gatk_duplicates.html)
* [GATK BQSR](gatk_bsqr.html)
* [GATK Haplotype caller](gatk_hc.html)
* [GATK Genomic db import](gatk_dbimport.html)
* [GATK Genotyping gVCFs](gatk_genotypegvcf.html)
* [GATK VQSR](gatk_vqsr.html)
* [GATK Genotype refine](gatk_genotyperefine.html)
* [Pre-annotation processing](pre_annoprocess.html)
* [Pre-annotation MAF](pre_anno_maf.html)

### Annotation

Variant annotation is a critical step in clinical and statistical genetics.
Popular tools for applying annotation data to VCF format genetic data include:

* Variant Effect Predictor (VEP) [link: VEP](http://www.ensembl.org/info/docs/tools/vep/index.html)
* NIRVANA [link: NIRVANA](https://illumina.github.io/NirvanaDocumentation/)
* ANNOVAR [link: ANNOVAR](https://annovar.openbioinformatics.org/en/latest/)

We are using [VEP](http://www.ensembl.org/info/docs/tools/vep/index.html) with [Conda](https://docs.conda.io/en/latest/), but we are likely to test additional methods
([licence](http://www.ensembl.org/info/about/legal/code_licence.html)).
Additionally, these tools must be paired with a set of data sources containing the annotation information which will be applied to each variant.
* [View our list of approx. 160 databases]({{ site.baseurl }}{% link pages/annotation_table.md %}).

The variant consequence may be one of the defining criteria by which variants can 
be included in analysis since they are _interpretable_ or of ostensibly _known significance_.

The consequences provided by VEP can provide a simple reference example to understand its function.
For example, HIGH impact variants might be a likely consequence for identifying candidates disease-causing:
[Ensembl Variation - Calculated variant consequences](https://grch37.ensembl.org/info/genome/variation/prediction/predicted_data.html#consequences).\

{: .note }
You may have observed cases in literature where clinical research reporting relied on variant effect consequence alone for known disease-related genes, but this practice is likely to introduce spurious results. 
It is important to use established criteria for selecting consequences of interest combined with additional filtering methods to define evidence thresholds.
See the ACMG interpretation standards for examples.

<img 
src="{{ "pages/design_doc/images/VEP_consequences.jpg" | relative_url }}"
width="100%">

### Interpretation

We will perform a range of interpretation steps for:
1. Generalised single-case clinical variant classification
2. Cohort-level classification

For example, we will perform interpretation of variants by ACMG standards and guidelines.
Extensive annotation is applied during our genomics analysis.
Interpretation of genetic determinants of disease is based on many evidence sources.
One important source of interpretation comes from the
Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology
[richards2015standards], full text at doi:
[10.1038/gim.2015.30](https://www.gimjournal.org/article/S1098-3600(21)03031-8/fulltext).

Check the tables linked here:
[temporary link](https://lawlessgenomics.com/topic/acgm-criteria-table-main).
These are provided as they appear in the initial steps of our filtering protocol for the addition of ACMG-standardised labels to candidate causal variants.
* Criteria for classifications
* Caveats implementing filters

Implementing the guidelines for interpretation of annotation requires multiple programmatic steps. 
The number of individual caveat checks indicate the number of bioinformatic filter functions used.
Unnumbered caveat checks indicate that only a single filter function is required during reference to annotation databases.
However, each function depends on reference to either one or several evidence source databases (approximately 160 sources).

For reference, alternative public implementations of ACMG guidelines can be found in [li2017intervar] and [xavier2019tapes];
please note these tools have not implemented here nor is any assertion of their quality offered.
Examples of effective variant filtering and expected candidate variant yield in studies of rare human disease are provided by [pedersen2021effective].


We plan to use our tools built for these requirements which are currently in review:
* ACMGuru for automated clinical genetics evidence interpretation. 
* ProteoMCLustR for unbiased whole-genome pathway clustering; 
* SkatRbrain for statistical sequence kernel association testing with variant collapse.
* UntangleR for pathway visualisation.
* AutoDestructR for protein structure variant mapping. 
All tools were designed for modular automated high-performance computing. 


-e 

File: ./design_doc/gatk_1.md



-e 

File: ./design_doc/pre_anno_maf.md

---
layout: default
title: Pre-annotation MAF
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 8
---

## Pre-Annotation MAF Filtering

### Overview
The Pre-Annotation MAF Filtering step selectively removes variants from VCF files that exceed a specified minor allele frequency threshold. This step is crucial for focusing analyses on rare variants, which are often of particular interest in studies of rare genetic disorders.

### Implementation
The `13_pre_annotation_MAF.sh` script employs vcftools to apply MAF-based filtering to the VCF files prepared in the previous steps. This script is optimized to process multiple chromosomal segments, ensuring that only variants with desired allele frequency characteristics are retained for further analysis.

#### Script Description
Configured for resource-intensive tasks:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: maf_pre_annotation
- **Job Array**: Processes up to 25 chromosomal segments.

The script sets up the necessary environment and directories, preparing for efficient execution of MAF filtering.

#### Tools Used
- **vcftools**: This tool is used for applying filters to VCF files based on allele frequency data, effectively removing common variants according to the specified MAF threshold.

#### Process Flow
1. **Input File Checks**:
   - Verifies the presence of input VCF files to ensure that all necessary data is available for processing.

2. **MAF Filtering**:
   - Applies a filter to exclude variants with a MAF higher than the specified threshold (0.4 in this setup), focusing the dataset on rarer variants.
   - Recodes the VCF to include only the variants that pass the filtering criteria.

3. **Output Compression and Indexing**:
   - Compresses the filtered VCF files using bgzip and creates indexed files with tabix to facilitate efficient data retrieval and further processing.

4. **Cleanup**:
   - Removes intermediate files to conserve storage space and maintain a clean working environment.

### Quality Assurance
This step includes detailed logging of all operations and stringent checks to ensure that filtering is applied correctly and comprehensively. Error handling mechanisms safeguard against potential data processing issues.

### Conclusion
MAF filtering is a critical component of our variant analysis pipeline, enabling researchers to focus on variants of interest by filtering out common genetic variations. This process not only refines the dataset but also ensures that subsequent analyses, such as variant annotation and association studies, are more targeted and meaningful.


-e 

File: ./design_doc/gatk_vqsr.md

---
layout: default
title: GATK VQSR
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 6
---

## Variant Quality Score Recalibration (VQSR)

### Overview
Variant Quality Score Recalibration (VQSR) is an advanced technique used in our pipeline to assess and recalibrate the confidence scores of identified variants. By using known, reliable sources as benchmarks, VQSR improves the accuracy of variant calls, distinguishing true biological variants from technical artifacts.

### Implementation
The `10_vqsr.sh` script executes the VQSR process using GATK's VariantRecalibrator and ApplyVQSR tools. This script is configured for high-demand computational tasks to handle variant data across all chromosome segments.

#### Script Description
Optimized for high-throughput:

- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 4
- **Time Limit**: 96:00:00
- **Job Name**: vqsr
- **Job Array**: Capable of processing 25 chromosome segments in a single run.

The script begins by preparing the environment, loading necessary modules, and setting up directories.

#### Tools Used
- **GATK (v4.4.0.0)**: Employed for both the VariantRecalibrator and ApplyVQSR tools, which together adjust the quality scores of variants based on their statistical likelihood of being true genetic variants.

#### Process Flow
1. **Input Setup**:
   - Retrieves gVCF files for each chromosome from the previous genotyping step.

2. **Execution of VQSR**:
   - For SNPs and INDELs, separate recalibration and application steps are performed:
     - **VariantRecalibrator**: Generates recalibration models based on a set of user-defined annotations and known variant sites.
     - **ApplyVQSR**: Applies the recalibration model to the variant calls, filtering or adjusting their quality scores based on the recalibration.

3. **Known Sites and Resources**:
   - Utilizes multiple databases such as HapMap, Omni, 1000 Genomes, and Mills for recalibration, which help in training the recalibration model to distinguish between high-confidence and low-confidence variants.

4. **Output Generation**:
   - Produces recalibrated VCF files for SNPs and INDELs, which are stored in specified output directories.

### Quality Assurance
Includes extensive logging and monitoring of the recalibration process to ensure accuracy and efficiency. Error handling and checkpointing ensure that the process can resume from intermediate stages in case of failures.

### Conclusion
VQSR is essential for our DNA Germline Short Variant Discovery pipeline as it refines variant calls, enhancing the reliability and accuracy of downstream genetic analyses. This process leverages comprehensive known variant datasets and robust computational resources to maintain high standards of variant calling.

---



-e 

File: ./design_doc/gatk_genotypegvcf.md

---
layout: default
title: GATK Genotyping gVCFs
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 5
---

## Genotyping gVCFs

### Overview
Following the import of genomic variant call format (gVCF) files into a GenomicsDB workspace, the next stage in our pipeline involves genotyping these consolidated gVCFs. This step is crucial for calling variants across multiple samples simultaneously, which enhances the discovery and accuracy of genetic variants.

### Implementation
The `09_genotype_gvcf.sh` script manages the genotyping of variants from the GenomicsDB workspaces. This process uses the GATK's GenotypeGVCFs tool, specifically tailored to handle large genomic datasets with high computational efficiency.

#### Script Description
Configured for intensive computational tasks:

- **Dependency**: Waits for previous jobs to complete, ensuring that all necessary data is available for genotyping.
- **Nodes**: 1
- **Memory**: 30G
- **CPUs per Task**: 2
- **Time Limit**: 96:00:00
- **Job Name**: genotype_gvcf
- **Job Array**: Capable of processing 25 chromosomal segments in one batch.

The script starts by setting up the required environment, sourcing variables, and preparing input and output directories.

#### Tools Used
- **GATK (v4.4.0.0)**: Utilized for its `GenotypeGVCFs` tool, which is designed to perform the final genotyping step on the aggregated gVCF data stored in a GenomicsDB workspace.

#### Process Flow
1. **Input and Output Setup**:
   - Directories are established based on predefined paths, where GenomicsDB workspaces are the input and the output is specified as genomic VCF files.

2. **Execution of Genotyping**:
   - For each job in the array, corresponding to a specific chromosome or chromosomal segment, the script accesses the appropriate GenomicsDB workspace.
   - The `GenotypeGVCFs` command is executed to produce a gVCF file for each chromosome, containing the genotyped variants.

3. **Optimization and Resource Management**:
   - The Java options are configured to optimize memory usage and parallel processing capabilities to manage the large data volumes typically involved in genomic analysis.

### Quality Assurance
This stage includes comprehensive logging and error tracking to ensure the process is executed correctly and efficiently. Each step's outputs are systematically verified to maintain high data integrity and reproducibility.

### Conclusion
Genotyping of gVCFs is an essential process in our DNA Germline Short Variant Discovery pipeline, enabling the detailed analysis of genetic variations across multiple samples. By leveraging high-performance computing resources and sophisticated bioinformatics tools, this step ensures that our pipeline produces accurate and reliable variant calls.

---



-e 

File: ./design_doc/rna_seq.md

## RNA seq
Recommendation from 
Dr. sc. nat. Adhideb Ghosh
ETH Zürich, Functional Genomics Center Zürich, Switzerland

* nextflow RNAseq pipeline
    * adapted the workflow from Babraham institute (https://github.com/s-andrews/nextflow_pipelines/blob/master/nf_rnaseq) by also including the quantification step using featureCounts.

* STAR alignment of Kallisto pseudo-alignment
    * directly on command line.

* edgeR/DeSeq2 pipelines for performing differential gene expression analysis 
    * flexibility to check for outliers or confounders.

* Concise overview of different RNAseq normalization <https://hbctraining.github.io/DGE_workshop/lessons/02_DGE_count_normalization.html>

### Read length
Phase 1 we did 75 nucleotide paired end sequencing. So to my knowledge the read length should be 75bp.
Phase 2 we did 100PE mRNA-seq with 25M clusters per library. So the read length should be 100bp.

-e 

File: ./design_doc/design_doc.md

---
layout: default
title: Design documents
has_children: true
nav_order: 5
---

# Index page for design documents

Planned documents:
1. [DNA germline short variant discovery](dna_germline_short.html)
    - [GATK Duplicates](gatk_duplicates.html)
    - [GATK BQSR](gatk_bsqr.html)
    - [GATK Haplotype caller](gatk_hc.html)
    - [GATK Genomic db import](gatk_dbimport.html)
    - [GATK Genotyping gVCFs](gatk_genotypegvcf.html)
    - [GATK VQSR](gatk_vqsr.html)
    - [GATK Genotype refine](gatk_genotyperefine.html)
    - [Pre-annotation processing](pre_annoprocess.html)
    - [Pre-annotation MAF](pre_anno_maf.html)
2. DNA structural variation
3. RNA quantitative expression
4. Metabolomics and proteomics


-e 

File: ./design_doc/gatk_bsqr.md

---
layout: default
title: GATK BQSR
parent: DNA germline short variant discovery
grand_parent: Design documents
nav_order: 2
---

## Base Quality Score Recalibration (BQSR)

### Overview
The Base Quality Score Recalibration (BQSR) step in our pipeline is designed to adjust the base quality scores in BAM files based on known sites of variation and sequencing artifacts. This recalibration helps to mitigate biases that might have been introduced during sequencing.

### Implementation
The `06_bqsr.sh` script manages the BQSR process using the Genome Analysis Toolkit (GATK). It operates within a high-throughput computational framework facilitated by the SLURM job scheduler, specifically configured for efficient handling of genomic data.

#### Script Description
The script employs specific SLURM settings to optimize the use of computational resources:

- **Nodes**: 1
- **Memory**: 6G
- **CPUs per Task**: 4
- **Time Limit**: 32:00:00
- **Job Name**: bqsr
- **Job Array**: Processes batches of 1-8 samples simultaneously.

The script initializes by setting up the necessary environment, loading computational modules, and defining input and output directories based on predefined variables.

#### Tools Used
- **GATK (v4.4.0.0)**: Used for its `BaseRecalibrator` and `ApplyBQSR` tools to recalibrate base quality scores based on multiple known sites databases.

#### Process Flow
1. **File Handling**:
   - Locates BAM files previously processed for duplicate marking and merging.
   - Assigns each file to a specific SLURM array job based on its task ID.
   
2. **Recalibration Execution**:
   - The `BaseRecalibrator` tool generates a recalibration table based on the input BAM, a reference genome, and known variant sites.
   - The `ApplyBQSR` tool then adjusts the base quality scores using the generated recalibration table, producing a recalibrated BAM file.

3. **Known Sites**:
   - Multiple databases are used as references for known sites, including dbSNP and the Mills and 1000 Genomes gold standard indels, to ensure comprehensive coverage and accuracy in recalibration.

4. **Output**:
   - Each sample results in a recalibrated BAM file and an accompanying BQSR table, stored in the designated output directory.

### Quality Assurance
This step includes detailed logging of each process, capturing standard output and errors to facilitate troubleshooting and ensure reproducibility. The script also features robust error handling mechanisms to address potential failures during the recalibration process.

### Conclusion
BQSR is a critical component of our variant discovery pipeline, enhancing the reliability of variant calls by refining the accuracy of base quality scores in sequenced data. By leveraging advanced tools and high-performance computing resources, this step ensures that subsequent analyses are based on the highest quality data.


-e 

File: ./read_group.md

---
layout: default
title: Read group
date: 2024-06-11 00:00:01
nav_order: 5
---

Last update: 20240611


<!-- {: .no_toc } -->
<!-- <details open markdown="block"> -->
<!-- <summary>Table of contents</summary> -->
<!-- {: .text-delta } -->
<!-- - TOC -->
<!-- {:toc} -->
<!-- </details> -->
<!-- --- -->

This concept of "read groups" in sequencing data described here is copied from the GATK documentation by Derek Caetano-Anolles:
<https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups>.

You may be intersting in using this while doing [aggregate multiplex](aggregate_multiplex.html).

**Understanding Read Groups in Sequencing Data**

**Definition**: A 'read group' is a collection of reads from a single run of a sequencing instrument. In simpler setups where one library preparation from a biological sample is run on a single lane, all reads from that lane constitute a read group. In more complex cases involving multiplexing, each subset of reads from separate library preparations run on the same lane forms distinct read groups.

**Identification**: Read groups are identified by specific tags in the SAM/BAM/CRAM file, defined in the official SAM specification. These tags include:
- **ID**: Read group identifier, unique per read group, referenced in the read record and often based on the flowcell and lane.
- **PU**: Platform Unit, captures information about the flowcell barcode, lane, and sample barcode.
- **SM**: Sample name, indicating the sample sequenced in the read group.
- **PL**: Platform used, such as ILLUMINA or PACBIO.
- **LB**: DNA preparation library identifier.

**Importance**: Proper assignment of these tags is crucial for differentiating samples and mitigating technical artifacts during data processing steps like duplicate marking and base recalibration.

**Example Usage**:
- To view read group information in a BAM file:
  ```
  samtools view -H sample.bam | grep '^@RG'
  ```
  This command extracts lines starting with `@RG` from the BAM header, revealing the read group details.

**Example Read Group Fields**:
```
@RG ID:H0164.2 PL:illumina PU:H0164ALXX140820.2 LB:Solexa-272222 SM:NA12878
```
- **ID**: H0164.2 (flowcell and lane)
- **PU**: H0164ALXX140820.2 (flowcell, lane, and sample barcode)
- **LB**: Solexa-272222 (library identifier)
- **SM**: NA12878 (sample name)
- **PL**: illumina (sequencing platform)

**Multi-sample and Multiplexed Example**:
For a trio of samples (MOM, DAD, KID) with two libraries each (200 bp and 400 bp inserts), and each library sequenced across two lanes, the read group tags in the headers might appear as follows:

- **Dad’s Data**:
  ```
  @RG ID:FLOWCELL1.LANE1 PL:ILLUMINA LB:LIB-DAD-1 SM:DAD PI:200
  @RG ID:FLOWCELL1.LANE2 PL:ILLUMINA LB:LIB-DAD-1 SM:DAD PI:200
  @RG ID:FLOWCELL1.LANE3 PL:ILLUMINA LB:LIB-DAD-2 SM:DAD PI:400
  @RG ID:FLOWCELL1.LANE4 PL:ILLUMINA LB:LIB-DAD-2 SM:DAD PI:400
  ```
- **Mom’s and Kid's Data** similarly detailed.

## An example

While doing alignment with BWA I check that the info is updated like this: 

`# This could go in variables.sh with more explicite names
sm=$(echo ${sample_id} | awk -F '_' '{print $1}')
pu=$(zcat ${FILE1} | awk 'NR==1 {split($1,a,":"); print a[3] "." a[4] "." "'$sm'
"}')
lb=$(echo ${sample_id} | awk -F '_' '{print $1 "_" $2}')
pl="NovaSeq6000_WGS_TruSeq"

echo "ID = ${sample_id}"
echo "SM = ${sm}"
echo "PL = ${pl}"
echo "PU = ${pu}"
echo "LB = ${lb}"

# Define your read group
rg="@RG\tID:${sample_id}\tSM:${sm}\tPL:${pl}\tPU:${pu}\tLB:${lb}"

echo "RG = ${rg}"

echo "starting bwa mem and samtools"
bwa mem \
        ${REF} \
        ${FILE1} \
        ${FILE2} \
        -R $rg \
        -v 1 -M -t 8 |\
        samtools view --threads 8 -O BAM -o ${output_file}

# check read group e.g.
# samtools view -H HCY073_NGS000011412_NA_S20_L004.bam | grep '^@RG'
# remove fq temp files
# we can also use logs to see if we have any read group collision which should b
e unique`

Then in GATK when files are being merged later in BAM format, `MarkDuplicatesSpark` handles the read group info correctly from each individual sample for a subject. 


**Conclusion**: Understanding and correctly implementing read group information is critical for high-quality genomic data processing, helping distinguish between various technical and biological factors that affect sequencing outcomes.



-e 

File: ./concepts.md

---
layout: default
title: Data concepts
date: 2023-06-16 00:00:01
nav_order: 5
---

Last update: 20230619

{: .no_toc }
<details open markdown="block">
<summary>Table of contents</summary>
{: .text-delta }
- TOC
{:toc}
</details>

---

## How data is collected and stored
* The Swiss Personalised Health Network (SPHN): collects and hosts omic data that we will ultimately use
* Branch of SPHN: [Data Coordination Center (DCC)](https://sphn.ch/network/data-coordination-center/)
* DCC is also a part of [Swiss Institute of Bioinformatics (SIB)](https://www.sib.swiss/personalized-health-informatics)
* The data is to be stored and processed on [BioMedIT network](https://www.biomedit.ch)
* Our tenant on BioMedIT is called MOMIC and this is on the [sciCORE infrastructure](https://scicore.ch).
* The database structure requires Resource Description Framework [(RDF) Schema](https://www.biomedit.ch/rdf/sphn-ontology/sphn)

## Database concepts
The DCC is responsible for making research data suitable for database management. 
To do this they design the concepts.
Training [material available here](https://sphn.ch/training/), but is a DCC responsibility.

Several concepts for the types of data that we use alredy exist.
e.g. genetic data concepts.

We also assist in generating new omics-related concepts for future expansion. 
This page will be updated to summarise the progress as these new concepts are defined. 

<img title="Illustration Semantic-Interoperability-Framework" alt="Alt text" src="https://www.sib.swiss/images/sib/7-about-us/media/news_2021/Semantic-Interoperability-Framework.png">

## Availability 
The following information is copied from <https://sphn.ch/2023/03/20/sphn-dataset-rdf-schema-2023-release/>

* The SPHN Dataset is openly available here: <https://sphn.ch/document/sphn-dataset/>
* The SPHN RDF Schema is browsable here: <https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2>
* The external terminologies in RDF are accessible on BioMedIT Portal: <https://portal.dcc.sib.swiss/>
* The Quality Assurance Framework is available here: <https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/quality_assurance>
* Project templates are available here: <https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/templates>
* A comprehensive documentation is openly accessible here: <https://sphn-semantic-framework.readthedocs.io/en/latest/>

## Specific genetic examples
* <https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#GeneticVariation>
* <https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#SingleNucleotideVariation>
* <https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#VariantDescriptor>
* <https://www.biomedit.ch/rdf/sphn-ontology/sphn/2023/2#VariantNotation>

## Potential concepts
 <hr/>
<img src="{{ "assets/images/concept_ideas_1.png" | relative_url }}" width="100%">
 <hr/>
<img src="{{ "assets/images/concept_ideas_2.png" | relative_url }}" width="100%">
 <hr/>
<img src="{{ "assets/images/concept_ideas_3.png" | relative_url }}" width="100%">
 <hr/>
<img src="{{ "assets/images/concept_ideas_4.png" | relative_url }}" width="100%">

## Examples

### Genomics England (GE)
GE represents one of the best national platforms so far and they have complete analysis of ~200'000 genomes for clinical use.
* [Homepage](https://www.genomicsengland.co.uk)
* [Bioinformatics page](https://www.genomicsengland.co.uk/bioinformatics)

GE model documentation shows the concepts in use: <http://gelreportmodels.genomicsengland.co.uk/models.html#>.
Concept development could start with the GE 1.3.0-SNAPSHOT. 
For example, `VariantMetadata` is a good starting place since it includes "individual" (i.e. subject), "sample type", "experiment", etc., which are required in most genomic data scenarios. 
We could work through a prioritised list of concepts over several months-years for any new concept that matches user requirements.
They have derived some logical structures derived from Ontobee, OpenCB, and probably other common sources.

**Example: metadata**

Here are some examples which most users will require, from `VariantMetadata`: 

```
|--VariantMetadata:
   |-- Cohort
      |-- Experiment
         |--- center, date, molecule, technique, library, libraryLayout, platform, description
   |-- Individual
       |-- id, family, father, mother, sex, phenotype, samples
   |-- Program
       |-- ...
   |-- Sample
       |-- ...
   |-- SampleSetType
	    |-- CASE_CONTROL, CASE_SET, CONTROL_SET, PAIRED, TIME_SERIES, FAMILY, TRIO, MISCELLANEOUS, UNKNOWN
   |--Species
       |-- ...
```

**Example: variant**
Some of the most important concepts for our genomics needs are listed on the [variant procol page](http://gelreportmodels.genomicsengland.co.uk/html_schemas/org.opencb.biodata.models.variant.avro/1.3.0-SNAPSHOT/variant.html#/schema/org.opencb.biodata.models.variant.avro.Variants), which come from org.opencb.biodata.models.variant.avro.
These are beyond our current needs but this list of 55 entries likely cover most of the conceivable needs:

```
AdditionalAttribute, AlleleOrigin Enum, AllelesCode Enum, AlternateCoordinate, ClinVar, ClinicalSignificance Enum, Confidence Enum, ConsequenceType, ConsistencyStatus Enum, Cosmic, Cytoband, Drug, DrugResponseClassification Enum, EthnicCategory Enum, EvidenceEntry, EvidenceImpact Enum, EvidenceSource, EvidenceSubmission, ExonOverlap, Expression, ExpressionCall Enum, FeatureTypes Enum, FileEntry, GeneDrugInteraction, GeneTraitAssociation, GenomicFeature, Genotype, Gwas, HeritableTrait, ModeOfInheritance Enum, Penetrance Enum, PopulationFrequency, Property, ProteinFeature, ProteinVariantAnnotation, Repeat, Score, SequenceOntologyTerm, SomaticInformation, StructuralVariantType Enum, StructuralVariation, StudyEntry, TraitAssociation Enum, TumorigenesisClassification Enum, VariantAnnotation, VariantAvro, VariantClassification, VariantFunctionalEffect Enum, VariantHardyWeinbergStats, VariantStats, VariantTraitAssociation, VariantType Enum, Xref,
```

Let's pick one example from that list which would come under the heading "variant interpretation" - `ClinicalSignificance`:

**Example: variant interpretation - `ClinicalSignificance`**

We use this exact example for our clinical genetics work: [ClinicalSignificance](http://gelreportmodels.genomicsengland.co.uk/html_schemas/org.opencb.biodata.models.variant.avro/1.3.0-SNAPSHOT/variant.html#/schema/org.opencb.biodata.models.variant.avro.ClinicalSignificance).

Mendelian variants classification with ACMG terminology as defined in Richards, S. et al. (2015). Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology. Genetics in Medicine, 17(5), 405?423. <https://doi.org/10.1038/gim.2015.30.>

Classification for variants associated with disease, etc., based on the ACMG recommendations and ClinVar classification
(<https://www.ncbi.nlm.nih.gov/clinvar/docs/clinsig/>).
* `benign_variant` : Benign variants interpreted for Mendelian disorders
* `likely_benign_variant` : Likely benign variants interpreted for Mendelian disorders with a certainty of at least 90%
* `pathogenic_variant` : Pathogenic variants interpreted for Mendelian disorders
* `likely_pathogenic_variant` : Likely pathogenic variants interpreted for Mendelian disorders with a certainty of at least 90%
* `uncertain_significance` : Uncertain significance variants interpreted for Mendelian disorders. Variants with conflicting evidences should be classified as uncertain_significance
* Enum symbols:
	* `benign`, `likely_benign`, `VUS`, `likely_pathogenic`, `pathogenic`, `uncertain_significance`

However, we do not restrict our use of ACMG standard for variant interpretation using only `ClinicalSignificance`, since there are a large number of other variant interpretation datasets which can be used to make the final determination.
One method we use is the ACMG scoring sytem to score all variants based on the ACMG evidence categorisation method.
Therefore, `ACMG_score` could be derived from `ClinicalSignificance`, but also from other sources.
Perhaps [EvidenceEntry](http://gelreportmodels.genomicsengland.co.uk/html_schemas/org.opencb.biodata.models.variant.avro/1.3.0-SNAPSHOT/variant.html#/schema/org.opencb.biodata.models.variant.avro.EvidenceEntry) is a major entry which includes most of these subtypes for interpretation evidence.

### OpenCB
* Used for some Genomics England concepts
* OpenCB which provides scalable a storage engine framework with data and metadata catalogue.
* <https://github.com/opencb/opencga>
* <http://docs.opencb.org>

### Ontobee
* Used for some Genomics England concepts
* Ontology data server with RDF source code.
* Example GE concept for "Variants / AlleleOrigin / germline_variant" comes from :
* <https://ontobee.org/ontology/SO?iri=http://purl.obolibrary.org/obo/SO_0001762>
* [germline_variant RDF source code](https://ontobee.org/ontology/rdf/SO?iri=http://purl.obolibrary.org/obo/SO_0001778)

```
|-- sequence_attribute
   |-- variant_quality
   |-- variant_origin
      |-- maternal_variant
      |-- paternal_variant
      |-- somatic_variant
      |-- pedigree_specific_variant
      |-- population_specific_variant
      |-- de_novo_variant
      |-- germline_variant
         |-- RDF sourcode 
```

-e 

File: ./progress.md

---
layout: default
title: Progress reports
nav_order: 5
---

# Progress reports

Last update: 20230707

Author: Dylan Lawless

<!-- ![Bioinformatic progress](./gant.png){width=70% height} -->
\includegraphics[scale=0.4]{./gant.png}


## Month: 202307 (start date)

### Complete
* SIB security training
    * Responsible use of health data
    * Responsible Use of BioMedIT
* UZH VPN via Kispi
* BioMedIT server access complete:
    * project: MOMIC,  node: sciCOREmed (scicore), University of Basel
    * project: Trans-omic analysis, node: Leonhard Med, Scientific IT Services (SIS), ETH Zurich
* Editor environment: 
    * scicore - done
    * LeoMed - in progress
* Decision on code management
* Decision on containerisation
* completion of SPSS exome paper
* WES pipeline package for BioMedIT

### Planned
* repository imports
* reference database test
* slurm sbatch test
* gitlab test
* snakemake test
* conda/mamba test
* ssh from kispi hardware
* meetings with collaborating groups

 Project start: 20230701

----

## Month: 202306 (pre-start date)
* SIB course on Enrichment Analysis (June 23).
* SPHN DCC workshop on concept development.
* Concept development examples of needs and reference to models: <https://swisspedhealth-pipelinedev.github.io/docs/pages/concepts.html>
* PipeDev - first design doc started: <https://swisspedhealth-pipelinedev.github.io/docs/pages/design_doc/dna_germline_short.html>

## Month: 202305 (pre-start date)
* PipeDev - documentation website v1 live: <https://swisspedhealth-pipelinedev.github.io/docs/pages/variables.html>
* PipeDev - data sources database started: <https://swisspedhealth-pipelinedev.github.io/docs/pages/annotation_table>
* PipeDev - styles defined
* PipeDev - presentation slides

## Month: All older (pre-start date)
* PipeDev - general plan slides: <https://swisspedhealth-pipelinedev.github.io/docs/pages/present/presentations.html>
* PipeDev - git orgnaisation reserved for publications: <https://github.com/SwissPedHealth-PipelineDev>
* PipeDev - git repo started
* SwissPedHealth - website demo v2
* SwissPedHealth - website demo v1

---


-e 

File: ./bwa.md

---
layout: default
title: BWA
date: 2023-07-27 00:00:01
nav_order: 5
---

Last update: 20230727
## BWA

* GATK: (How to) Map reads to a reference with alternate contigs like GRCH38 <https://gatk.broadinstitute.org/hc/en-us/articles/360037498992--How-to-Map-reads-to-a-reference-with-alternate-contigs-like-GRCH38>

SMOC data arrives in multiple batches to ensure read depth, etc.,
resulting in multiple sets of FASTQ files per sample all of which should have distinct read group IDs (RGID).
Therefore at some points we must compile all reads for a single subject.

The following is from GATK, which we will also perform:
> (like at the) Broad Institute, we run the initial steps of the pre-processing workflow (mapping and sorting) separately on each individual read group. Then we merge the data to produce a single BAM file for each sample (aggregation); this is done conveniently at the same time that we do the duplicate marking, by running Mark Duplicates on all read group BAM files for a sample at the same time. Then we run Base Recalibration on the aggregated per-sample BAM files. See the worked-out example below for more details.

* GATK: How should I pre-process data from multiplexed sequencing and multi-library designs? <https://gatk.broadinstitute.org/hc/en-us/articles/360035889471-How-should-I-pre-process-data-from-multiplexed-sequencing-and-multi-library-designs->
* GATK: Read groups <https://gatk.broadinstitute.org/hc/en-us/articles/360035890671>


## Read groups

Meaning of the read group fields required by GATK

* ID = Read group identifier 
	* This tag identifies which read group each read belongs to, so each read group's ID must be unique. It is referenced both in the read group definition line in the file header (starting with @RG) and in the RG:Z tag for each read record. Note that some Picard tools have the ability to modify IDs when merging SAM files in order to avoid collisions. In Illumina data, read group IDs are composed using the flowcell name and lane number, making them a globally unique identifier across all sequencing data in the world. 
	* Use for BQSR: ID is the lowest denominator that differentiates factors contributing to technical batch effects: therefore, a read group is effectively treated as a separate run of the instrument in data processing steps such as base quality score recalibration (unless you have PU defined), since they are assumed to share the same error model.
* PU = Platform Unit 
	* The PU holds three types of information, the {FLOWCELL_BARCODE}.{LANE}.{SAMPLE_BARCODE}. The {FLOWCELL_BARCODE} refers to the unique identifier for a particular flow cell. The {LANE} indicates the lane of the flow cell and the {SAMPLE_BARCODE} is a sample/library-specific identifier. Although the PU is not required by GATK but takes precedence over ID for base recalibration if it is present. In the example shown earlier, two read group fields, ID and PU, appropriately differentiate flow cell lane, marked by .4, a factor that contributes to batch effects.
* SM = Sample 
	* The name of the sample sequenced in this read group. GATK tools treat all read groups with the same SM value as containing sequencing data for the same sample, and this is also the name that will be used for the sample column in the VCF file. Therefore it is critical that the SM field be specified correctly. When sequencing pools of samples, use a pool name instead of an individual sample name.
* PL = Platform/technology used to produce the read 
	* This constitutes the only way to know what sequencing technology was used to generate the sequencing data. Valid values: ILLUMINA, SOLID, LS454, HELICOS and PACBIO.
* LB = DNA preparation library identifier 
	* MarkDuplicates uses the LB field to determine which read groups might contain molecular duplicates, in case the same DNA library was sequenced on multiple lanes.


## Our method for creating read group info

NovaSeq SMPC fastq filename
* `<SAMPLE_ID>_<NGS_ID>_<POOL_ID>_<S#>_<LANE>_<R1|R2>.fastq.gz`

NovaSeq SMOC fastq header
* `@<instrument>:<run number>:<flowcell ID>:<lane>:<tile>:<x-pos>:<y-pos> <read>:<is filtered>:<control number>:<sample number>`

Read group definitions
* ID = Read group identifier
* SM = Sample
* PL = Platform/technology used to produce the read
* PU = Platform Unit
* LB = DNA preparation library identifier

Read group sources
* ID = sample_id , e.g. HCY073_NGS000011412_NA_S20_L004
* SM = sample_id regex field 1, e.g. HCY073
* PL = Machine and library method, e.g. "NovaSeq6000_WGS_TruSeq", hardcoded - automate later
* PU = {FLOWCELL_BARCODE}.{LANE}.{SAMPLE_BARCODE} which we can derive from fastq header 1 regex fields: 3.4., and sample_id regex field 1-2.
* LB = sample_id regex field 1-2

```
sm=$(echo ${sample_id} | awk -F '_' '{print $1}')
pu=$(zcat ${FILE1} | awk 'NR==1 {split($1,a,":"); print a[3] "." a[4] "." "'$sm'"}')
lb=$(echo ${sample_id} | awk -F '_' '{print $1 "_" $2}')
pl="NovaSeq6000_WGS_TruSeq"

echo "ID = ${sample_id}"
echo "SM = ${sm}"
echo "PL = ${pl}"
echo "PU = ${pu}"
echo "LB = ${lb}"

`rg="@RG\tID:${sample_id}\tSM:${sm}\tPL:${pl}\tPU:${pu}\tLB:${lb}"`
bwa mem \
        ${REF} \
        ${FILE1} \
        ${FILE2} \
        -R $rg \
        -v 1 -M -t 8 |\
        samtools view --threads 8 -O BAM -o ${output_file}
```

Check read group e.g.`samtools view -H file.bam | grep '^@RG'`.
We can also use logs to see if we have any read group collision which should be unique.


-e 

File: ./bookmarks.md

---
layout: default
title: Bookmarks
date: 2023-06-21 00:00:01
nav_order: 5
---

Last update: 20230621

An unstructured log of bookmarks that we use for guidance

* GATK <https://gatk.broadinstitute.org/hc/en-us>
* Genomics England: Bioinformatics and data science <https://www.genomicsengland.co.uk/bioinformatics>
* Genomics England: model report <http://gelreportmodels.genomicsengland.co.uk/>
* NIH Biowulf HPC <https://hpc.nih.gov>
* Fastq <https://en.wikipedia.org/wiki/FASTQ_format>
* Fastq <https://knowledge.illumina.com/software/general/software-general-reference_material-list/000002211>
* Fastq <https://help.basespace.illumina.com/files-used-by-basespace/fastq-files>

-e 

File: ./aggregate_multiplex.md

---
layout: default
title: Aggregate multiplexed data
date: 2024-06-11 00:00:01
nav_order: 5
---

Last update: 20240611

<!-- {: .no_toc } -->
<!-- <details open markdown="block"> -->
<!-- <summary>Table of contents</summary> -->
<!-- {: .text-delta } -->
<!-- - TOC -->
<!-- {:toc} -->
<!-- </details> -->
<!-- --- -->

The following content is modified from:
> How should I pre-process data from multiplexed sequencing and multi-library designs?

<https://gatk.broadinstitute.org/hc/en-us/articles/360035889471-How-should-I-pre-process-data-from-multiplexed-sequencing-and-multi-library-designs>

We use `05_rmdup_merge.sh` to process bam files for data aggregation and deduplication.

**Script Summary: `05_rmdup_merge.sh`**

**Purpose**: This script is tailored for efficiently merging and deduplicating sequencing data from multiple libraries and lanes per individual subject. It addresses complex setups where subjects are represented across numerous sequencing files.

**Process Description**:
- **Data Preparation**: Each subject's sequencing data, potentially spanning multiple libraries and lanes, is initially processed separately to ensure accurate [read group](read_group.html) assignment and preliminary sorting.
- **Aggregation and Deduplication**:
  - **File Aggregation**: BAM files from the same subject, but different lanes or libraries, are combined into a single dataset. This step merges these various inputs into one unified file.
  - **Deduplication**: Implements GATK’s `MarkDuplicatesSpark` to simultaneously mark and remove both PCR and optical duplicates from the merged files, improving data accuracy and quality.
- **Output Generation**: Outputs a single, consolidated, and deduplicated BAM file for each subject, ready for further analysis like Base Recalibration.

**Example of File Processing**:

- **Input Files**:
  - For subject `sampleA`, files from two different lanes:
    - `sampleA_lane1_R1.fq`
    - `sampleA_lane1_R2.fq`
    - `sampleA_lane2_R1.fq`
    - `sampleA_lane2_R2.fq`
  - For subject `sampleB`, files from two different lanes:
    - `sampleB_lane1_R1.fq`
    - `sampleB_lane1_R2.fq`
    - `sampleB_lane2_R1.fq`
    - `sampleB_lane2_R2.fq`

- **Processing**:
  - These paired FASTQ files are first individually processed to assign [read group](read_group.html)s and generate initial BAM files:
    - From `sampleA_lane1_R1.fq` and `sampleA_lane1_R2.fq` → `sampleA_rgA1.bam`
    - From `sampleA_lane2_R1.fq` and `sampleA_lane2_R2.fq` → `sampleA_rgA2.bam`
    - From `sampleB_lane1_R1.fq` and `sampleB_lane1_R2.fq` → `sampleB_rgB1.bam`
    - From `sampleB_lane2_R1.fq` and `sampleB_lane2_R2.fq` → `sampleB_rgB2.bam`
  - **Aggregation and Deduplication**: The script then aggregates and deduplicates [read group](read_group.html) BAMs for each subject:
    - `sampleA` [read group](read_group.html)s (`sampleA_rgA1.bam` and `sampleA_rgA2.bam`) are merged and deduplicated to produce `sampleA.merged.dedup.bam`.
    - Similarly, `sampleB` [read group](read_group.html)s (`sampleB_rgB1.bam` and `sampleB_rgB2.bam`) are merged and deduplicated to produce `sampleB.merged.dedup.bam`.

- **Output**:
  - The final outputs are deduplicated BAM files for each subject, such as `sampleA.merged.dedup.bam` and `sampleB.merged.dedup.bam`. These files integrate all sequencing data from different lanes or libraries for each subject and are now ready for subsequent quality control steps like Base Recalibration.


-e 

File: ./git.md

---
layout: default
title: git
date: 2023-08-01 00:00:01
nav_order: 5
---

Last update: 20230801

## Forwarding git repos from github and gitlab

This method was written by Vito.

An easier way to sync git repos (dcc gitlab and github) to leomed: 
* Using remote port forwarding via your local machine:
* Local .ssh/config

```
Host leomed
  User vzanotelli
  HostName login-ethsec.leomed.ethz.ch
  ProxyJump vzanotelli@jump-ethsec.leomed.ethz.ch
  ControlMaster auto
  ControlPath ~/.ssh/%r@%h:%p
  RemoteForward 7239 git.dcc.sib.swiss:22 # 7239 can be any unused port
  RemoteForward 7240 github.com:22
```

* On leomed :
1. create a ssh key and register it with git/gitlab
2. configure the gitlab ssh configuration to re-route to your local machine

* leomed ~/.ssh/config
```
Host git.dcc.sib.swiss
	Hostname localhost
	Port 7239 # the same port as above
	IdentityFile ~/.ssh/id_ed25519 # path to your ssh key

Host github.com
	Hostname localhost
	Port 7240
	IdentityFile ~/.ssh/id_ed25519
```
After this you can clone any repository on github/gitlab directly.

-e 

File: ./annotation_table.md

---
layout: default
title: Annotation table
date: 2023-05-31 00:00:01
nav_order: 5
---
<head>
<!-- function to resize table iframe to make height 100% to prevent nested scolling. -->
<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
  }
</script>
</head>

Last update: 20230531

<h1>Annotation table</h1>

<img 
src="{{ "data/annotation_datasets/output/annotation_datasets_category.png" | relative_url }}"
width="100%">

<iframe 
src="{{ "data/annotation_datasets/output/annotation_datasets_table.html" | relative_url }}"
width="100%"
frameborder="0" scrolling="no" onload="resizeIframe(this)" />


<!-- Without the script above, some of these methods may be useful: -->
<!-- <iframe --> 
<!-- src="{{ site.baseurl }}{% link data/annotation_datasets/output/annotation_datasets_table.html %}" -->
<!-- width="100%" -->
<!-- onload="this.height=screen.height;" -->
<!-- ></iframe> --> 
<!-- id="igraph" --> 
<!-- height="5000" -->
<!-- seamless="seamless" -->
<!-- style="border:none;" --> 
<!-- scrolling="no" -->
<!-- onload="this.width=screen.width;" -->
<!-- onload="this.width=screen.width;this.height=screen.height;" -->


-e 

File: ./fastq.md

---
layout: default
title: FASTQ format data
date: 2023-07-27 00:00:01
nav_order: 5
---

Last update: 20230727

{: .no_toc }
<details open markdown="block">
<summary>Table of contents</summary>
{: .text-delta }
- TOC
{:toc}
</details>

---

## FASTQ format data

### Summary 
* Analysis pipelines must account for the run directory name since it is possible that >1 file has the same filename and thus output may be overwritten.
* WGS data from SMOC is produced currently with Novaseq6000.
* h2030gc fastq file names:
    * `<SAMPLE_ID>_<NGS_ID>_<POOL_ID>_<S#>_<LANE>_<R1|R2>.fastq.gz`
* Illumina fastq header:
    * `@<instrument>:<run number>:<flowcell ID>:<lane>:<tile>:<x-pos>:<y-pos> <read>:<is filtered>:<control number>:<sample number>`
    * For the Undetermined FASTQ files only, the sequence observed in the index read is written to the FASTQ header in place of the sample number. This information can be useful for troubleshooting demultiplexing.

| Element	| Requirements	| Description	| 
|---------------|---------------|---------------|
| @	| @	| Each sequence identifier line starts with @ |
| <instrument>	| Characters allowed: a–z, A–Z, 0–9 and underscore	| Instrument ID |
| <run number>	| Numerical	| Run number on instrument |
| <flowcell ID>	| Characters allowed: a–z, A–Z, 0–9	| 
| Flowcell ID	| <lane>	| Numerical	| Lane number |
| <tile>	| Numerical	| Tile number	| <x_pos>	| Numerical	| X coordinate of cluster	| 
| <y_pos>	| Numerical	| Y coordinate of cluster	| 
| <read>	| Numerical	| Read number. 1 can be single read or Read 2 of paired-end	| 
| <is filtered>	| Y or N	| Y if the read is filtered (did not pass), N otherwise	| 
| <control number>	| Numerical	| 0 when none of the control bits are on, otherwise it is an even number. On HiSeq X systems, control specification is not performed and this number is always 0. |
| <sample number>	| Numerical	| Sample number from sample sheet |



### Details
WGS data from SMOC is produced currently with Novaseq6000.
Files are returned in one directory based on the order and several run directories containing the fastq files.

```
|--- order
   |--- run1
      |- s1_ABC_123_S1_L001_R1.fastq.gz
      |- s1_ABC_123_S1_L001_R2.fastq.gz
   |--- run2
   |--- run3
```

File names are structured as follows:

`<SAMPLE_ID>_<NGS_ID>_<POOL_ID>_<S#>_<LANE>_<R1|R2>.fastq.gz`

where

* `<SAMPLE_ID>`: is the sample ID given in the original sample sheet.
* `<NGS_ID>`: the identifier of the library preparation. Usually does not change unless a new sequencing library needs to be prepared.
* `<POOL_ID>`: the identifier of the pool. Your samples have NA here, as they are not pooled.
* `* <S#>`: 'S' followed by a number given by the sequencer.
* `<LANE>`: flow cell lane
* `*<R1|R2>`: reads R1 and R2 (for paired-end sequencing).

In this way, a library sequenced several times to achieve coverage can have the same name if S# is the same (decided by the sequencer).

The FASTQ files are in directories representing individual runs, for example 221031_A00485_0334_AHNFF5DSX3 is run 334, performed on 31/10/2022 on the Novaseq6000 (A00485) and flow cell AHNFF5DSX3.


## Links
* <https://en.wikipedia.org/wiki/FASTQ_format>
* <https://knowledge.illumina.com/software/general/software-general-reference_material-list/000002211>
* <https://help.basespace.illumina.com/files-used-by-basespace/fastq-files>


-e 

File: ./hpc.md

---
layout: default
title: HPC infrastructure
date: 2023-06-16 00:00:01
nav_order: 5
---
Last update: 20230616




## Hardware
Example comparisons

### sciCORE <https://scicore.ch/using-scicore/hpc-resources/>

**Cluster BioMedIT**
* Total nodes 15
* Total cores 976 total, 200 user
* Total RAM 6 TB
* Total GPUs 8
* Inter-connect Eth 100G
* Total Disk 900 TB

**Cluster sciCORE**
* Total nodes 215
* Total cores 13632
* Total RAM 73 TB
* Total GPUs 80
* Inter-connect Eth 100G, Infiniband
* Total Disk 11 PB

### EPFL <https://www.epfl.ch/research/facilities/scitas/jed/>
* Peak performance Rpeak: 2’322 TFLOPs
* Total RAM: 233,5 TB
* Storage: 350 TB
* The cluster is composed of 419 compute nodes, each with
	* 2 Intel(R) Xeon(R) Platinum 8360Y processors running at 2.4 GHz, with 36 cores each (72 cores per machine),
	* 3 TB of SSD disk
* for a total of 30’240 cores (including the frontend node)
* 375 nodes have 512 GB of RAM,
* 42 nodes have 1 TB of RAM,
* 2 nodes have 2 TB of RAM
	* approx 250 TB total

## HPC documentation
sciCORE uses SLURM workload manager <https://slurm.schedmd.com/overview.html>

Examples of documentation on simimlar infrastructure.
* NIH Biowulf <https://hpc.nih.gov>
* For the sciCORE cluster: <https://wiki.biozentrum.unibas.ch/display/scicore/sciCORE+user+guide>
* For using SLURM: <https://wiki.biozentrum.unibas.ch/display/scicore/SLURM+user+guide>
* EPFL SCITAS: <https://www.epfl.ch/research/facilities/scitas/documentation/>

## Acknowledgements
* sciCORE: "Calculations were performed at sciCORE (http://scicore.unibas.ch/) scientific computing core facility at University of Basel."
* sciCORE/SIB: "Calculations were performed at sciCORE (http://scicore.unibas.ch/) scientific computing core facility at University of Basel, with support by the SIB Swiss Institute of Bioinformatics."

## Data stream
Read about where the data is generated, how it comes fro BioMedIT and how the responsibility of management is controlled here: 
[Data stream](data_stream.html)

-e 

File: ./layout.md

---
layout: default
title: Layout
date: 2023-06-16 00:00:01
nav_order: 5
---

Last update: 20230531
## Data generation and control

We rely on strict data control processes. 
Once study participation is established according to the legal and ethical framework, samples are sent for data gernation.
Omic data is generated by SMOC and data is entered and controlled within SPHN DCC BioMedIT sciCORE on the MOMIC tenant. 

## Data generation
SMOC has 3 branches:
* CGAC - Genomics.
	* based in [Health2030 genome center](https://www.health2030genome.ch) Geneva.
* CPAC - Proteotyping.
	* based in [ETHZ Metabolomics & Biophysics](https://fgcz.ch/omics_areas/met.html)
* CMAC - Metabolomics & Lipidomics.
	* based in [ETHZProteomics](https://fgcz.ch/omics_areas/prot.html)

## Data control
Organisation:
* Swiss Personalised Health Network (SPHN)
	* Data Coordination Center (DCC)
	* DCC is also a part of SIB
		* data on BioMedIT network
			* our tenant is called MOMIC
			* on the sciCORE infrastructure
* omic data from SMOC http://smoc.ethz.ch/
* database structure requires [Resource Description Framework (RDF) Schema](https://www.biomedit.ch/rdf/sphn-ontology/sphn)
* training [material available here](https://sphn.ch/training/), but is a DCC responsibility
[Useful image](https://www.sib.swiss/images/sib/7-about-us/media/news_2021/Semantic-Interoperability-Framework.png), our aim it to show #3 "Use cases"
<!-- Semantic-Interoperability-Framework.png -->

## Simplified example
* For our genetic analysis dataset we might have 1000 patient_ID
* For each patient_ID we have several sequence files and a sequence metadata file
* For each patient_ID we require all available clinical data

**Genetic_sequence_data:**
```
file_name: SPH_001_R1.fq.gz
file_content: FASTQ sequence raw data
storage_location: ./project/data/
```

**Genetic_sequence_meta_data:**
```
sample_facility: SMOC
sequence_method: exome_capture
sequence_preparation: twist_humancorePlusRefSeq_hg38
sequence_technology: NovaSeq6000
demultiplexing_version: 1.1.13
order_ID: 12345
sample_reception: 20210102
sample_completion: 20210109
patient_ID: SPH_001
md5sun: 7d776c010149208c782ed7253ce70159
file_name: SPH_001_R1.fq.gz
```

**Clinical data:**
```
patient_ID: SPH_001
hospital_ID: bern_01
age_days: 100
sex: 0
admission_date: 20210101
discharge_data: 20210201
picu.adm: NA
death.date: NA
comorbities: 0
clin.focus: UTI
pathogen_found: p.aeruginosa
severity_score: 5
```

## Workflow order
**First stage order:**
1. Clinical data is used to select the patient_ID
2. The patient_ID is used to select the Genetic_sequence_data and Genetic_sequence_meta_data.
3. Genetic_sequence_meta_data is used to confirm that Genetic_sequence_data is usable.
4. Genetic_sequence_data is used as input for primary analysis pipeline.
5. Primary analysis pipeline consists of ~20 individual tasks.
6. Primary analysis pipeline tasks include multiple new data formats which are later deleted.
7. Primary analysis pipeline outputs final format for storage (e.g. VCF format).
8. Primary analysis pipeline outputs additional formats if required. Examples:
	* clinical genetics report
	* eigenval/eigenvec
	* annotation
	* cohort level statistics

**Second stage:**
1. Output from primary stage is merged with all clinical data for statistical analysis
2. Analysis include that of the genomics lab, machine learning lab.
* Example 1: genetic association studies:
	* outcome:  genotype
	* predictors: clinical consequence
	* covariates: clinical/demographic features
* Example 2: machine learning study:
	* outcome: clinical consequence
	* predictors: clinical/demographic features
	* covariates: genetics

## Examples of data re-use:
1. All original raw sequence data will be re-run incrementally as the first stage receives major pipeline updates.
2. Secondary stage will require re-using the same output from first stage many times for different projects.
3. Pertinent findings from both first stage and second stage will require follow ups. Data from both stages will be re-used.
4. Output from first stage and second stage will include summary statistics that will can be reused for QC and follow-up statistics.

## Other notes:
* Here is an example of an existing concept for [GeneticVariation](https://www.biomedit.ch/rdf/sphn-ontology/sphn#GeneticVariation)
* Note that genetic variant and location are rarely sufficient for final storage. We rely on accurate nomenclature, reference genome version, variant call quality, variant call confidence, etc.
* Note that the final output from first stage is typically [VCF format](https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format) (but will adapt to needs)
* We also typically annotate a VCF with >150 annotation databases (i.e. like one "column" per annotation per variant row)
* We also typically store as a cohort-level VCF (e.g. 1000 samples) rather than one VCF per sample


-e 

File: ./variables.md

---
layout: default
title: Variables
nav_order: 5
---

# Variables
<!-- {: .no_toc } -->
<!-- <details open markdown="block"> -->
<!--   <summary> -->
<!--     Table of contents -->
<!--   </summary> -->
<!--   {: .text-delta } -->
<!-- - TOC -->
<!-- {:toc} -->
<!-- </details> -->

---


* For each script, shared variables will be sourced from the variables file.
* Each pipeline can have its own custom variables file which will be sourced in its entirety or selectively from the master.
* The variables file contains entries such as:

```
DATABASE="./sph/database/"
REF_GRCh38="${DATABASE}/ref/grch38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"
```

* For all new permanent datasets, tools, etc. we add it to the index table.
* We will assign the locations for all shared datasets, tools, etc.
* See [annotation table](annotation_table) for the list of datasets. (This will be updated to include varaibles.)

{: .note }
We will automatically generate the master variables file from the index table which contains meta data about dates, versions, application, etc. for each of the tools, databases, etc. Therefore, the only manual curation required is for the index table, rather than individual variables file/files. To be integrated on [annotation table](annotation_table).


```
+-- ..
|-- (sph)
|
|-- database
|   |-- ref
|   |   |-- grch37
|   |   |-- grch38
|   |   +-- ..
|   |
|   |-- vep
|   |   |-- ..
|   |   |-- ..
|   |   +-- ..
|   |
|   |-- gnomad
|   |   |-- ..
|   |   |-- ..
|   |   +-- ..
|   |
|   |-- (other files, pages with no children)
|   +-- ..
|
|-- tools
|   |-- gatk
|   |
|   |-- vep
|   |   |-- ..
|   |   +-- ..
|   |
|   |-- vt
|   |
|   +-- ..
|
|-- (sph)
+-- ..
```


-e 

File: ./fastp.md

---
layout: default
title: FASTP
date: 2023-07-27 00:00:01
nav_order: 5
---

Last update: 20230727

{: .no_toc }
<details open markdown="block">
<summary>Table of contents</summary>
{: .text-delta }
- TOC
{:toc}
</details>

---

## FASTP
A tool designed to provide fast all-in-one preprocessing for FastQ files. 
This tool is developed in C++ with multithreading supported to afford high performance.

* `fastp.sh` runs on every file in the raw data directory.
* Outputs the same directory structure with processed `.fq.gz` data.
* Checks for existing output before starting and therefore can run incrementally.
* Prints qulity reports to `.json` and `.html`.

## Links
* <https://github.com/OpenGene/fastp>

