<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="manifest" href="/site.webmanifest"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#edbe7a"><link rel="shortcut icon" href="/favicon.ico"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/browserconfig.xml"><meta name="theme-color" content="#ffffff"><title>RL finite MDP | Dev docs</title><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="RL finite MDP" /><meta property="og:locale" content="en_US" /><meta name="description" content="Dev docs made with Jeykll using the Just the Docs theme." /><meta property="og:description" content="Dev docs made with Jeykll using the Just the Docs theme." /><link rel="canonical" href="http://localhost:4000/pages/rl_finte_mdp.html" /><meta property="og:url" content="http://localhost:4000/pages/rl_finte_mdp.html" /><meta property="og:site_name" content="Dev docs" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RL finite MDP" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Dev docs made with Jeykll using the Just the Docs theme.","headline":"RL finite MDP","url":"http://localhost:4000/pages/rl_finte_mdp.html"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="/" class="site-title lh-tight"> Dev docs </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><a href="/pages/bam2fastq.html" class="nav-list-link">BWA</a><li class="nav-list-item"><a href="/pages/bayesian_example1.html" class="nav-list-link">Bayes 1 probability in placenta previa</a><li class="nav-list-item"><a href="/pages/bayesian_example2.html" class="nav-list-link">Bayesian 2 probability in placenta previa</a><li class="nav-list-item"><a href="/pages/bayesian_genetic_carrier.html" class="nav-list-link">Bayesian discrete probability example in genetics</a><li class="nav-list-item"><a href="/pages/bayesian_mcmc_samplers.html" class="nav-list-link">Bayes MCMC samplers</a><li class="nav-list-item"><a href="/pages/bayesian_multiparameter_bioassay.html" class="nav-list-link">Bayes multiparameter bioassay demo</a><li class="nav-list-item"><a href="/pages/bayesian_multiparameter_models.html" class="nav-list-link">Bayes multiparameter models</a><li class="nav-list-item"><a href="/pages/benchmark_pipelines.html" class="nav-list-link">Benchmarking pipeline output</a><li class="nav-list-item"><a href="/pages/bevimed.html" class="nav-list-link">BeviMed</a><li class="nav-list-item"><a href="/pages/biomedit.html" class="nav-list-link">BioMedIT</a><li class="nav-list-item"><a href="/pages/bookmarks.html" class="nav-list-link">Bookmarks</a><li class="nav-list-item"><a href="/pages/bwa.html" class="nav-list-link">BWA</a><li class="nav-list-item"><a href="/pages/causal_inference_stats.html" class="nav-list-link">Causal inference stats</a><li class="nav-list-item"><a href="/pages/causal_inference_whole_game.html" class="nav-list-link">Causal inference mosquito nets</a><li class="nav-list-item"><a href="/pages/data_concepts.html" class="nav-list-link">Data concepts</a><li class="nav-list-item"><a href="/pages/data_stream.html" class="nav-list-link">Data stream</a><li class="nav-list-item"><a href="#" class="nav-list-expander" aria-label="toggle links in Design documents category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="/pages/design_doc.html" class="nav-list-link">Design documents</a><ul class="nav-list"><li class="nav-list-item "><a href="/pages/design_PCA_SNV_INDEL_V1.html" class="nav-list-link">Design PCA SNV INDEL v1</a><li class="nav-list-item "><a href="/pages/design_dna_snvindel_v1.html" class="nav-list-link">Design DNA SNV INDEL v1</a><li class="nav-list-item "><a href="/pages/design_dna_snvindel_v1_release.html" class="nav-list-link">Design release DNA SNV INDEL v1</a><li class="nav-list-item "><a href="/pages/design_dna_snvindel_v2_release.html" class="nav-list-link">Design release DNA SNV INDEL v2</a><li class="nav-list-item "><a href="/pages/design_qv_evidence_flag.html" class="nav-list-link">Design QV evidence framework</a><li class="nav-list-item "><a href="/pages/design_qv_snvindel_v1.html" class="nav-list-link">Design QV SNV INDEL v1</a><li class="nav-list-item "><a href="/pages/design_statistical_genomics_v1.html" class="nav-list-link">Design statistical genomics v1</a></ul><li class="nav-list-item"><a href="/pages/dna_annotation.html" class="nav-list-link">DNA annotation</a><li class="nav-list-item"><a href="/pages/dna_interpretation.html" class="nav-list-link">DNA interpretation</a><li class="nav-list-item"><a href="/pages/dna_qc.html" class="nav-list-link">DNA QC</a><li class="nav-list-item"><a href="/pages/docker_singularity.html" class="nav-list-link">Docker with singularity</a><li class="nav-list-item"><a href="/pages/exomiser.html" class="nav-list-link">Exomiser</a><li class="nav-list-item"><a href="/pages/exomiser_phenodigm.html" class="nav-list-link">Exomiser phenodigm</a><li class="nav-list-item"><a href="/pages/fastp.html" class="nav-list-link">FASTP</a><li class="nav-list-item"><a href="/pages/fastq.html" class="nav-list-link">FASTQ format data</a><li class="nav-list-item"><a href="/pages/filter_vcf_bcftools.html" class="nav-list-link">Filter VCF with bcftools</a><li class="nav-list-item"><a href="/pages/financial_management.html" class="nav-list-link">Financial management</a><li class="nav-list-item"><a href="/pages/guru.html" class="nav-list-link">Guru variant interpretation</a><li class="nav-list-item"><a href="/pages/gwas.html" class="nav-list-link">GWAS analysis</a><li class="nav-list-item"><a href="/pages/hpc.html" class="nav-list-link">HPC infrastructure</a><li class="nav-list-item"><a href="/pages/inf_causal_metab_sem.html" class="nav-list-link">Inference of causal metabolite networks</a><li class="nav-list-item"><a href="/pages/layout.html" class="nav-list-link">Layout</a><li class="nav-list-item"><a href="/pages/mathjax.html" class="nav-list-link">MathJax config</a><li class="nav-list-item"><a href="/pages/mbdf_models.html" class="nav-list-link">MBDF models</a><li class="nav-list-item"><a href="/pages/mbdf_supervised.html" class="nav-list-link">MBDF supervised</a><li class="nav-list-item"><a href="/pages/mbdf_unsupervised.html" class="nav-list-link">MBDF unsupervised</a><li class="nav-list-item"><a href="/pages/metadata.html" class="nav-list-link">WGS metadata</a><li class="nav-list-item"><a href="/pages/metadata_users.html" class="nav-list-link">WGS metadata users</a><li class="nav-list-item"><a href="/pages/metrics_bcftoolscounts.html" class="nav-list-link">Metrics Bcftools counts</a><li class="nav-list-item"><a href="/pages/metrics_bcftoolsstats.html" class="nav-list-link">Metrics Bcftools stats</a><li class="nav-list-item"><a href="/pages/metrics_collectwgsmetrics.html" class="nav-list-link">Metrics CollectWgsMetrics</a><li class="nav-list-item"><a href="/pages/multiblock_data_fusion.html" class="nav-list-link">Multiblock data fusion</a><li class="nav-list-item"><a href="/pages/my_voice.html" class="nav-list-link">QV - My voice</a><li class="nav-list-item"><a href="/pages/panels_disease.html" class="nav-list-link">Panels disease gene</a><li class="nav-list-item"><a href="/pages/pca_biplot_1kg.html" class="nav-list-link">PCA biplot 1000genomes</a><li class="nav-list-item"><a href="/pages/pca_features.html" class="nav-list-link">PCA features</a><li class="nav-list-item"><a href="/pages/platform_updates.html" class="nav-list-link">Platform updates</a><li class="nav-list-item"><a href="/pages/precision_med.html" class="nav-list-link">Precision Medicine Unit (PMU)</a><li class="nav-list-item"><a href="/pages/present/presentations.html" class="nav-list-link">Presentations</a><li class="nav-list-item"><a href="/pages/qv_design.html" class="nav-list-link">QV design principles</a><li class="nav-list-item"><a href="/pages/read_group.html" class="nav-list-link">Read group</a><li class="nav-list-item"><a href="/pages/ref.html" class="nav-list-link">Reference genome</a><li class="nav-list-item active"><a href="/pages/rl_finte_mdp.html" class="nav-list-link active">RL finite MDP</a><li class="nav-list-item"><a href="/pages/slurm_manager.html" class="nav-list-link">SLURM monitoring</a><li class="nav-list-item"><a href="/pages/slurm_sbatch.html" class="nav-list-link">SLURM sbatch headers</a><li class="nav-list-item"><a href="/pages/stats_altman_bland_analysis_of_methods.html" class="nav-list-link">Stats Analysis of methods</a><li class="nav-list-item"><a href="/pages/stats_altman_bland_ci_from_p.html" class="nav-list-link">Stats CI from P</a><li class="nav-list-item"><a href="/pages/stats_altman_bland_correlation.html" class="nav-list-link">Stats Correlation, regression and repeated data</a><li class="nav-list-item"><a href="/pages/stats_altman_bland_odds_ratios.html" class="nav-list-link">Stats Odds ratios, SE & CI</a><li class="nav-list-item"><a href="/pages/stats_altman_bland_roc_curve.html" class="nav-list-link">Stats Receiver operating characteristic plots</a><li class="nav-list-item"><a href="/pages/stats_altman_bland_sensitivity_specificity.html" class="nav-list-link">Stats Sensitivity and specificity</a><li class="nav-list-item"><a href="/pages/storage_architecture_plan.html" class="nav-list-link">Storage architecture plan</a><li class="nav-list-item"><a href="/pages/storage_estimates.html" class="nav-list-link">Storage estimates</a><li class="nav-list-item"><a href="/pages/storage_use_sync_and_versioning.html" class="nav-list-link">Storage, usage, and Git practices</a><li class="nav-list-item"><a href="/pages/style.html" class="nav-list-link">Style page guide</a><li class="nav-list-item"><a href="/pages/style_writing.html" class="nav-list-link">Style writing guide</a><li class="nav-list-item"><a href="/pages/sv.html" class="nav-list-link">Structural variation detection</a><li class="nav-list-item"><a href="/pages/synth_data.html" class="nav-list-link">Synthetic data</a><li class="nav-list-item"><a href="/pages/Phoenix_Sepsis_Score_logic.html" class="nav-list-link">Sepsis score: Phoenix</a><li class="nav-list-item"><a href="/pages/variant_concept.html" class="nav-list-link">Variant to RDF concept</a><li class="nav-list-item"><a href="/pages/vcf.html" class="nav-list-link">VCF - Variant Call Format</a><li class="nav-list-item"><a href="/pages/vcf_gvcf.html" class="nav-list-link">VCF and gVCF</a><li class="nav-list-item"><a href="/pages/virtual_panels.html" class="nav-list-link">Virtual gene panels</a><li class="nav-list-item"><a href="/pages/vsat.html" class="nav-list-link">VSAT with SKAT</a><li class="nav-list-item"><a href="/pages/vsat_setID.html" class="nav-list-link">SetID for VSAT</a><li class="nav-list-item"><a href="/pages/variables.html" class="nav-list-link">Variables</a><li class="nav-list-item"><a href="/pages/acat.html" class="nav-list-link">ACAT</a><li class="nav-list-item"><a href="/pages/acmg_criteria_table_main.html" class="nav-list-link">ACMG criteria</a><li class="nav-list-item"><a href="/pages/aggregate_multiplex.html" class="nav-list-link">Aggregate multiplexed data</a><li class="nav-list-item"><a href="/pages/annotation_table.html" class="nav-list-link">Annotation table</a><li class="nav-list-item"><a href="/pages/documentation_log.html" class="nav-list-link">Documentation log</a></ul></nav><footer class="site-footer"> Maintained by <a href="https://github.com/DylanLawless">Dylan Lawless</a> for <a href="https://switzerlandomics.ch">SwitzerlandOmics.ch</a>.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Dev docs" aria-label="Search Dev docs" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://github.com/docs-switzerlandomics.github.io" class="site-button" > Github </a><li class="aux-nav-list-item"> <a href="https://switzerlandomics.ch" class="site-button" > SwitzerlandOmics.ch </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><div id="main-content" class="main-content" role="main"><h1 id="finite-markov-decision-process-mdp"> <a href="#finite-markov-decision-process-mdp" class="anchor-heading" aria-labelledby="finite-markov-decision-process-mdp"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Finite Markov Decision Process (MDP)</h1><p>Last update: 20250322</p><p>In reinforcement learning, the agent interacts with an environment through a well-defined interface. The agent’s goal is to maximise cumulative rewards by selecting actions that influence the state of the environment. Key concepts include:</p><ul><li><strong>Goals:</strong> The objectives the agent strives to achieve.<li><strong>Rewards:</strong> Immediate feedback received after performing an action.<li><strong>Returns:</strong> The total accumulated reward over an episode.<li><strong>Episodes:</strong> Sequences of states, actions, and rewards that conclude when a terminal condition (goal achieved or failure) is met.</ul><p>These ideas are exemplified in the cart pole balancing problem, where the agent must keep a pole balanced on a moving cart. Here, the environment continuously provides rewards based on the pole’s stability, and an episode ends when the pole falls or the cart leaves a designated area.</p><figure> <img src="https://raw.githubusercontent.com/DylanLawless/R-reinforcement-learning-an-introduction/main/figures/fig_ex_3_4.png" width="49%" /> <img src="https://raw.githubusercontent.com/DylanLawless/R-reinforcement-learning-an-introduction/main/figures/fig_ex_3_4_compressed.gif" width="49%" /><figcaption>Figure example 3.4 Cart pole balance metrics and the final result of learning to balance.</figcaption></figure><p>Moving on to a discrete setting, the gridworld example represents a simple Markov Decision Process (MDP) with a 5×5 grid where each cell is a state. The environment is defined by standard grid dynamics, with a discount factor (γ = 0.9) and a termination criterion based on a small change threshold (1e–6). The agent can move in one of four directions (up, right, down, left), but there are two special states (A and B) that trigger exceptional transitions:</p><ul><li><strong>Special Transitions:</strong><ul><li>When in state A (cell at (1,4)), any action leads the agent to state A’ (cell at (1,0)) with a reward of +10.<li>When in state B (cell at (3,4)), any action leads the agent to state B’ (cell at (3,2)) with a reward of +5.</ul></ul><p>For other states, moving off the grid results in staying in the same state with a penalty of –1, while valid moves provide zero immediate reward.</p><p>Two main value functions are computed:</p><ul><li><strong>Uniform Policy Value Function (V):</strong><br /> This is computed by averaging the backups (expected returns) over all actions, simulating a uniform random policy.<li><strong>Optimal Value Function (V*):</strong><br /> Here, the Bellman optimality equation is used (taking the maximum over actions) to iteratively compute the optimal state-value function. Alongside V*, the optimal policy (π*) is derived, indicating for each state the action(s) that maximise the expected return.</ul><p>The resulting figures illustrate these concepts:</p><ul><li><strong>Figure 3.2 (Combined Uniform Policy and Exceptional Dynamics):</strong><ul><li><em>Left Panel – Exceptional Reward Dynamics:</em><br /> This plot shows the grid with curved red arrows that indicate the special transitions from state A to A’ and from state B to B’. The arrows are annotated with the respective rewards (+10 and +5), highlighting the exceptional dynamics of the MDP.<li><em>Right Panel – Uniform State-Value Function:</em><br /> Here, the state-value function computed under a uniform random policy is visualised. Each cell displays its value (printed as text) and is colour-coded, providing a clear picture of how the environment’s dynamics (including the special transitions) propagate values throughout the grid.</ul></ul><figure> <img src="https://raw.githubusercontent.com/DylanLawless/R-reinforcement-learning-an-introduction/main/figures/fig_3_2.png" width="100%" /><figcaption>Figure 3.2: Grid example with random policy</figcaption></figure><ul><li><strong>Figure 3.5 (Combined Optimal Value and Policy):</strong><br /> This composite figure consists of three subplots:<ol><li><em>Exceptional Reward Dynamics:</em> (Same as in Figure 3.2)<br /> Reiterates the exceptional transitions to contextualise the optimal calculations.<li><em>Optimal State-Value Function (V*):</em><br /> The computed optimal values are displayed in each cell with both numerical annotations and a colour gradient, illustrating the effect of choosing the best possible actions.<li><em>Optimal Policy (π*):</em><br /> This plot overlays directional arrows inside each grid cell, representing the optimal action(s) derived from V*. The arrows correspond to the directions (up, right, down, left) that yield the highest expected return. In some cells, multiple arrows appear if several actions are equally optimal.</ol></ul><figure> <img src="https://raw.githubusercontent.com/DylanLawless/R-reinforcement-learning-an-introduction/main/figures/fig_3_5.png" width="100%" /><figcaption>Figure 3.5: Optimal solutions to the gridworld example</figcaption></figure><p>Together, these figures provide a comprehensive visualisation of the theory behind reinforcement learning, illustrating both the agent–environment interface (as demonstrated in the pole balancing task) and the formal MDP framework (as implemented in the gridworld example). They demonstrate how rewards and returns are generated in episodes, how value functions are computed, and how optimal decision-making is represented both in value terms and as explicit action choices within the grid.</p><hr><footer><div class="d-flex mt-2"></div></footer></div></div><div class="search-overlay"></div></div>
